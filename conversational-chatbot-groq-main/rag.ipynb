{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import os\n",
    "from groq import Groq\n",
    "import random\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.chains import ConversationChain, LLMChain\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain_community.document_loaders import AsyncChromiumLoader\n",
    "from langchain_community.document_transformers import BeautifulSoupTransformer\n",
    "\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_community.document_loaders import WikipediaLoader, WebBaseLoader, TextLoader\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def fetch_content(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    text_content = ''\n",
    "    for paragraph in soup.find_all('p'):  # Extract text content from <p> tags\n",
    "        text_content += paragraph.get_text() + ' '\n",
    "    return text_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "urls = [\"https://indianexpress.com/article/india/om-birla-election-lok-sabha-speaker-modi-opposition-9415980/\" , \"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\"]\n",
    "\n",
    "list_text = []\n",
    "docs = []\n",
    "for url in urls:\n",
    "    doc = Document(page_content=fetch_content(url),\n",
    "              metadata={\n",
    "                  \"source\": url\n",
    "              }\n",
    "             )\n",
    "    docs.append(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beautiful Soup is a\n",
      "Python library for pulling data out of HTML and XML files. It works\n",
      "with your favorite parser to provide idiomatic ways of navigating,\n",
      "searching, and modifying the parse tree. It commonly saves programmers\n",
      "hours or days of work. These instructions illustrate all major features of Beautiful Soup 4,\n",
      "with examples. I show you what the library is good for, how it works,\n",
      "how to use it, how to make it do what you want, and what to do when it\n",
      "violates your expectations. This document covers Beautiful Soup version 4.12.2. The examples in\n",
      "this documentation were written for Python 3.8. You might be looking for the documentation for Beautiful Soup 3.\n",
      "If so, you should know that Beautiful Soup 3 is no longer being\n",
      "developed and that all support for it was dropped on December\n",
      "31, 2020. If you want to learn about the differences between Beautiful\n",
      "Soup 3 and Beautiful Soup 4, see Porting code to BS4. This documentation has been translated into other languages by\n",
      "Beautiful Soup users: 这篇文档当然还有中文版. このページは日本語で利用できます(外部リンク) 이 문서는 한국어 번역도 가능합니다. Este documento também está disponível em Português do Brasil. Este documento también está disponible en una traducción al español. Эта документация доступна на русском языке. If you have questions about Beautiful Soup, or run into problems,\n",
      "send mail to the discussion group. If\n",
      "your problem involves parsing an HTML document, be sure to mention\n",
      "what the diagnose() function says about\n",
      "that document. When reporting an error in this documentation, please mention which\n",
      "translation you’re reading. Here’s an HTML document I’ll be using as an example throughout this\n",
      "document. It’s part of a story from Alice in Wonderland: Running the “three sisters” document through Beautiful Soup gives us a\n",
      "BeautifulSoup object, which represents the document as a nested\n",
      "data structure: Here are some simple ways to navigate that data structure: One common task is extracting all the URLs found within a page’s <a> tags: Another common task is extracting all the text from a page: Does this look like what you need? If so, read on. If you’re using a recent version of Debian or Ubuntu Linux, you can\n",
      "install Beautiful Soup with the system package manager: $ apt-get install python3-bs4 Beautiful Soup 4 is published through PyPi, so if you can’t install it\n",
      "with the system packager, you can install it with easy_install or\n",
      "pip. The package name is beautifulsoup4. Make sure you use the\n",
      "right version of pip or easy_install for your Python version\n",
      "(these may be named pip3 and easy_install3 respectively). $ easy_install beautifulsoup4 $ pip install beautifulsoup4 (The BeautifulSoup package is not what you want. That’s\n",
      "the previous major release, Beautiful Soup 3. Lots of software uses\n",
      "BS3, so it’s still available, but if you’re writing new code you\n",
      "should install beautifulsoup4.) If you don’t have easy_install or pip installed, you can\n",
      "download the Beautiful Soup 4 source tarball and\n",
      "install it with setup.py. $ python setup.py install If all else fails, the license for Beautiful Soup allows you to\n",
      "package the entire library with your application. You can download the\n",
      "tarball, copy its bs4 directory into your application’s codebase,\n",
      "and use Beautiful Soup without installing it at all. I use Python 3.10 to develop Beautiful Soup, but it should work with\n",
      "other recent versions. Beautiful Soup supports the HTML parser included in Python’s standard\n",
      "library, but it also supports a number of third-party Python parsers.\n",
      "One is the lxml parser. Depending on your setup,\n",
      "you might install lxml with one of these commands: $ apt-get install python-lxml $ easy_install lxml $ pip install lxml Another alternative is the pure-Python html5lib parser, which parses HTML the way a\n",
      "web browser does. Depending on your setup, you might install html5lib\n",
      "with one of these commands: $ apt-get install python3-html5lib $ pip install html5lib This table summarizes the advantages and disadvantages of each parser library: Parser Typical usage Advantages Disadvantages Python’s html.parser BeautifulSoup(markup, \"html.parser\") Batteries included Decent speed Not as fast as lxml,\n",
      "less lenient than\n",
      "html5lib. lxml’s HTML parser BeautifulSoup(markup, \"lxml\") Very fast External C dependency lxml’s XML parser BeautifulSoup(markup, \"lxml-xml\")\n",
      "BeautifulSoup(markup, \"xml\") Very fast The only currently supported\n",
      "XML parser External C dependency html5lib BeautifulSoup(markup, \"html5lib\") Extremely lenient Parses pages the same way a\n",
      "web browser does Creates valid HTML5 Very slow External Python\n",
      "dependency If you can, I recommend you install and use lxml for speed. Note that if a document is invalid, different parsers will generate\n",
      "different Beautiful Soup trees for it. See Differences\n",
      "between parsers for details. To parse a document, pass it into the BeautifulSoup\n",
      "constructor. You can pass in a string or an open filehandle: First, the document is converted to Unicode, and HTML entities are\n",
      "converted to Unicode characters: Beautiful Soup then parses the document using the best available\n",
      "parser. It will use an HTML parser unless you specifically tell it to\n",
      "use an XML parser. (See Parsing XML.) Beautiful Soup transforms a complex HTML document into a complex tree\n",
      "of Python objects. But you’ll only ever have to deal with about four\n",
      "kinds of objects: Tag, NavigableString, BeautifulSoup,\n",
      "and Comment. These objects represent the HTML elements\n",
      "that comprise the page. A Tag object corresponds to an XML or HTML tag in the original document. Tags have a lot of attributes and methods, and I’ll cover most of them\n",
      "in Navigating the tree and Searching the tree. For now, the most\n",
      "important methods of a tag are for accessing its name and attributes. Every tag has a name: If you change a tag’s name, the change will be reflected in any\n",
      "markup generated by Beautiful Soup down the line: An HTML or XML tag may have any number of attributes. The tag <b\n",
      "id=\"boldest\"> has an attribute “id” whose value is\n",
      "“boldest”. You can access a tag’s attributes by treating the tag like\n",
      "a dictionary: You can access the dictionary of attributes directly as .attrs: You can add, remove, and modify a tag’s attributes. Again, this is\n",
      "done by treating the tag as a dictionary: HTML 4 defines a few attributes that can have multiple values. HTML 5\n",
      "removes a couple of them, but defines a few more. The most common\n",
      "multi-valued attribute is class (that is, a tag can have more than\n",
      "one CSS class). Others include rel, rev, accept-charset,\n",
      "headers, and accesskey. By default, Beautiful Soup stores the value(s)\n",
      "of a multi-valued attribute as a list: When you turn a tag back into a string, the values of any multi-valued\n",
      "attributes are consolidated: If an attribute looks like it has more than one value, but it’s not\n",
      "a multi-valued attribute as defined by any version of the HTML\n",
      "standard, Beautiful Soup stores it as a simple string: You can force all attributes to be stored as strings by passing\n",
      "multi_valued_attributes=None as a keyword argument into the\n",
      "BeautifulSoup constructor: You can use get_attribute_list to always return the value in a list\n",
      "container, whether it’s a string or multi-valued attribute value: If you parse a document as XML, there are no multi-valued attributes: Again, you can configure this using the multi_valued_attributes argument: You probably won’t need to do this, but if you do, use the defaults as\n",
      "a guide. They implement the rules described in the HTML specification: A tag can contain strings as pieces of text. Beautiful Soup\n",
      "uses the NavigableString class to contain these pieces of text: A NavigableString is just like a Python Unicode string, except\n",
      "that it also supports some of the features described in Navigating\n",
      "the tree and Searching the tree. You can convert a\n",
      "NavigableString to a Unicode string with str: You can’t edit a string in place, but you can replace one string with\n",
      "another, using replace_with(): NavigableString supports most of the features described in\n",
      "Navigating the tree and Searching the tree, but not all of\n",
      "them. In particular, since a string can’t contain anything (the way a\n",
      "tag may contain a string or another tag), strings don’t support the\n",
      ".contents or .string attributes, or the find() method. If you want to use a NavigableString outside of Beautiful Soup,\n",
      "you should call unicode() on it to turn it into a normal Python\n",
      "Unicode string. If you don’t, your string will carry around a\n",
      "reference to the entire Beautiful Soup parse tree, even when you’re\n",
      "done using Beautiful Soup. This is a big waste of memory. The BeautifulSoup object represents the parsed document as a\n",
      "whole. For most purposes, you can treat it as a Tag\n",
      "object. This means it supports most of the methods described in\n",
      "Navigating the tree and Searching the tree. You can also pass a BeautifulSoup object into one of the methods\n",
      "defined in Modifying the tree, just as you would a Tag. This\n",
      "lets you do things like combine two parsed documents: Since the BeautifulSoup object doesn’t correspond to an actual\n",
      "HTML or XML tag, it has no name and no attributes. But sometimes it’s\n",
      "useful to reference its .name (such as when writing code that works\n",
      "with both Tag and BeautifulSoup objects),\n",
      "so it’s been given the special .name “[document]”: Tag, NavigableString, and\n",
      "BeautifulSoup cover almost everything you’ll see in an\n",
      "HTML or XML file, but there are a few leftover bits. The main one\n",
      "you’ll probably encounter is the Comment. The Comment object is just a special type of NavigableString: But when it appears as part of an HTML document, a Comment is\n",
      "displayed with special formatting: Beautiful Soup defines a few NavigableString subclasses to\n",
      "contain strings found inside specific HTML tags. This makes it easier\n",
      "to pick out the main body of the page, by ignoring strings that\n",
      "probably represent programming directives found within the\n",
      "page. (These classes are new in Beautiful Soup 4.9.0, and the\n",
      "html5lib parser doesn’t use them.) A NavigableString subclass that represents embedded CSS\n",
      "stylesheets; that is, any strings found inside a <style> tag\n",
      "during document parsing. A NavigableString subclass that represents embedded\n",
      "Javascript; that is, any strings found inside a <script> tag\n",
      "during document parsing. A NavigableString subclass that represents embedded HTML\n",
      "templates; that is, any strings found inside a <template> tag during\n",
      "document parsing. Beautiful Soup defines some NavigableString classes for\n",
      "holding special types of strings that can be found in XML\n",
      "documents. Like Comment, these classes are subclasses of\n",
      "NavigableString that add something extra to the string on\n",
      "output. A NavigableString subclass representing the declaration at the beginning of\n",
      "an XML document. A NavigableString subclass representing the document type\n",
      "declaration which may\n",
      "be found near the beginning of an XML document. A NavigableString subclass that represents a CData section. A NavigableString subclass that represents the contents\n",
      "of an XML processing instruction. Here’s the “Three sisters” HTML document again: I’ll use this as an example to show you how to move from one part of\n",
      "a document to another. Tags may contain strings and more tags. These elements are the tag’s\n",
      "children. Beautiful Soup provides a lot of different attributes for\n",
      "navigating and iterating over a tag’s children. Note that Beautiful Soup strings don’t support any of these\n",
      "attributes, because a string can’t have children. The simplest way to navigate the parse tree is to find a tag by name. To\n",
      "do this, you can use the find() method: For convenience, just saying the name of the tag you want is equivalent\n",
      "to find() (if no built-in attribute has that name). If you want the\n",
      "<head> tag, just say soup.head: You can use this trick again and again to zoom in on a certain part\n",
      "of the parse tree. This code gets the first <b> tag beneath the <body> tag: find() (and its convenience equivalent) gives you only the first tag\n",
      "by that name: If you need to get all the <a> tags, you can use find_all(): For more complicated tasks, such as pattern-matching and filtering, you can\n",
      "use the methods described in Searching the tree. A tag’s children are available in a list called .contents: The BeautifulSoup object itself has children. In this case, the\n",
      "<html> tag is the child of the BeautifulSoup object.: A string does not have .contents, because it can’t contain\n",
      "anything: Instead of getting them as a list, you can iterate over a tag’s\n",
      "children using the .children generator: If you want to modify a tag’s children, use the methods described in\n",
      "Modifying the tree. Don’t modify the the .contents list\n",
      "directly: that can lead to problems that are subtle and difficult to\n",
      "spot. The .contents and .children attributes consider only a tag’s\n",
      "direct children. For instance, the <head> tag has a single direct\n",
      "child–the <title> tag: But the <title> tag itself has a child: the string “The Dormouse’s\n",
      "story”. There’s a sense in which that string is also a child of the\n",
      "<head> tag. The .descendants attribute lets you iterate over all\n",
      "of a tag’s children, recursively: its direct children, the children of\n",
      "its direct children, and so on: The <head> tag has only one child, but it has two descendants: the\n",
      "<title> tag and the <title> tag’s child. The BeautifulSoup object\n",
      "only has one direct child (the <html> tag), but it has a whole lot of\n",
      "descendants: If a tag has only one child, and that child is a NavigableString,\n",
      "the child is made available as .string: If a tag’s only child is another tag, and that tag has a\n",
      ".string, then the parent tag is considered to have the same\n",
      ".string as its child: If a tag contains more than one thing, then it’s not clear what\n",
      ".string should refer to, so .string is defined to be\n",
      "None: If there’s more than one thing inside a tag, you can still look at\n",
      "just the strings. Use the .strings generator to see all descendant\n",
      "strings: Newlines and spaces that separate tags are also strings. You can remove extra\n",
      "whitespace by using the .stripped_strings generator instead: Here, strings consisting entirely of whitespace are ignored, and\n",
      "whitespace at the beginning and end of strings is removed. Continuing the “family tree” analogy, every tag and every string has a\n",
      "parent: the tag that contains it. You can access an element’s parent with the .parent attribute. In\n",
      "the example “three sisters” document, the <head> tag is the parent\n",
      "of the <title> tag: The title string itself has a parent: the <title> tag that contains\n",
      "it: The parent of a top-level tag like <html> is the BeautifulSoup object\n",
      "itself: And the .parent of a BeautifulSoup object is defined as None: You can iterate over all of an element’s parents with\n",
      ".parents. This example uses .parents to travel from an <a> tag\n",
      "buried deep within the document, to the very top of the document: Consider a simple document like this: The <b> tag and the <c> tag are at the same level: they’re both direct\n",
      "children of the same tag. We call them siblings. When a document is\n",
      "pretty-printed, siblings show up at the same indentation level. You\n",
      "can also use this relationship in the code you write. You can use .next_sibling and .previous_sibling to navigate\n",
      "between page elements that are on the same level of the parse tree: The <b> tag has a .next_sibling, but no .previous_sibling,\n",
      "because there’s nothing before the <b> tag on the same level of the\n",
      "tree. For the same reason, the <c> tag has a .previous_sibling\n",
      "but no .next_sibling: The strings “text1” and “text2” are not siblings, because they don’t\n",
      "have the same parent: In real documents, the .next_sibling or .previous_sibling of a\n",
      "tag will usually be a string containing whitespace. Going back to the\n",
      "“three sisters” document: You might think that the .next_sibling of the first <a> tag would\n",
      "be the second <a> tag. But actually, it’s a string: the comma and\n",
      "newline that separate the first <a> tag from the second: The second <a> tag is then the .next_sibling of the comma string: You can iterate over a tag’s siblings with .next_siblings or\n",
      ".previous_siblings: (If the argument syntax to find tags by their attribute value is unfamiliar,\n",
      "don’t worry; this is covered later in The keyword arguments.) Take a look at the beginning of the “three sisters” document: An HTML parser takes this string of characters and turns it into a\n",
      "series of events: “open an <html> tag”, “open a <head> tag”, “open a\n",
      "<title> tag”, “add a string”, “close the <title> tag”, “open a <p>\n",
      "tag”, and so on. The order in which the opening tags and strings are\n",
      "encountered is called document order. Beautiful Soup offers tools for\n",
      "searching a document’s elements in document order. The .next_element attribute of a string or tag points to whatever\n",
      "was parsed immediately after the opening of the current tag or after\n",
      "the current string. It might be the same as .next_sibling, but it’s\n",
      "usually drastically different. Here’s the final <a> tag in the “three sisters” document. Its\n",
      ".next_sibling is a string: the conclusion of the sentence that was\n",
      "interrupted by the start of the <a> tag: But the .next_element of that <a> tag, the thing that was parsed\n",
      "immediately after the <a> tag, is not the rest of that sentence:\n",
      "it’s the string “Tillie” inside it: That’s because in the original markup, the word “Tillie” appeared\n",
      "before that semicolon. The parser encountered an <a> tag, then the\n",
      "word “Tillie”, then the closing </a> tag, then the semicolon and rest of\n",
      "the sentence. The semicolon is on the same level as the <a> tag, but the\n",
      "word “Tillie” was encountered first. The .previous_element attribute is the exact opposite of\n",
      ".next_element. It points to the opening tag or string that was\n",
      "parsed immediately before this one: You should get the idea by now. You can use these iterators to move\n",
      "forward or backward in the document as it was parsed: Beautiful Soup defines a lot of methods for searching the parse tree,\n",
      "but they’re all very similar. I’m going to spend a lot of time explaining\n",
      "the two most popular methods: find() and find_all(). The other\n",
      "methods take almost exactly the same arguments, so I’ll just cover\n",
      "them briefly. Once again, I’ll be using the “three sisters” document as an example: By passing in a filter to a method like find_all(), you can\n",
      "zoom in on the parts of the document you’re interested in. Before talking in detail about find_all() and similar methods, I\n",
      "want to show examples of different filters you can pass into these\n",
      "methods. These filters show up again and again, throughout the\n",
      "search API. You can use them to filter based on a tag’s name,\n",
      "on its attributes, on the text of a string, or on some combination of\n",
      "these. The simplest filter is a string. Pass a string to a search method and\n",
      "Beautiful Soup will perform a tag-name match against that exact string.\n",
      "This code finds all the <b> tags in the document: If you pass in a byte string, Beautiful Soup will assume the string is\n",
      "encoded as UTF-8. You can avoid this by passing in a Unicode string instead. If you pass in a regular expression object, Beautiful Soup will filter\n",
      "against that regular expression using its search() method. This code\n",
      "finds all the tags whose names start with the letter “b”; in this\n",
      "case, the <body> tag and the <b> tag: This code finds all the tags whose names contain the letter ‘t’: The value True matches every tag it can. This code finds all\n",
      "the tags in the document, but none of the text strings: If none of the other matches work for you, define a function that\n",
      "takes an element as its only argument. The function should return\n",
      "True if the argument matches, and False otherwise. Here’s a function that returns True if a tag defines the “class”\n",
      "attribute but doesn’t define the “id” attribute: Pass this function into find_all() and you’ll pick up all the <p>\n",
      "tags: This function picks up only the <p> tags. It doesn’t pick up the <a>\n",
      "tags, because those tags define both “class” and “id”. It doesn’t pick\n",
      "up tags like <html> and <title>, because those tags don’t define\n",
      "“class”. The function can be as complicated as you need it to be. Here’s a\n",
      "function that returns True if a tag is surrounded by string\n",
      "objects: If you pass in a list, Beautiful Soup will look for a match against\n",
      "any string, regular expression, or function in that list. This\n",
      "code finds all the <a> tags and all the <b> tags: Now we’re ready to look at the search methods in detail. Method signature: find_all(name, attrs, recursive, string, limit, **kwargs) The find_all() method looks through a tag’s descendants and\n",
      "retrieves all descendants that match your filters. I gave several\n",
      "examples in Kinds of filters, but here are a few more: Some of these should look familiar, but others are new. What does it\n",
      "mean to pass in a value for string, or id? Why does\n",
      "find_all(\"p\", \"title\") find a <p> tag with the CSS class “title”?\n",
      "Let’s look at the arguments to find_all(). Pass in a value for name and you’ll tell Beautiful Soup to only\n",
      "consider tags with certain names. Text strings will be ignored, as\n",
      "will tags whose names that don’t match. This is the simplest usage: Recall from Kinds of filters that the value to name can be a\n",
      "string, a regular expression, a list, a function, or the value\n",
      "True. Any keyword argument that’s not recognized will be turned into a filter\n",
      "that matches tags by their attributes. If you pass in a value for an argument called id, Beautiful Soup will\n",
      "filter against each tag’s ‘id’ attribute value: Just as with tags, you can filter an attribute based on a string,\n",
      "a regular expression, a list, a function, or the value True. If you pass in a regular expression object for href, Beautiful Soup will\n",
      "pattern-match against each tag’s ‘href’ attribute value: The value True matches every tag that defines the attribute. This code\n",
      "finds all tags with an id attribute: soup.find_all(id=True)\n",
      "# [<a class=”sister” href=”http://example.com/elsie” id=”link1”>Elsie</a>,\n",
      "#  <a class=”sister” href=”http://example.com/lacie” id=”link2”>Lacie</a>,\n",
      "#  <a class=”sister” href=”http://example.com/tillie” id=”link3”>Tillie</a>] For more complex matches, you can define a function that takes an attribute\n",
      "value as its only argument. The function should return True if the value\n",
      "matches, and False otherwise. Here’s a function that finds all a tags whose href attribute does not\n",
      "match a regular expression: If you pass in a list for an argument, Beautiful Soup will look for an\n",
      "attribute-value match against any string, regular expression, or function in\n",
      "that list. This code finds the first and last link: soup.find_all(id=[“link1”, re.compile(“3$”)])\n",
      "# [<a class=”sister” href=”http://example.com/elsie” id=”link1”>Elsie</a>,\n",
      "#  <a class=”sister” href=”http://example.com/tillie” id=”link3”>Tillie</a>] You can filter against multiple attributes at once by passing multiple\n",
      "keyword arguments: Some attributes, like the data-* attributes in HTML 5, have names that\n",
      "can’t be used as the names of keyword arguments: You can use these attributes in searches by putting them into a\n",
      "dictionary and passing the dictionary into find_all() as the\n",
      "attrs argument: Similarly, you can’t use a keyword argument to search for HTML’s ‘name’ attribute,\n",
      "because Beautiful Soup uses the name argument to contain the name\n",
      "of the tag itself. Instead, you can give a value to ‘name’ in the\n",
      "attrs argument: It’s very useful to search for a tag that has a certain CSS class, but\n",
      "the name of the CSS attribute, “class”, is a reserved word in\n",
      "Python. Using class as a keyword argument will give you a syntax\n",
      "error. As of Beautiful Soup 4.1.2, you can search by CSS class using\n",
      "the keyword argument class_: As with any keyword argument, you can pass class_ a string, a regular\n",
      "expression, a function, or True: Remember that a single tag can have multiple\n",
      "values for its “class” attribute. When you search for a tag that\n",
      "matches a certain CSS class, you’re matching against any of its CSS\n",
      "classes: You can also search for the exact string value of the class attribute: But searching for variants of the string value won’t work: In older versions of Beautiful Soup, which don’t have the class_\n",
      "shortcut, you can use the attrs argument trick mentioned above.\n",
      "Create a dictionary whose value for “class” is the string (or regular\n",
      "expression, or whatever) you want to search for: To search for tags that match two or more CSS classes at once, use the\n",
      "select() CSS selector method described here: With the string argument, you can search for strings instead of tags. As\n",
      "with name and attribute keyword arguments, you can pass in a string, a\n",
      "regular expression, a function, a list, or the value True.\n",
      "Here are some examples: If you use the string argument in a tag search, Beautiful Soup will find\n",
      "all tags whose .string matches your value for string. This code finds\n",
      "the <a> tags whose .string is “Elsie”: The string argument is new in Beautiful Soup 4.4.0. In earlier\n",
      "versions it was called text: find_all() returns all the tags and strings that match your\n",
      "filters. This can take a while if the document is large. If you don’t\n",
      "need all the results, you can pass in a number for limit. This\n",
      "works just like the LIMIT keyword in SQL. It tells Beautiful Soup to\n",
      "stop gathering results after it’s found a certain number. There are three links in the “three sisters” document, but this code\n",
      "only finds the first two: By default, mytag.find_all() will examine all the descendants of mytag:\n",
      "its children, its children’s children, and so on. To consider only direct\n",
      "children, you can pass in recursive=False. See the difference here: Here’s that part of the document: The <title> tag is beneath the <html> tag, but it’s not directly\n",
      "beneath the <html> tag: the <head> tag is in the way. Beautiful Soup\n",
      "finds the <title> tag when it’s allowed to look at all descendants of\n",
      "the <html> tag, but when recursive=False restricts it to the\n",
      "<html> tag’s immediate children, it finds nothing. Beautiful Soup offers a lot of tree-searching methods (covered below),\n",
      "and they mostly take the same arguments as find_all(): name,\n",
      "attrs, string, limit, and attribute keyword arguments. But the\n",
      "recursive argument is specific to the find_all() and find() methods.\n",
      "Passing recursive=False into a method like find_parents() wouldn’t be\n",
      "very useful. For convenience, calling a BeautifulSoup object or\n",
      "Tag object as a function is equivalent to calling\n",
      "find_all() (if no built-in method has the name of the tag you’re\n",
      "looking for). These two lines of code are equivalent: These two lines are also equivalent: Method signature: find(name, attrs, recursive, string, **kwargs) The find_all() method scans the entire document looking for\n",
      "results, but sometimes you only want to find one result. If you know a\n",
      "document has only one <body> tag, it’s a waste of time to scan the\n",
      "entire document looking for more. Rather than passing in limit=1\n",
      "every time you call find_all, you can use the find()\n",
      "method. These two lines of code are nearly equivalent: The only difference is that find_all() returns a list containing\n",
      "the single result, and find() just returns the result. If find_all() can’t find anything, it returns an empty list. If\n",
      "find() can’t find anything, it returns None: Remember the soup.head.title trick from Navigating using tag\n",
      "names? That trick works by repeatedly calling find(): Method signature: find_parents(name, attrs, string, limit, **kwargs) Method signature: find_parent(name, attrs, string, **kwargs) I spent a lot of time above covering find_all() and\n",
      "find(). The Beautiful Soup API defines ten other methods for\n",
      "searching the tree, but don’t be afraid. Five of these methods are\n",
      "basically the same as find_all(), and the other five are basically\n",
      "the same as find(). The only differences are in how they move from\n",
      "one part of the tree to another. First let’s consider find_parents() and\n",
      "find_parent(). Remember that find_all() and find() work\n",
      "their way down the tree, looking at tag’s descendants. These methods\n",
      "do the opposite: they work their way up the tree, looking at a tag’s\n",
      "(or a string’s) parents. Let’s try them out, starting from a string\n",
      "buried deep in the “three daughters” document: One of the three <a> tags is the direct parent of the string in\n",
      "question, so our search finds it. One of the three <p> tags is an\n",
      "indirect parent (ancestor) of the string, and our search finds that as\n",
      "well. There’s a <p> tag with the CSS class “title” somewhere in the\n",
      "document, but it’s not one of this string’s parents, so we can’t find\n",
      "it with find_parents(). You may have noticed a similarity between find_parent() and\n",
      "find_parents(), and the .parent and .parents attributes\n",
      "mentioned earlier. These search methods actually use the .parents\n",
      "attribute to iterate through all parents (unfiltered), checking each one\n",
      "against the provided filter to see if it matches. Method signature: find_next_siblings(name, attrs, string, limit, **kwargs) Method signature: find_next_sibling(name, attrs, string, **kwargs) These methods use .next_siblings to\n",
      "iterate over the rest of an element’s siblings in the tree. The\n",
      "find_next_siblings() method returns all the siblings that match,\n",
      "and find_next_sibling() returns only the first one: Method signature: find_previous_siblings(name, attrs, string, limit, **kwargs) Method signature: find_previous_sibling(name, attrs, string, **kwargs) These methods use .previous_siblings to iterate over an element’s\n",
      "siblings that precede it in the tree. The find_previous_siblings()\n",
      "method returns all the siblings that match, and\n",
      "find_previous_sibling() returns only the first one: Method signature: find_all_next(name, attrs, string, limit, **kwargs) Method signature: find_next(name, attrs, string, **kwargs) These methods use .next_elements to\n",
      "iterate over whatever tags and strings that come after it in the\n",
      "document. The find_all_next() method returns all matches, and\n",
      "find_next() returns only the first match: In the first example, the string “Elsie” showed up, even though it was\n",
      "contained within the <a> tag we started from. In the second example,\n",
      "the last <p> tag in the document showed up, even though it’s not in\n",
      "the same part of the tree as the <a> tag we started from. For these\n",
      "methods, all that matters is that an element matches the filter and\n",
      "it shows up later in the document in document order. Method signature: find_all_previous(name, attrs, string, limit, **kwargs) Method signature: find_previous(name, attrs, string, **kwargs) These methods use .previous_elements to\n",
      "iterate over the tags and strings that came before it in the\n",
      "document. The find_all_previous() method returns all matches, and\n",
      "find_previous() only returns the first match: The call to find_all_previous(\"p\") found the first paragraph in\n",
      "the document (the one with class=”title”), but it also finds the\n",
      "second paragraph, the <p> tag that contains the <a> tag we started\n",
      "with. This shouldn’t be too surprising: we’re looking at all the tags\n",
      "that show up earlier in the document in document order than the one we started with. A\n",
      "<p> tag that contains an <a> tag must have shown up before the <a>\n",
      "tag it contains. BeautifulSoup and Tag objects support CSS selectors through\n",
      "their .css property. The actual selector implementation is handled\n",
      "by the Soup Sieve\n",
      "package, available on PyPI as soupsieve. If you installed\n",
      "Beautiful Soup through pip, Soup Sieve was installed at the same\n",
      "time, so you don’t have to do anything extra. The Soup Sieve documentation lists all the currently supported CSS\n",
      "selectors, but\n",
      "here are some of the basics. You can find tags by name: Find tags by ID: Find tags contained anywhere within other tags: Find tags directly within other tags: Find all matching next siblings of tags: Find the next sibling tag (but only if it matches): Find tags by CSS class: Find tags that match any selector from a list of selectors: Test for the existence of an attribute: Find tags by attribute value: There’s also a method called select_one(), which finds only the\n",
      "first tag that matches a selector: As a convenience, you can call select() and select_one() can\n",
      "directly on the BeautifulSoup or Tag object, omitting the\n",
      ".css property: CSS selector support is a convenience for people who already know the\n",
      "CSS selector syntax. You can do all of this with the Beautiful Soup\n",
      "API. If CSS selectors are all you need, you should skip Beautiful Soup\n",
      "altogether and parse the document with lxml: it’s a lot\n",
      "faster. But Soup Sieve lets you combine CSS selectors with the\n",
      "Beautiful Soup API. Soup Sieve offers a substantial API beyond the select() and\n",
      "select_one() methods, and you can access most of that API through\n",
      "the .css attribute of Tag or BeautifulSoup. What follows\n",
      "is just a list of the supported methods; see the Soup Sieve\n",
      "documentation for full\n",
      "documentation. The iselect() method works the same as select(), but it\n",
      "returns a generator instead of a list: The closest() method returns the nearest parent of a given Tag\n",
      "that matches a CSS selector, similar to Beautiful Soup’s\n",
      "find_parent() method: The match() method returns a Boolean depending on whether or not a\n",
      "specific Tag matches a selector: The filter() method returns the subset of a tag’s direct children\n",
      "that match a selector: The escape() method escapes CSS identifiers that would otherwise\n",
      "be invalid: If you’ve parsed XML that defines namespaces, you can use them in CSS\n",
      "selectors.: Beautiful Soup tries to use namespace prefixes that make sense based\n",
      "on what it saw while parsing the document, but you can always provide\n",
      "your own dictionary of abbreviations: The .css property was added in Beautiful Soup 4.12.0. Prior to this,\n",
      "only the .select() and .select_one() convenience methods were\n",
      "supported. The Soup Sieve integration was added in Beautiful Soup 4.7.0. Earlier\n",
      "versions had the .select() method, but only the most commonly-used\n",
      "CSS selectors were supported. Beautiful Soup’s main strength is in searching the parse tree, but you\n",
      "can also modify the tree and write your changes as a new HTML or XML\n",
      "document. I covered this earlier, in Tag.attrs, but it bears repeating. You\n",
      "can rename a tag, change the values of its attributes, add new\n",
      "attributes, and delete attributes: If you set a tag’s .string attribute to a new string, the tag’s contents are\n",
      "replaced with that string: Be careful: if the tag contained other tags, they and all their\n",
      "contents will be destroyed. You can add to a tag’s contents with Tag.append(). It works just\n",
      "like calling .append() on a Python list: Starting in Beautiful Soup 4.7.0, Tag also supports a method\n",
      "called .extend(), which adds every element of a list to a Tag,\n",
      "in order: If you need to add a string to a document, no problem–you can pass a\n",
      "Python string in to append(), or you can call the NavigableString\n",
      "constructor: If you want to create a comment or some other subclass of\n",
      "NavigableString, just call the constructor: (This is a new feature in Beautiful Soup 4.4.0.) What if you need to create a whole new tag? The best solution is to\n",
      "call the factory method BeautifulSoup.new_tag(): Only the first argument, the tag name, is required. Tag.insert() is just like Tag.append(), except the new element\n",
      "doesn’t necessarily go at the end of its parent’s\n",
      ".contents. It’ll be inserted at whatever numeric position you\n",
      "say. It works just like .insert() on a Python list: The insert_before() method inserts tags or strings immediately\n",
      "before something else in the parse tree: The insert_after() method inserts tags or strings immediately\n",
      "following something else in the parse tree: Tag.clear() removes the contents of a tag: PageElement.extract() removes a tag or string from the tree. It\n",
      "returns the tag or string that was extracted: At this point you effectively have two parse trees: one rooted at the\n",
      "BeautifulSoup object you used to parse the document, and one rooted\n",
      "at the tag that was extracted. You can go on to call extract() on\n",
      "a child of the element you extracted: Tag.decompose() removes a tag from the tree, then completely\n",
      "destroys it and its contents: The behavior of a decomposed Tag or NavigableString is not\n",
      "defined and you should not use it for anything. If you’re not sure\n",
      "whether something has been decomposed, you can check its\n",
      ".decomposed property (new in Beautiful Soup 4.9.0): PageElement.replace_with() extracts a tag or string from the tree,\n",
      "then replaces it with one or more tags or strings of your choice: replace_with() returns the tag or string that got replaced, so\n",
      "that you can examine it or add it back to another part of the tree. The ability to pass multiple arguments into replace_with() is new\n",
      "in Beautiful Soup 4.10.0. PageElement.wrap() wraps an element in the Tag object you specify. It\n",
      "returns the new wrapper: This method is new in Beautiful Soup 4.0.5. Tag.unwrap() is the opposite of wrap(). It replaces a tag with\n",
      "whatever’s inside that tag. It’s good for stripping out markup: Like replace_with(), unwrap() returns the tag\n",
      "that was replaced. After calling a bunch of methods that modify the parse tree, you may end up\n",
      "with two or more NavigableString objects next to each other.\n",
      "Beautiful Soup doesn’t have any problems with this, but since it can’t happen\n",
      "in a freshly parsed document, you might not expect behavior like the\n",
      "following: You can call Tag.smooth() to clean up the parse tree by consolidating adjacent strings: This method is new in Beautiful Soup 4.8.0. The prettify() method will turn a Beautiful Soup parse tree into a\n",
      "nicely formatted Unicode string, with a separate line for each\n",
      "tag and each string: You can call prettify() on the top-level BeautifulSoup object,\n",
      "or on any of its Tag objects: Since it adds whitespace (in the form of newlines), prettify()\n",
      "changes the meaning of an HTML document and should not be used to\n",
      "reformat one. The goal of prettify() is to help you visually\n",
      "understand the structure of the documents you work with. If you just want a string, with no fancy formatting, you can call\n",
      "str() on a BeautifulSoup object, or on a Tag within it: The str() function returns a string encoded in UTF-8. See\n",
      "Encodings for other options. You can also call encode() to get a bytestring, and decode()\n",
      "to get Unicode. If you give Beautiful Soup a document that contains HTML entities like\n",
      "“&lquot;”, they’ll be converted to Unicode characters: If you then convert the document to a bytestring, the Unicode characters\n",
      "will be encoded as UTF-8. You won’t get the HTML entities back: By default, the only characters that are escaped upon output are bare\n",
      "ampersands and angle brackets. These get turned into “&amp;”, “&lt;”,\n",
      "and “&gt;”, so that Beautiful Soup doesn’t inadvertently generate\n",
      "invalid HTML or XML: You can change this behavior by providing a value for the\n",
      "formatter argument to prettify(), encode(), or\n",
      "decode(). Beautiful Soup recognizes five possible values for\n",
      "formatter. The default is formatter=\"minimal\". Strings will only be processed\n",
      "enough to ensure that Beautiful Soup generates valid HTML/XML: If you pass in formatter=\"html\", Beautiful Soup will convert\n",
      "Unicode characters to HTML entities whenever possible: If you pass in formatter=\"html5\", it’s similar to\n",
      "formatter=\"html\", but Beautiful Soup will\n",
      "omit the closing slash in HTML void tags like “br”: In addition, any attributes whose values are the empty string\n",
      "will become HTML-style Boolean attributes: (This behavior is new as of Beautiful Soup 4.10.0.) If you pass in formatter=None, Beautiful Soup will not modify\n",
      "strings at all on output. This is the fastest option, but it may lead\n",
      "to Beautiful Soup generating invalid HTML/XML, as in these examples: If you need more sophisticated control over your output, you can\n",
      "instantiate one of Beautiful Soup’s formatter classes and pass that\n",
      "object in as formatter. Used to customize the formatting rules for HTML documents. Here’s a formatter that converts strings to uppercase, whether they\n",
      "occur in a string object or an attribute value: Here’s a formatter that increases the indentation width when pretty-printing: Used to customize the formatting rules for XML documents. Subclassing HTMLFormatter or XMLFormatter will\n",
      "give you even more control over the output. For example, Beautiful\n",
      "Soup sorts the attributes in every tag by default: To turn this off, you can subclass the Formatter.attributes()\n",
      "method, which controls which attributes are output and in what\n",
      "order. This implementation also filters out the attribute called “m”\n",
      "whenever it appears: One last caveat: if you create a CData object, the text inside\n",
      "that object is always presented exactly as it appears, with no\n",
      "formatting. Beautiful Soup will call your entity substitution\n",
      "function, just in case you’ve written a custom function that counts\n",
      "all the strings in the document or something, but it will ignore the\n",
      "return value: If you only want the human-readable text inside a document or tag, you can use the\n",
      "get_text() method. It returns all the text in a document or\n",
      "beneath a tag, as a single Unicode string: You can specify a string to be used to join the bits of text\n",
      "together: You can tell Beautiful Soup to strip whitespace from the beginning and\n",
      "end of each bit of text: But at that point you might want to use the .stripped_strings\n",
      "generator instead, and process the text yourself: As of Beautiful Soup version 4.9.0, when lxml or html.parser are in\n",
      "use, the contents of <script>, <style>, and <template>\n",
      "tags are generally not considered to be ‘text’, since those tags are not part of\n",
      "the human-visible content of the page. As of Beautiful Soup version 4.10.0, you can call get_text(),\n",
      ".strings, or .stripped_strings on a NavigableString object. It will\n",
      "either return the object itself, or nothing, so the only reason to do\n",
      "this is when you’re iterating over a mixed list. If you just need to parse some HTML, you can dump the markup into the\n",
      "BeautifulSoup constructor, and it’ll probably be fine. Beautiful\n",
      "Soup will pick a parser for you and parse the data. But there are a\n",
      "few additional arguments you can pass in to the constructor to change\n",
      "which parser is used. The first argument to the BeautifulSoup constructor is a string or\n",
      "an open filehandle—the source of the markup you want parsed. The second\n",
      "argument is how you’d like the markup parsed. If you don’t specify anything, you’ll get the best HTML parser that’s\n",
      "installed. Beautiful Soup ranks lxml’s parser as being the best, then\n",
      "html5lib’s, then Python’s built-in parser. You can override this by\n",
      "specifying one of the following: What type of markup you want to parse. Currently supported values are\n",
      "“html”, “xml”, and “html5”. The name of the parser library you want to use. Currently supported\n",
      "options are “lxml”, “html5lib”, and “html.parser” (Python’s\n",
      "built-in HTML parser). The section Installing a parser contrasts the supported parsers. If you don’t have an appropriate parser installed, Beautiful Soup will\n",
      "ignore your request and pick a different parser. Right now, the only\n",
      "supported XML parser is lxml. If you don’t have lxml installed, asking\n",
      "for an XML parser won’t give you one, and asking for “lxml” won’t work\n",
      "either. Beautiful Soup presents the same interface to a number of different\n",
      "parsers, but each parser is different. Different parsers will create\n",
      "different parse trees from the same document. The biggest differences\n",
      "are between the HTML parsers and the XML parsers. Here’s a short\n",
      "document, parsed as HTML using the parser that comes with Python: Since a standalone <b/> tag is not valid HTML, html.parser turns it into\n",
      "a <b></b> tag pair. Here’s the same document parsed as XML (running this requires that you\n",
      "have lxml installed). Note that the standalone <b/> tag is left alone, and\n",
      "that the document is given an XML declaration instead of being put\n",
      "into an <html> tag.: There are also differences between HTML parsers. If you give Beautiful\n",
      "Soup a perfectly-formed HTML document, these differences won’t\n",
      "matter. One parser will be faster than another, but they’ll all give\n",
      "you a data structure that looks exactly like the original HTML\n",
      "document. But if the document is not perfectly-formed, different parsers will\n",
      "give different results. Here’s a short, invalid document parsed using\n",
      "lxml’s HTML parser. Note that the <a> tag gets wrapped in <body> and\n",
      "<html> tags, and the dangling </p> tag is simply ignored: Here’s the same document parsed using html5lib: Instead of ignoring the dangling </p> tag, html5lib pairs it with an\n",
      "opening <p> tag. html5lib also adds an empty <head> tag; lxml didn’t\n",
      "bother. Here’s the same document parsed with Python’s built-in HTML\n",
      "parser: Like lxml, this parser ignores the closing </p> tag. Unlike\n",
      "html5lib or lxml, this parser makes no attempt to create a\n",
      "well-formed HTML document by adding <html> or <body> tags. Since the document “<a></p>” is invalid, none of these techniques is\n",
      "the ‘correct’ way to handle it. The html5lib parser uses techniques\n",
      "that are part of the HTML5 standard, so it has the best claim on being\n",
      "the ‘correct’ way, but all three techniques are legitimate. Differences between parsers can affect your script. If you’re planning\n",
      "on distributing your script to other people, or running it on multiple\n",
      "machines, you should specify a parser in the BeautifulSoup\n",
      "constructor. That will reduce the chances that your users parse a\n",
      "document differently from the way you parse it. Any HTML or XML document is written in a specific encoding like ASCII\n",
      "or UTF-8. But when you load that document into Beautiful Soup, you’ll\n",
      "discover it’s been converted to Unicode: It’s not magic. (That sure would be nice.) Beautiful Soup uses a\n",
      "sub-library called Unicode, Dammit to detect a document’s encoding\n",
      "and convert it to Unicode. The autodetected encoding is available as\n",
      "the .original_encoding attribute of the BeautifulSoup object: Unicode, Dammit guesses correctly most of the time, but sometimes it\n",
      "makes mistakes. Sometimes it guesses correctly, but only after a\n",
      "byte-by-byte search of the document that takes a very long time. If\n",
      "you happen to know a document’s encoding ahead of time, you can avoid\n",
      "mistakes and delays by passing it to the BeautifulSoup constructor\n",
      "as from_encoding. Here’s a document written in ISO-8859-8. The document is so short that\n",
      "Unicode, Dammit can’t get a lock on it, and misidentifies it as\n",
      "ISO-8859-7: We can fix this by passing in the correct from_encoding: If you don’t know what the correct encoding is, but you know that\n",
      "Unicode, Dammit is guessing wrong, you can pass the wrong guesses in\n",
      "as exclude_encodings: Windows-1255 isn’t 100% correct, but that encoding is a compatible\n",
      "superset of ISO-8859-8, so it’s close enough. (exclude_encodings\n",
      "is a new feature in Beautiful Soup 4.4.0.) In rare cases (usually when a UTF-8 document contains text written in\n",
      "a completely different encoding), the only way to get Unicode may be\n",
      "to replace some characters with the special Unicode character\n",
      "“REPLACEMENT CHARACTER” (U+FFFD, �). If Unicode, Dammit needs to do\n",
      "this, it will set the .contains_replacement_characters attribute\n",
      "to True on the UnicodeDammit or BeautifulSoup object. This\n",
      "lets you know that the Unicode representation is not an exact\n",
      "representation of the original–some data was lost. If a document\n",
      "contains �, but .contains_replacement_characters is False,\n",
      "you’ll know that the � was there originally (as it is in this\n",
      "paragraph) and doesn’t stand in for missing data. When you write out an output document from Beautiful Soup, you get a UTF-8\n",
      "document, even if the input document wasn’t in UTF-8 to begin with. Here’s a\n",
      "document written in the Latin-1 encoding: Note that the <meta> tag has been rewritten to reflect the fact that\n",
      "the document is now in UTF-8. If you don’t want UTF-8, you can pass an encoding into prettify(): You can also call encode() on the BeautifulSoup object, or any\n",
      "element in the soup, just as if it were a Python string: Any characters that can’t be represented in your chosen encoding will\n",
      "be converted into numeric XML entity references. Here’s a document\n",
      "that includes the Unicode character SNOWMAN: The SNOWMAN character can be part of a UTF-8 document (it looks like\n",
      "☃), but there’s no representation for that character in ISO-Latin-1 or\n",
      "ASCII, so it’s converted into “&#9731” for those encodings: You can use Unicode, Dammit without using Beautiful Soup. It’s useful\n",
      "whenever you have data in an unknown encoding and you just want it to\n",
      "become Unicode: Unicode, Dammit’s guesses will get a lot more accurate if you install\n",
      "one of these Python libraries: charset-normalizer, chardet, or\n",
      "cchardet. The more data you give Unicode, Dammit, the more\n",
      "accurately it will guess. If you have your own suspicions as to what\n",
      "the encoding might be, you can pass them in as a list: Unicode, Dammit has two special features that Beautiful Soup doesn’t\n",
      "use. You can use Unicode, Dammit to convert Microsoft smart quotes to HTML or XML\n",
      "entities: You can also convert Microsoft smart quotes to ASCII quotes: Hopefully you’ll find this feature useful, but Beautiful Soup doesn’t\n",
      "use it. Beautiful Soup prefers the default behavior, which is to\n",
      "convert Microsoft smart quotes to Unicode characters along with\n",
      "everything else: Sometimes a document is mostly in UTF-8, but contains Windows-1252\n",
      "characters such as (again) Microsoft smart quotes. This can happen\n",
      "when a website includes data from multiple sources. You can use\n",
      "UnicodeDammit.detwingle() to turn such a document into pure\n",
      "UTF-8. Here’s a simple example: This document is a mess. The snowmen are in UTF-8 and the quotes are\n",
      "in Windows-1252. You can display the snowmen or the quotes, but not\n",
      "both: Decoding the document as UTF-8 raises a UnicodeDecodeError, and\n",
      "decoding it as Windows-1252 gives you gibberish. Fortunately,\n",
      "UnicodeDammit.detwingle() will convert the string to pure UTF-8,\n",
      "allowing you to decode it to Unicode and display the snowmen and quote\n",
      "marks simultaneously: UnicodeDammit.detwingle() only knows how to handle Windows-1252\n",
      "embedded in UTF-8 (or vice versa, I suppose), but this is the most\n",
      "common case. Note that you must know to call UnicodeDammit.detwingle() on your\n",
      "data before passing it into BeautifulSoup or the UnicodeDammit\n",
      "constructor. Beautiful Soup assumes that a document has a single\n",
      "encoding, whatever it might be. If you pass it a document that\n",
      "contains both UTF-8 and Windows-1252, it’s likely to think the whole\n",
      "document is Windows-1252, and the document will come out looking like\n",
      "â˜ƒâ˜ƒâ˜ƒ“I like snowmen!”. UnicodeDammit.detwingle() is new in Beautiful Soup 4.1.0. The html.parser and html5lib parsers can keep track of where in\n",
      "the original document each Tag was found. You can access this\n",
      "information as Tag.sourceline (line number) and Tag.sourcepos\n",
      "(position of the start tag within a line): Note that the two parsers mean slightly different things by\n",
      "sourceline and sourcepos. For html.parser, these numbers\n",
      "represent the position of the initial less-than sign. For html5lib,\n",
      "these numbers represent the position of the final greater-than sign: You can shut off this feature by passing store_line_numbers=False\n",
      "into the BeautifulSoup constructor: This feature is new in 4.8.1, and the parsers based on lxml don’t\n",
      "support it. Beautiful Soup says that two NavigableString or Tag objects\n",
      "are equal when they represent the same HTML or XML markup, even if their\n",
      "attributes are in a different order or they live in different parts of the\n",
      "object tree. In this example, the two <b> tags are treated as equal, because\n",
      "they both look like “<b>pizza</b>”: If you want to see whether two variables refer to exactly the same\n",
      "object, use is: You can use copy.copy() to create a copy of any Tag or\n",
      "NavigableString: The copy is considered equal to the original, since it represents the\n",
      "same markup as the original, but it’s not the same object: The only real difference is that the copy is completely detached from\n",
      "the original Beautiful Soup object tree, just as if extract() had\n",
      "been called on it: This is because two different Tag objects can’t occupy the same\n",
      "space at the same time. Beautiful Soup offers a number of ways to customize how the parser\n",
      "treats incoming HTML and XML. This section covers the most commonly\n",
      "used customization techniques. Let’s say you want to use Beautiful Soup to look at a document’s <a>\n",
      "tags. It’s a waste of time and memory to parse the entire document and\n",
      "then go over it again looking for <a> tags. It would be much faster to\n",
      "ignore everything that wasn’t an <a> tag in the first place. The\n",
      "SoupStrainer class allows you to choose which parts of an incoming\n",
      "document are parsed. You just create a SoupStrainer and pass it in\n",
      "to the BeautifulSoup constructor as the parse_only argument. (Note that this feature won’t work if you’re using the html5lib parser.\n",
      "If you use html5lib, the whole document will be parsed, no\n",
      "matter what. This is because html5lib constantly rearranges the parse\n",
      "tree as it works, and if some part of the document didn’t actually\n",
      "make it into the parse tree, it’ll crash. To avoid confusion, in the\n",
      "examples below I’ll be forcing Beautiful Soup to use Python’s\n",
      "built-in parser.) The SoupStrainer class takes the same arguments as a typical\n",
      "method from Searching the tree: name, attrs, string, and **kwargs. Here are\n",
      "three SoupStrainer objects: I’m going to bring back the “three sisters” document one more time,\n",
      "and we’ll see what the document looks like when it’s parsed with these\n",
      "three SoupStrainer objects: The SoupStrainer behavior is as follows: When a tag matches, it is kept (including all its contents, whether they also\n",
      "match or not). When a tag does not match, the tag itself is not kept, but parsing continues\n",
      "into its contents to look for other tags that do match. You can also pass a SoupStrainer into any of the methods covered\n",
      "in Searching the tree. This probably isn’t terribly useful, but I\n",
      "thought I’d mention it: In an HTML document, an attribute like class is given a list of\n",
      "values, and an attribute like id is given a single value, because\n",
      "the HTML specification treats those attributes differently: You can turn this off by passing in\n",
      "multi_valued_attributes=None. Than all attributes will be given a\n",
      "single value: You can customize this behavior quite a bit by passing in a\n",
      "dictionary for multi_valued_attributes. If you need this, look at\n",
      "HTMLTreeBuilder.DEFAULT_CDATA_LIST_ATTRIBUTES to see the\n",
      "configuration Beautiful Soup uses by default, which is based on the\n",
      "HTML specification. (This is a new feature in Beautiful Soup 4.8.0.) When using the html.parser parser, you can use the\n",
      "on_duplicate_attribute constructor argument to customize what\n",
      "Beautiful Soup does when it encounters a tag that defines the same\n",
      "attribute more than once: The default behavior is to use the last value found for the tag: With on_duplicate_attribute='ignore' you can tell Beautiful Soup\n",
      "to use the first value found and ignore the rest: (lxml and html5lib always do it this way; their behavior can’t be\n",
      "configured from within Beautiful Soup.) If you need more control, you can pass in a function that’s called on each\n",
      "duplicate value: (This is a new feature in Beautiful Soup 4.9.1.) When a parser tells Beautiful Soup about a tag or a string, Beautiful\n",
      "Soup will instantiate a Tag or NavigableString object to\n",
      "contain that information. Instead of that default behavior, you can\n",
      "tell Beautiful Soup to instantiate subclasses of Tag or\n",
      "NavigableString, subclasses you define with custom behavior: This can be useful when incorporating Beautiful Soup into a test\n",
      "framework. (This is a new feature in Beautiful Soup 4.8.1.) If you’re having trouble understanding what Beautiful Soup does to a\n",
      "document, pass the document into the diagnose() function. (This function is new in\n",
      "Beautiful Soup 4.2.0.) Beautiful Soup will print out a report showing\n",
      "you how different parsers handle the document, and tell you if you’re\n",
      "missing a parser that Beautiful Soup could be using: Just looking at the output of diagnose() might show you how to solve the\n",
      "problem. Even if not, you can paste the output of diagnose() when\n",
      "asking for help. There are two different kinds of parse errors. There are crashes,\n",
      "where you feed a document to Beautiful Soup and it raises an\n",
      "exception (usually an HTMLParser.HTMLParseError). And there is\n",
      "unexpected behavior, where a Beautiful Soup parse tree looks a lot\n",
      "different than the document used to create it. These problems are almost never problems with Beautiful Soup itself.\n",
      "This is not because Beautiful Soup is an amazingly well-written piece\n",
      "of software. It’s because Beautiful Soup doesn’t include any parsing\n",
      "code. Instead, it relies on external parsers. If one parser isn’t\n",
      "working on a certain document, the best solution is to try a different\n",
      "parser. See Installing a parser for details and a parser\n",
      "comparison. If this doesn’t help, you might need to inspect the\n",
      "document tree found inside the BeautifulSoup object, to see where\n",
      "the markup you’re looking for actually ended up. SyntaxError: Invalid syntax (on the line ROOT_TAG_NAME =\n",
      "'[document]'): Caused by running an old Python 2 version of\n",
      "Beautiful Soup under Python 3, without converting the code. ImportError: No module named HTMLParser - Caused by running an old\n",
      "Python 2 version of Beautiful Soup under Python 3. ImportError: No module named html.parser - Caused by running the\n",
      "Python 3 version of Beautiful Soup under Python 2. ImportError: No module named BeautifulSoup - Caused by running\n",
      "Beautiful Soup 3 code in an environment that doesn’t have BS3\n",
      "installed. Or, by writing Beautiful Soup 4 code without knowing that\n",
      "the package name has changed to bs4. ImportError: No module named bs4 - Caused by running Beautiful\n",
      "Soup 4 code in an environment that doesn’t have BS4 installed. By default, Beautiful Soup parses documents as HTML. To parse a\n",
      "document as XML, pass in “xml” as the second argument to the\n",
      "BeautifulSoup constructor: You’ll need to have lxml installed. If your script works on one computer but not another, or in one\n",
      "virtual environment but not another, or outside the virtual\n",
      "environment but not inside, it’s probably because the two\n",
      "environments have different parser libraries available. For example,\n",
      "you may have developed the script on a computer that has lxml\n",
      "installed, and then tried to run it on a computer that only has\n",
      "html5lib installed. See Differences between parsers for why this\n",
      "matters, and fix the problem by mentioning a specific parser library\n",
      "in the BeautifulSoup constructor. Because HTML tags and attributes are case-insensitive, all three HTML\n",
      "parsers convert tag and attribute names to lowercase. That is, the\n",
      "markup <TAG></TAG> is converted to <tag></tag>. If you want to\n",
      "preserve mixed-case or uppercase tags and attributes, you’ll need to\n",
      "parse the document as XML. UnicodeEncodeError: 'charmap' codec can't encode character\n",
      "'\\xfoo' in position bar (or just about any other\n",
      "UnicodeEncodeError) - This problem shows up in two main\n",
      "situations. First, when you try to print a Unicode character that\n",
      "your console doesn’t know how to display. (See this page on the\n",
      "Python wiki for help.)\n",
      "Second, when you’re writing to a file and you pass in a Unicode\n",
      "character that’s not supported by your default encoding. In this\n",
      "case, the simplest solution is to explicitly encode the Unicode\n",
      "string into UTF-8 with u.encode(\"utf8\"). KeyError: [attr] - Caused by accessing tag['attr'] when the\n",
      "tag in question doesn’t define the attr attribute. The most\n",
      "common errors are KeyError: 'href' and KeyError: 'class'.\n",
      "Use tag.get('attr') if you’re not sure attr is\n",
      "defined, just as you would with a Python dictionary. AttributeError: 'ResultSet' object has no attribute 'foo' - This\n",
      "usually happens because you expected find_all() to return a\n",
      "single tag or string. But find_all() returns a list of tags\n",
      "and strings–a ResultSet object. You need to iterate over the\n",
      "list and look at the .foo of each one. Or, if you really only\n",
      "want one result, you need to use find() instead of\n",
      "find_all(). AttributeError: 'NoneType' object has no attribute 'foo' - This\n",
      "usually happens because you called find() and then tried to\n",
      "access the .foo` attribute of the result. But in your case,\n",
      "find() didn’t find anything, so it returned None, instead of\n",
      "returning a tag or a string. You need to figure out why your\n",
      "find() call isn’t returning anything. AttributeError: 'NavigableString' object has no attribute\n",
      "'foo' - This usually happens because you’re treating a string as\n",
      "though it were a tag. You may be iterating over a list, expecting\n",
      "that it contains nothing but tags, when it actually contains both tags and\n",
      "strings. Beautiful Soup will never be as fast as the parsers it sits on top\n",
      "of. If response time is critical, if you’re paying for computer time\n",
      "by the hour, or if there’s any other reason why computer time is more\n",
      "valuable than programmer time, you should forget about Beautiful Soup\n",
      "and work directly atop lxml. That said, there are things you can do to speed up Beautiful Soup. If\n",
      "you’re not using lxml as the underlying parser, my advice is to\n",
      "start. Beautiful Soup parses documents\n",
      "significantly faster using lxml than using html.parser or html5lib. You can speed up encoding detection significantly by installing the\n",
      "cchardet library. Parsing only part of a document won’t save you much time parsing\n",
      "the document, but it can save a lot of memory, and it’ll make\n",
      "searching the document much faster. New translations of the Beautiful Soup documentation are greatly\n",
      "appreciated. Translations should be licensed under the MIT license,\n",
      "just like Beautiful Soup and its English documentation are. There are two ways of getting your translation into the main code base\n",
      "and onto the Beautiful Soup website: Create a branch of the Beautiful Soup repository, add your\n",
      "translation, and propose a merge with the main branch, the same\n",
      "as you would do with a proposed change to the source code. Send a message to the Beautiful Soup discussion group with a link to\n",
      "your translation, or attach your translation to the message. Use the Chinese or Brazilian Portuguese translations as your model. In\n",
      "particular, please translate the source file doc/source/index.rst,\n",
      "rather than the HTML version of the documentation. This makes it\n",
      "possible to publish the documentation in a variety of formats, not\n",
      "just HTML. Beautiful Soup 3 is the previous release series, and is no longer\n",
      "supported. Development of Beautiful Soup 3 stopped in 2012, and the\n",
      "package was completely discontinued in 2021. There’s no reason to\n",
      "install it unless you’re trying to get very old software to work, but\n",
      "it’s published through PyPi as BeautifulSoup: $ pip install BeautifulSoup You can also download a tarball of the final release, 3.2.2. If you ran pip install beautifulsoup or pip install\n",
      "BeautifulSoup, but your code doesn’t work, you installed Beautiful\n",
      "Soup 3 by mistake. You need to run pip install beautifulsoup4. The documentation for Beautiful Soup 3 is archived online. Most code written against Beautiful Soup 3 will work against Beautiful\n",
      "Soup 4 with one simple change. All you should have to do is change the\n",
      "package name from BeautifulSoup to bs4. So this: becomes this: If you get the ImportError “No module named BeautifulSoup”, your\n",
      "problem is that you’re trying to run Beautiful Soup 3 code, but you\n",
      "only have Beautiful Soup 4 installed. If you get the ImportError “No module named bs4”, your problem\n",
      "is that you’re trying to run Beautiful Soup 4 code, but you only\n",
      "have Beautiful Soup 3 installed. Although BS4 is mostly backward-compatible with BS3, most of its\n",
      "methods have been deprecated and given new names for PEP 8 compliance. There are numerous other\n",
      "renames and changes, and a few of them break backward compatibility. Here’s what you’ll need to know to convert your BS3 code and habits to BS4: Beautiful Soup 3 used Python’s SGMLParser, a module that was\n",
      "deprecated and removed in Python 3.0. Beautiful Soup 4 uses\n",
      "html.parser by default, but you can plug in lxml or html5lib and\n",
      "use that instead. See Installing a parser for a comparison. Since html.parser is not the same parser as SGMLParser, you\n",
      "may find that Beautiful Soup 4 gives you a different parse tree than\n",
      "Beautiful Soup 3 for the same markup. If you swap out html.parser\n",
      "for lxml or html5lib, you may find that the parse tree changes yet\n",
      "again. If this happens, you’ll need to update your scraping code to\n",
      "process the new tree. renderContents -> encode_contents replaceWith -> replace_with replaceWithChildren -> unwrap findAll -> find_all findAllNext -> find_all_next findAllPrevious -> find_all_previous findNext -> find_next findNextSibling -> find_next_sibling findNextSiblings -> find_next_siblings findParent -> find_parent findParents -> find_parents findPrevious -> find_previous findPreviousSibling -> find_previous_sibling findPreviousSiblings -> find_previous_siblings getText -> get_text nextSibling -> next_sibling previousSibling -> previous_sibling Some arguments to the Beautiful Soup constructor were renamed for the\n",
      "same reasons: BeautifulSoup(parseOnlyThese=...) -> BeautifulSoup(parse_only=...) BeautifulSoup(fromEncoding=...) -> BeautifulSoup(from_encoding=...) I renamed one method for compatibility with Python 3: Tag.has_key() -> Tag.has_attr() I renamed one attribute to use more accurate terminology: Tag.isSelfClosing -> Tag.is_empty_element I renamed three attributes to avoid using words that have special\n",
      "meaning to Python. Unlike the others, these changes are not backwards\n",
      "compatible. If you used these attributes in BS3, your code will break\n",
      "in BS4 until you change them. UnicodeDammit.unicode -> UnicodeDammit.unicode_markup Tag.next -> Tag.next_element Tag.previous -> Tag.previous_element These methods are left over from the Beautiful Soup 2 API. They’ve\n",
      "been deprecated since 2006 and should not be used at all: Tag.fetchNextSiblings Tag.fetchPreviousSiblings Tag.fetchPrevious Tag.fetchPreviousSiblings Tag.fetchParents Tag.findChild Tag.findChildren I gave the generators PEP 8-compliant names, and transformed them into\n",
      "properties: childGenerator() -> children nextGenerator() -> next_elements nextSiblingGenerator() -> next_siblings previousGenerator() -> previous_elements previousSiblingGenerator() -> previous_siblings recursiveChildGenerator() -> descendants parentGenerator() -> parents So instead of this: You can write this: (But the old code will still work.) Some of the generators used to yield None after they were done, and\n",
      "then stop. That was a bug. Now the generators just stop. There are two new generators, .strings and\n",
      ".stripped_strings. .strings yields\n",
      "NavigableString objects, and .stripped_strings yields Python\n",
      "strings that have had whitespace stripped. There is no longer a BeautifulStoneSoup class for parsing XML. To\n",
      "parse XML you pass in “xml” as the second argument to the\n",
      "BeautifulSoup constructor. For the same reason, the\n",
      "BeautifulSoup constructor no longer recognizes the isHTML\n",
      "argument. Beautiful Soup’s handling of empty-element XML tags has been\n",
      "improved. Previously when you parsed XML you had to explicitly say\n",
      "which tags were considered empty-element tags. The selfClosingTags\n",
      "argument to the constructor is no longer recognized. Instead,\n",
      "Beautiful Soup considers any empty tag to be an empty-element tag. If\n",
      "you add a child to an empty-element tag, it stops being an\n",
      "empty-element tag. An incoming HTML or XML entity is always converted into the\n",
      "corresponding Unicode character. Beautiful Soup 3 had a number of\n",
      "overlapping ways of dealing with entities, which have been\n",
      "removed. The BeautifulSoup constructor no longer recognizes the\n",
      "smartQuotesTo or convertEntities arguments. (Unicode,\n",
      "Dammit still has smart_quotes_to, but its default is now to turn\n",
      "smart quotes into Unicode.) The constants HTML_ENTITIES,\n",
      "XML_ENTITIES, and XHTML_ENTITIES have been removed, since they\n",
      "configure a feature (transforming some but not all entities into\n",
      "Unicode characters) that no longer exists. If you want to turn Unicode characters back into HTML entities on\n",
      "output, rather than turning them into UTF-8 characters, you need to\n",
      "use an output formatter. Tag.string now operates recursively. If tag A\n",
      "contains a single tag B and nothing else, then A.string is the same as\n",
      "B.string. (Previously, it was None.) Multi-valued attributes like class have lists of strings as\n",
      "their values, not simple strings. This may affect the way you search by CSS\n",
      "class. Tag objects now implement the __hash__ method, such that two\n",
      "Tag objects are considered equal if they generate the same\n",
      "markup. This may change your script’s behavior if you put Tag\n",
      "objects into a dictionary or set. If you pass one of the find* methods both string and\n",
      "a tag-specific argument like name, Beautiful Soup will\n",
      "search for tags that match your tag-specific criteria and whose\n",
      "Tag.string matches your string\n",
      "value. It will not find the strings themselves. Previously,\n",
      "Beautiful Soup ignored the tag-specific arguments and looked for\n",
      "strings. The BeautifulSoup constructor no longer recognizes the\n",
      "markupMassage argument. It’s now the parser’s responsibility to\n",
      "handle markup correctly. The rarely-used alternate parser classes like\n",
      "ICantBelieveItsBeautifulSoup and BeautifulSOAP have been\n",
      "removed. It’s now the parser’s decision how to handle ambiguous\n",
      "markup. The prettify() method now returns a Unicode string, not a bytestring. \n"
     ]
    }
   ],
   "source": [
    "print(docs[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size =1000\n",
    "chunk_overlap = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap, \n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
    ")\n",
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    separator = '\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "c_pages = c_splitter.split_text(docs[0].page_content)\n",
    "print(len(c_pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "r_pages = r_splitter.split_text(docs[0].page_content)\n",
    "print(len(r_pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Replace multiple newlines with a single newline\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    # Remove leading and trailing whitespaces\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1131, which is longer than the specified 1000\n",
      "Created a chunk of size 1429, which is longer than the specified 1000\n",
      "Created a chunk of size 1692, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "45\n",
      "Page 1:\n",
      "page_content='Eagle - Wikipedia\\nJump to content\\nMain menu\\nMain menu\\nmove to sidebar\\nhide\\n\\t\\tNavigation\\n\\t\\nMain pageContentsCurrent eventsRandom articleAbout WikipediaContact usDonate\\n\\t\\tContribute\\n\\t\\nHelpLearn to editCommunity portalRecent changesUpload file\\nSearch\\nSearch\\nAppearance\\nCreate account\\nLog in\\nPersonal tools\\n Create account Log in\\n\\t\\tPages for logged out editors learn more\\nContributionsTalk\\nContents\\nmove to sidebar\\nhide\\n(Top)\\n1\\nDescription\\n2\\nHabitat\\n3\\nDistribution\\n4\\nGroups\\nToggle Groups subsection\\n4.1\\nFish eagles\\n4.2\\nBooted eagles\\n4.3\\nSnake eagles\\n4.4\\nHarpy eagles\\n5\\nSpecies\\n6\\nIn culture\\nToggle In culture subsection\\n6.1\\nEtymology\\n6.2\\nReligion and spirituality\\n6.3\\nHeraldry\\n7\\nNotes\\n8\\nReferences\\n9\\nExternal links\\nToggle the table of contents\\nEagle\\n141 languages' metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 2:\n",
      "page_content='AfrikaansአማርኛअंगिकाÆngliscالعربيةAragonésAsturianuAymar aruتۆرکجهBasa Baliবাংলা閩南語 / Bân-lâm-gúБългарскиབོད་ཡིགBosanskiBrezhonegБуряадCatalàЧӑвашлаČeštinaChiShonaCymraegDagbanliDanskالدارجةDavvisámegiellaDeitschDeutschDiné bizaadEestiΕλληνικάЭрзяньEspañolEsperantoEuskaraEʋegbeفارسیFøroysktFrançaisFurlanGaeilgeGàidhligGalegoગુજરાતી𐌲𐌿𐍄𐌹𐍃𐌺客家語/Hak-kâ-ngî한국어HausaՀայերենहिन्दीHrvatskiBahasa HulontaloIdoBahasa IndonesiaעבריתJawaKabɩyɛಕನ್ನಡKapampanganकॉशुर / کٲشُرKernowekIkirundiKiswahiliKreyòl ayisyenKurdîЛаккуLatinaLatviešuЛезгиLietuviųLingua Franca NovaLombardMagyarМакедонскиMalagasyമലയാളംमराठीمصرىဘာသာမန်مازِرونیBahasa MelayuMinangkabauМонголမြန်မာဘာသာNāhuatlNederlandsनेपालीनेपाल भाषा日本語NordfriiskNorsk bokmålNorsk nynorskOccitanOʻzbekcha / ўзбекчаਪੰਜਾਬੀPangcahپنجابیپښتوPicardPlattdüütschPolskiPortuguêsQırımtatarcaRomânăРусскийसंस्कृतम्ScotsShqipSimple EnglishسنڌيСловѣньскъ / ⰔⰎⰑⰂⰡⰐⰠⰔⰍⰟSoomaaligaکوردیСрпски / srpskiSrpskohrvatski /' metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 3:\n",
      "page_content='EnglishسنڌيСловѣньскъ / ⰔⰎⰑⰂⰡⰐⰠⰔⰍⰟSoomaaligaکوردیСрпски / srpskiSrpskohrvatski / српскохрватскиSundaSuomiSvenskaTagalogதமிழ்TaqbaylitTayalతెలుగుไทยТоҷикӣᏣᎳᎩTsetsêhestâheseTürkçeУдмуртУкраїнськаاردوئۇيغۇرچە / UyghurcheTiếng ViệtVõroWalonWinaray吴语ייִדיש粵語中文ⵜⴰⵎⴰⵣⵉⵖⵜ ⵜⴰⵏⴰⵡⴰⵢⵜ' metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 4:\n",
      "page_content='Edit links\\nArticleTalk\\nEnglish\\nReadEditView history\\nTools\\nTools\\nmove to sidebar\\nhide\\n\\t\\tActions\\n\\t\\nReadEditView history\\n\\t\\tGeneral\\n\\t\\nWhat links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationCite this pageGet shortened URLDownload QR codeWikidata item\\n\\t\\tPrint/export\\n\\t\\nDownload as PDFPrintable version\\n\\t\\tIn other projects\\n\\t\\nWikimedia CommonsWikiquote\\nAppearance\\nmove to sidebar\\nhide\\nFrom Wikipedia, the free encyclopedia\\nLarge carnivore bird\\nThis article is about the bird. For other uses, see Eagle (disambiguation) and Eagles (disambiguation).\\nEagle\\nFrom left to right, top row first: golden eagle (Aquila chrysaetos), brown snake eagle (Circaetus cinereus), solitary eagle (Buteogallus solitarius), black eagle (Ictinaetus malaiensis) and African fish eagle (Haliaeetus vocifer).\\nScientific classification\\nDomain:\\nEukaryota\\nKingdom:\\nAnimalia\\nPhylum:\\nChordata\\nClass:\\nAves\\nOrder:\\nAccipitriformes\\nFamily:\\nAccipitridae\\nSpecies\\nSee text' metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 5:\n",
      "page_content='Domain:\\nEukaryota\\nKingdom:\\nAnimalia\\nPhylum:\\nChordata\\nClass:\\nAves\\nOrder:\\nAccipitriformes\\nFamily:\\nAccipitridae\\nSpecies\\nSee text\\nEagle is the common name for the golden eagle, bald eagle, and other  birds of prey in the family Accipitridae. Eagles belong to several groups of genera, some of which are closely related. True eagles comprise the genus Aquila. Most of the 68 species of eagles are from Eurasia and Africa.[1] Outside this area, just 14 species can be found—two in North America, nine in Central and South America, and three in Australia.\\nEagles are not a natural group but denote essentially any kind of bird of prey large enough to hunt sizeable (about 50\\xa0cm long or more overall) vertebrates.\\nDescription[edit]' metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 6:\n",
      "page_content='Description[edit]\\nEagles are large, powerfully-built birds of prey, with heavy heads and beaks. Even the smallest eagles, such as the booted eagle (Hieraaetus pennatus), which is comparable in size to a common buzzard (Buteo buteo) or red-tailed hawk (B. jamaicensis), have relatively longer and more evenly broad wings, and more direct, faster flight, despite the reduced size of their aerodynamic feathers. Most eagles are larger than any other raptors, apart from some vultures. The smallest species of eagle is the Great Nicobar serpent eagle (Spilornis klossi), at 450\\xa0g (1\\xa0lb) and 40\\xa0cm (16\\xa0in). The largest species are discussed below. Like all birds of prey, eagles have very large hooked beaks for ripping flesh from their prey, strong, muscular legs, and powerful talons.' metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 7:\n",
      "page_content=\"The beak is typically heavier than that of most other birds of prey. Eagles' eyes are extremely powerful. It is estimated that the wedge-tailed eagle has a visual acuity twice that of a typical human.[2][3][4] This acuity enables eagles to spot potential prey from a very long distance. This keen eyesight is primarily attributed to their extremely large pupils which ensure minimal diffraction (scattering) of the incoming light. Like most diurnal raptors, eagles have little ability to see ultraviolet light.[5] The female of all known species of eagles is larger than the male.[6][7]\\nEagles normally build their nests, called eyries, in tall trees or on high cliffs. Many species lay two eggs, but the older, larger chick frequently kills its younger sibling once it has hatched. The parents take no action to stop the killing.[8][9]\" metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 8:\n",
      "page_content=\"It is said[by whom?] that eagles fly above clouds but this is not true.[citation needed] Eagles fly during storms and glide from the wind's pressure. This saves the bird's energy.\" metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 9:\n",
      "page_content='Due to the size and power of many eagle species, they are ranked at the top of the food chain as apex predators in the avian world. The type of prey varies by genus. The Haliaeetus and Icthyophaga eagles prefer to capture fish, though the species in the former often capture various animals, especially other water birds, and are powerful kleptoparasites of other birds. The snake and serpent eagles of the genera Circaetus, Terathopius, and Spilornis predominantly prey on the great diversity of snakes found in the tropics of Africa and Asia. The eagles of the genus Aquila are often the top birds of prey in open habitats, taking almost any medium-sized vertebrate they can catch. Where Aquila eagles are absent, other eagles, such as the buteonine black-chested buzzard-eagle of South America, may assume the position of top raptorial predator in open areas. Many other eagles, including the species-rich genus Spizaetus, live predominantly in woodlands and forests. These eagles often target' metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 10:\n",
      "page_content='in open areas. Many other eagles, including the species-rich genus Spizaetus, live predominantly in woodlands and forests. These eagles often target various arboreal or ground-dwelling mammals and birds, which are often unsuspectingly ambushed in such dense, knotty environments. Hunting techniques differ among the species and genera, with some individual eagles having engaged in quite varied techniques based on their environment and prey at any given time. Most eagles grab prey without landing and take flight with it, so the prey can be carried to a perch and torn apart.[10]' metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 11:\n",
      "page_content='The bald eagle is noted for having flown with the heaviest load verified to be carried by any flying bird, since one eagle flew with a 6.8\\xa0kg (15\\xa0lb) mule deer fawn.[11][12] However, a few eagles may target prey considerably heavier than themselves; such prey is too heavy to fly with, thus it is either eaten at the site of the kill or taken in pieces back to a perch or nest. Golden and crowned eagles have killed ungulates weighing up to 30\\xa0kg (66\\xa0lb) and a martial eagle even killed a 37\\xa0kg (82\\xa0lb) duiker, 7–8 times heavier than the preying eagle.[10][13] Authors on birds David Allen Sibley, Pete Dunne, and Clay Sutton described the behavioral difference between hunting eagles and other birds of prey thus (in this case the bald and golden eagles as compared to other North American raptors):[14]' metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 12:\n",
      "page_content='They have at least one singular characteristic. It has been observed[by whom?] that most birds of prey look back over their shoulders before striking prey (or shortly thereafter); predation is after all a two-edged sword[further explanation needed]. All hawks seem to have this habit, from the smallest kestrel to the largest Ferruginous – but not the Eagles.' metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 13:\n",
      "page_content='Among the eagles are some of the largest birds of prey: only the condors and some of the Old World vultures are markedly larger. It is regularly[when?] debated[according to whom?] which should be considered the largest species of eagle. They could be measured variously in total length, body mass, or wingspan. Different lifestyle needs among various eagles result in variable measurements from species to species. For example, many forest-dwelling eagles, including the very large harpy eagle, have relatively short wingspans, a feature necessary for being able to maneuver in quick, short bursts through densely forested habitats.[10] Eagles in the genus Aquila, found almost exclusively in open country, are noted for their ability to soar, and have relatively long wings for their size.[10]' metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 14:\n",
      "page_content=\"These lists of the top five eagles are based on weight, length, and wingspan, respectively. Unless otherwise noted by reference, the figures listed are the median reported for each measurement in the guide Raptors of the World[15] in which only measurements that could be personally verified by the authors were listed.[10]\\nRank\\nCommon name\\nScientific name\\nBody mass\\n1\\nPhilippine eagle\\nPithecophaga jefferyi\\n8.3\\xa0kg (18+1⁄4\\xa0lb) (Average Weight)\\n2\\nSteller's sea eagle\\nHaliaeetus pelagicus\\n7.4\\xa0kg (16+1⁄4\\xa0lb) (Average Weight)\\n3\\nHarpy eagle\\nHarpia harpyja\\n6.35\\xa0kg (14\\xa0lb) (Average Weight)\\n4\\nWhite-tailed eagle\\nHaliaeetus albicilla\\n4.8\\xa0kg (10+1⁄2\\xa0lb) (Average Weight) [16]\\n5\\nMartial eagle\\nPolemaetus bellicosus\\n4.6\\xa0kg (10+1⁄4\\xa0lb)[16] (Average Weight)\\nRank\\nCommon name\\nScientific name\\nTotal length\\n1\\nPhilippine eagle\\nPithecophaga jefferyi\\n100\\xa0cm (3\\xa0ft 3\\xa0in)[17]\\n2\\nHarpy eagle\\nHarpia harpyja\\n98.5\\xa0cm (3\\xa0ft 3\\xa0in)\\n3\\nWedge-tailed eagle\\nAquila audax\\n95.5\\xa0cm (3\\xa0ft 2\\xa0in)\\n4\\nSteller's sea eagle\" metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 15:\n",
      "page_content=\"100\\xa0cm (3\\xa0ft 3\\xa0in)[17]\\n2\\nHarpy eagle\\nHarpia harpyja\\n98.5\\xa0cm (3\\xa0ft 3\\xa0in)\\n3\\nWedge-tailed eagle\\nAquila audax\\n95.5\\xa0cm (3\\xa0ft 2\\xa0in)\\n4\\nSteller's sea eagle\\nHaliaeetus pelagicus\\n95\\xa0cm (3\\xa0ft 1\\xa0in)\\n5\\nCrowned eagle\\nStephanoaetus coronatus\\n87.5\\xa0cm (2\\xa0ft 10\\xa0in)\\nRank\\nCommon name\\nScientific name\\nMedian wingspan\\n1\\nWhite-tailed eagle\\nHaliaeetus albicilla\\n218.5\\xa0cm (7\\xa0ft 2\\xa0in)\\n2\\nSteller's sea eagle\\nHaliaeetus pelagicus\\n212.5\\xa0cm (7\\xa0ft 0\\xa0in)\\n3\\nWedge-tailed eagle\\nAquila audax\\n210\\xa0cm (6\\xa0ft 11\\xa0in)[18][19]\\n4\\nGolden eagle\\nAquila chrysaetos\\n207\\xa0cm (6\\xa0ft 9\\xa0in)\\n5\\nMartial eagle\\nPolemaetus bellicosus\\n206.5\\xa0cm (6\\xa0ft 9\\xa0in)\\nHabitat[edit]\\nThe eagles are generally distributed in all types of habitats and nearly all parts of the world. The birds can be found in northern tundra to tropical rainforests and deserts. In North America, bald eagles and golden eagles are very common.\\nDistribution[edit]\\nAustralasian\" metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 16:\n",
      "page_content=\"Distribution[edit]\\nAustralasian\\nAustralia: wedge-tailed eagle (range extends into southern New Guinea), white-bellied sea-eagle (range extends into Asia), little eagle.\\nNew Guinea: Papuan eagle, white-bellied sea-eagle, pygmy eagle.\\nNearctic (USA and Canada): golden eagle (also found in Palearctic), bald eagle.\\nNeotropical (Central and South America): Spizaetus (four species), solitary eagles (two spp.), harpy eagle, crested eagle, black-chested buzzard-eagle.\\nPalearctic (Europe, Northern Africa, Asia without South Asia and Southeast Asia)\\nEurasia: Golden eagle,[20] White-tailed eagle.\\nSubsaharan Africa: African fish eagle, Martial Eagle, Crowned eagle, Verreaux's eagle, Tawny eagle, Long-crested eagle\\nGroups[edit]\\nEagles are often informally divided into four groups.[a][22]\" metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 17:\n",
      "page_content='Groups[edit]\\nEagles are often informally divided into four groups.[a][22]\\nThe snake eagles are placed in the subfamily Circaetinae. The fish eagles, booted eagles, and harpy eagles have traditionally been placed in the subfamily Buteoninae together with the buzzard-hawks (buteonine hawks) and harriers. Some authors may treat these groups as tribes of the Buteoninae; Lerner & Mindell[23] proposed separating the eagle groups into their own subfamilies of Accipitridae.\\nFish eagles[edit]\\nSea eagles or fish eagles take fish as a large part of their diets, either fresh or as carrion.\\nProposed subfamily Haliaeetinae. Genera: Haliaeetus, Icthyophaga.\\nSome authors include Gypohierax angolensis, the \"vulturine fish eagle\" (also called the palm-nut vulture) in this group.[22] However, genetic analyses indicate it is related to a grouping of Neophron–Gypaetus–Eutriorchis (Egyptian vulture, bearded vulture (lammergeier), and Madagascar serpent eagle).[24]' metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 18:\n",
      "page_content='The fish eagles have a close genetic relationship with Haliastur and Milvus; the whole group is only distantly related to the Buteo group.[24]\\nFish eagles exist in every continent throughout the world, except for South America.[25]\\nAlthough fish eagles can be found in many different places around the world, they have been classified as \"Near Threatened\". Reasons such as overfishing, pollution, habitat destruction, and the use of pesticides have contributed to the species\\' rapid population drop.[26]\\nBooted eagles[edit]' metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 19:\n",
      "page_content='Booted eagles[edit]\\nThe booted eagle is a group of eagle that typically migrates across the Sahara Desert to Europe. It usually reaches Europe around the beginning of March and leaves by the end of September. It\\'s interesting to note that these types of eagles usually mate with the same partner and return to the same areas years later. Female booted eagles usually lay 1-4 eggs, which promptly hatch after 37 to 40 days. Researchers estimate that there are between 3600 to 6900 pairs of booted eagles in Europe, which are mostly situated in the Iberian Peninsula.[27]\\nFor the species Hieraaetus pennatus (Aquila pennata), see booted eagle.\\nMain article: Booted eagles\\nBooted eagle in flight\\nBooted eagles or \"true eagles\"[22][28] have feathered tarsi (lower legs).\\nTribe Aquililae or proposed subfamily Aquilinae. Genera: Aquila, Hieraaetus; Spizaetus, Oroaetus, Spizastur; Nisaetus;[24] Ictinaetus, Lophoaetus; Polemaetus; and Stephanoaetus.[22][28]' metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 20:\n",
      "page_content=\"See comments under eagle species for changes to the composition of these genera.\\nSnake eagles[edit]\\nMost snake or serpent eagles, as the name suggests, primarily prey on snakes.\\nSubfamily Circaetinae. Genera: Circaetus, Spilornis, Dryotriorchis, Terathopius.[22]\\nEutriorchis (subfamily Gypaetinae or Circaetinae).\\nDespite filling the niche of a snake eagle, genetic studies suggest that the Madagascar serpent eagle (Eutriorchis) is not related to them.[24]\\nOver several decades, a great deal of research has been done on the \\xa0Snake-eagle's diet, which is mainly made up of reptiles, especially snakes. When it comes to catching snakes, it is generally accepted that the bird exhibits generalist feeding behavior, which means it does not hunt down specific types of snakes but rather feeds on them depending on their availability in the wild.[29]\\nHarpy eagles[edit]\" metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 21:\n",
      "page_content='Harpy eagles[edit]\\nHarpy eagles[22] or \"giant forest eagles\"[21] are large eagles that inhabit tropical forests. The group contains two to six species, depending on the author. Although these birds occupy similar niches and have traditionally been grouped, they are not all related: the solitary eagles are related to the black hawks and the Philippine eagle to the snake eagles.\\nHarpy eagles (proposed subfamily Harpiinae)\\nHarpia harpyja, harpy eagle ― Central and South America.\\nMorphnus guianensis, crested eagle ― Central and South America.\\nHarpyopsis novaeguineae, Papuan eagle ― New Guinea.\\nPhilippine eagle\\nPithecophaga jefferyi, Philippine eagle ― Philippines.\\nSolitary eagles\\nChaco eagle or crowned solitary eagle, Buteogallus (formerly Harpyhaliaetus) coronatus ― South America.\\nSolitary eagle or montane solitary eagle, Buteogallus (formerly Harpyhaliaetus) solitarius ― South America.\\nSpecies[edit]\\nMartial eagle in Namibia' metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 22:\n",
      "page_content=\"Solitary eagle or montane solitary eagle, Buteogallus (formerly Harpyhaliaetus) solitarius ― South America.\\nSpecies[edit]\\nMartial eagle in Namibia\\nPhilippine eagle (Pithecophaga jefferyi) in Southern Philippines\\nWedge-tailed eagle in Australia\\nEastern imperial eagle in Israel\\nMajor new research into eagle taxonomy suggests that the important genera Aquila and Hieraaetus are not composed of nearest relatives, and it is likely that a reclassification of these genera will soon take place, with some species being moved to Lophaetus or Ictinaetus.[23]\\nBonelli's eagle and the African hawk-eagle have been moved from Hieraaetus to Aquila.\\nEither the greater spotted eagle and lesser spotted eagle should move from Aquila to join the long-crested eagle in Lophaetus, or, perhaps better, all three of these species should move to Ictinaetus with the black eagle.\\nThe steppe eagle and tawny eagle, once thought to be conspecific, are not even each other's nearest relatives.\\nFamily Accipitridae\" metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 23:\n",
      "page_content=\"The steppe eagle and tawny eagle, once thought to be conspecific, are not even each other's nearest relatives.\\nFamily Accipitridae\\nMain article: Accipitridae\\nSubfamily Buteoninae – hawks (buzzards), true eagles and seaeagles\\nGenus Geranoaetus\\nBlack-chested buzzard-eagle, Geranoaetus melanoleucus\\nGenus Harpyhaliaetus\\nChaco eagle, Buteogallus coronatus\\nSolitary eagle, H. solitarius\\nGenus Morphnus\\nCrested eagle, Morphnus guianensis\\nGenus Harpia\\nHarpy eagle, Harpia harpyja\\nGenus Pithecophaga\\nPhilippine eagle, Pithecophaga jefferyi\\nGenus Harpyopsis\\nPapuan eagle, Harpyopsis novaeguineae\\nGenus Spizaetus\\nBlack hawk-eagle, S. tyrannus\\nOrnate hawk-eagle, S. ornatus\\nBlack-and-white hawk-eagle, S. melanoleucus – formerly Spizastur\\nBlack-and-chestnut eagle, S. isidori – formerly Oroaetus\\nGenus Nisaetus – previously included in Spizaetus\\nChangeable hawk-eagle, N. cirrhatus\\nFlores hawk-eagle N. floris – earlier a subspecies, S. c. floris\\nSulawesi hawk-eagle, N. lanceolatus\" metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 24:\n",
      "page_content=\"Changeable hawk-eagle, N. cirrhatus\\nFlores hawk-eagle N. floris – earlier a subspecies, S. c. floris\\nSulawesi hawk-eagle, N. lanceolatus\\nMountain hawk-eagle, N. nipalensis\\nLegge's hawk-eagle, Nisaetus kelaarti – previously a race of S. nipalensis\\nBlyth's hawk-eagle, N. alboniger\\nJavan hawk-eagle, N. bartelsi\\n(Northern) Philippine hawk-eagle, N. philippensis\\nPinsker's hawk-eagle (Southern Philippine hawk-eagle), Nisaetus pinskeri – earlier S. philippensis pinskeri\\nWallace's hawk-eagle, N. nanus\\nGenus Lophaetus\\nLong-crested eagle, Lophaetus occipitalis – possibly belongs in Ictinaetus\\nGenus Stephanoaetus\\nCrowned eagle, Stephanoaetus coronatus\\nMalagasy crowned eagle, †Stephanoaetus mahery\\nGenus Polemaetus\\nMartial eagle, Polemaetus bellicosus\\nGenus Hieraaetus\\nAyres's hawk-eagle, H. ayresii\\nLittle eagle, H. morphnoides\\nPygmy eagle, H. weiskei – previously subspecies H. m. weiskei\\nBooted eagle, H. pennatus\\nHaast's eagle, †H. moorei\\nGenus Lophotriorchis\" metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 25:\n",
      "page_content=\"Pygmy eagle, H. weiskei – previously subspecies H. m. weiskei\\nBooted eagle, H. pennatus\\nHaast's eagle, †H. moorei\\nGenus Lophotriorchis\\nRufous-bellied eagle, L. kienerii A steppe eagle in Lahore Zoo, Pakistan\\nGenus Aquila\\nBonelli's eagle, Aquila fasciata – formerly Hieraaetus fasciatus\\nAfrican hawk-eagle, A. spilogaster – formerly in Hieraaetus\\nCassin's hawk-eagle, A. africana – formerly in Hieraaetus or Spizaetus genera\\nGolden eagle, A. chrysaetos\\nEastern imperial eagle, A. heliaca\\nSpanish imperial eagle A. adalberti\\nSteppe eagle, A. nipalensis\\nTawny eagle, A. rapax\\nGreater spotted eagle, A. clanga – to be moved to Lophaetus or Ictinaetus\\nLesser spotted eagle, A. pomarina – to be moved to Lophaetus or Ictinaetus\\nIndian spotted eagle, A. hastata – to be moved to Lophaetus or Ictinaetus\\nVerreaux's eagle, A. verreauxii\\nGurney's eagle, A. gurneyi\\nWahlberg's eagle, A. wahlbergi – to be moved to Hieraaetus\\nWedge-tailed eagle, A. audax\\nGenus Ictinaetus\\nBlack eagle, Ictinaetus malaiensis\" metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 26:\n",
      "page_content=\"Wahlberg's eagle, A. wahlbergi – to be moved to Hieraaetus\\nWedge-tailed eagle, A. audax\\nGenus Ictinaetus\\nBlack eagle, Ictinaetus malaiensis\\nGenus Haliaeetus\\nWhite-tailed eagle, Haliaeetus albicilla\\nBald eagle, H. leucocephalus\\nSteller's sea eagle, H. pelagicus\\nPallas' sea eagle, H. leucoryphus\\nGenus Icthyophaga\\nLesser fish eagle, Icthyophaga humilis\\nGrey-headed fish eagle, I. ichthyaetus\\nAfrican fish eagle, I. vocifer\\nWhite-bellied sea eagle, I. leucogaster\\nSanford's sea eagle, I. sanfordi\\nMadagascar fish eagle, I. vociferoides\\nShort-toed snake eagle in flight\\nSubfamily Circaetinae: snake-eagles\\nGenus Terathopius\\nBateleur, Terathopius ecaudatus\\nGenus Circaetus\\nShort-toed snake eagle, Circaetus gallicus\\nBeaudouin's snake eagle, Circaetus beaudouini\\nBlack-chested snake eagle, C. pectoralis\\nBrown snake eagle, C. cinereus\\nFasciated snake eagle, C. fasciolatus\\nWestern banded snake eagle, C. cinerascens\\nGenus Dryotriorchis\\nCongo serpent eagle, D. spectabilis\\nGenus Spilornis\" metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 27:\n",
      "page_content='Western banded snake eagle, C. cinerascens\\nGenus Dryotriorchis\\nCongo serpent eagle, D. spectabilis\\nGenus Spilornis\\nCrested serpent eagle, Spilornis cheela\\nCentral Nicobar serpent eagle, S. minimus (subspecies or species)\\nGreat Nicobar serpent eagle, S. klossi\\nMountain serpent eagle, S. kinabaluensis\\nSulawesi serpent eagle, S. rufipectus\\nPhilippine serpent eagle, S. holospilus\\nAndaman serpent eagle, S. elgini\\nGenus Eutriorchis\\nMadagascar serpent eagle, Eutriorchis astur\\nIn culture[edit]\\nEagles, a Chinese Ming period painting; Located at the National Palace Museum\\nEtymology[edit]\\nThe modern English term for the bird is derived from Latin: aquila by way of French: aigle. The origin of aquila is unknown, but it is believed to possibly derive from aquilus (meaning dark-colored, swarthy, or blackish) as a reference to the plumage of eagles.' metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 28:\n",
      "page_content='Old English used the term earn, related to Scandinavia\\'s ørn/örn. It is similar to other Indo-European terms for \"bird\" or \"eagle\", including Greek: ὄρνις (ornís), Russian: орёл (orël), and Welsh: eryr.\\nIn the southern part of Finland, near the Gulf of Finland, is the town of Kotka, which literally means \"eagle\", while the town of L\\'Aquila in the central part of Italy literally means \"the eagle\".\\nThe sculpture of eagle at the top of the fountain at Plac Orła Białego in Szczecin, Poland\\nIn Britain before 1678, eagle referred specifically to the golden eagle, with the other native species, the white-tailed eagle, being known as erne. The modern name \"golden eagle\" for aquila chrysaetos was introduced by the naturalist John Ray.[30]\\nThe village of Eagle in Lincolnshire, England, has nothing to do with the bird; its name is derived from the Old English words for \"oak\" and \"wood\" (compare Oakley).[31]\\nReligion and spirituality[edit]\\nRepresentation of an eagle at Rio Carnival, 2014' metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 29:\n",
      "page_content=\"Religion and spirituality[edit]\\nRepresentation of an eagle at Rio Carnival, 2014\\nGaruda, the vahana (mount) of Vishnu, depicted with an eagle's beak and wings\\nIn the ancient Sumerian mythology, the mythical king Etana was said to have been carried into heaven by an eagle.[32] Classical writers such as Lucan and Pliny the Elder claimed that the eagle was able to look directly at the sun, and that they forced their fledglings to do the same. Those that blinked would be cast from the nest. This belief persisted until the Medieval era.[33]\\nThe eagle is the patron animal of the ancient Greek god Zeus. In particular, Zeus was said to have taken the form of an eagle in order to abduct Ganymede, and there are numerous artistic depictions of the eagle Zeus bearing Ganymede aloft, from Classical times up to the present (see illustrations in the Ganymede (mythology) page.)[34]\" metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 30:\n",
      "page_content='Eagles appear metaphorically in many translations of the Old Testament. God is spoken of as carrying Israel on \"eagles\\' wings\" in Exodus 19:4, Isaiah 40:31 compares those who wait on the Lord to flying eagles, and Psalm 103 mentions renewing one\\'s youth \"as the eagle\". In explaining this rejuvenation, Augustine of Hippo says in his commentary on the Psalms that eagles\\' beaks overgrow as they age and that they break them against rocks to restore them.[35] The translation, however, is uncertain: the word in the Hebrew, נשר, can also be translated vulture,[36] and is listed alongside specific kinds of vulture in Leviticus\\' discussion of unclean animals.' metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 31:\n",
      "page_content=\"The eagle is also often used in Christian iconography to represent the Gospel of John,[37] and eagle-shaped lecterns are common in Anglican and some Roman Catholic churches.[38] The eagle was believed to be able to look directly into the sun in the same way that the Gospel of John looks directly at Jesus' divinity, and the great distances the eagle flies represent the spread of the gospel to the ends of the earth.\\nThe United States eagle feather law stipulates that only individuals of certifiable Native American ancestry enrolled in a federally recognized tribe are legally authorized to obtain eagle feathers for religious or spiritual reasons.[39] In Canada, the poaching of eagle feathers for the booming U.S. market has sometimes resulted in the arrests of First Nations person for the crime.[40]\" metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 32:\n",
      "page_content='The Moche people of ancient Peru worshiped the eagle and often depicted eagles in their art.[41] The golden eagle was sacred to the Aztec god Huitzilopochtli while the harpy eagle was sacred to Quetzalcoatl.[42]\\nHeraldry[edit]\\nMain article: Eagle (heraldry)\\nThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (July 2016) (Learn how and when to remove this message)\\nCoat of arms of Austria.\\nCoat of arms of Kotka, FinlandCoat of arms of the United States' metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 33:\n",
      "page_content='Coat of arms of Austria.\\nCoat of arms of Kotka, FinlandCoat of arms of the United States\\nEagles are an exceptionally common symbol in heraldry, being considered the \"King of Birds\" in contrast to the lion, the \"King of Beasts\".  Whereas the lion (e.g. England) usually represents authority, the eagle is the symbol of power. They are particularly popular in Germanic countries such as Austria, due to their association with the Holy Roman Empire. The eagle of the Holy Roman Empire was two-headed, supposedly representing the two divisions, East and West, of the old Roman Empire.  This motif, derived from the Byzantine (Eastern Roman) Empire was also adopted by the Russian Empire and is still featured in the Flag of Albania. The Roman eagle was preceded by the eagle of Ptolemaic Egypt and the Achaemenid Empire. In the coat of arms of Kotka, Finland, the eagle is depicted carrying an anchor and the caduceus on its feet.' metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 34:\n",
      "page_content='Heraldic eagles are most often found displayed, i.e. with their wings and legs extended. They can also occur close, i.e. with their wings folded, or rising, i.e. about to take flight. The heads, wings, and legs of eagles can also be found independently. \\nEagles symbolize strength, courage, and independence and are commonly found in the heraldry of many nations across the world. Albania, Andorra, Armenia, Austria, Dagestan, Egypt, Germany, Ghana, Iraq, Jordan, Kazakhstan, Mexico, Montenegro, Nigeria, Philippines, Poland, Palestine, Panama, Russia, Romania, Serbia, South Sudan, Somaliland, the United States of America, Yemen, Zambia, and Zimbabwe are the\\xa0nations whose coats of arms feature an eagle. The eagle\\'s continuing significance and worldwide appeal as a forceful symbol in national identity and imagery is demonstrated by its widespread usage.[43]\\nNotes[edit]\\n^ \"There are four major groups of eagles: fish eagles, booted eagles, snake eagles and giant forest eagles.\"[21]' metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 35:\n",
      "page_content='Notes[edit]\\n^ \"There are four major groups of eagles: fish eagles, booted eagles, snake eagles and giant forest eagles.\"[21]\\nReferences[edit]\\n^ del Hoyo, J.; Elliot, A. & Sargatal, J. (editors). (1994). Handbook of the Birds of the World Volume 2: New World Vultures to Guineafowl. Lynx Edicions. ISBN\\xa084-87334-15-6\\n^ Mitkus, Mindaugas; Potier, Simon; Martin, Graham R.; Duriez, Olivier; Kelber, Almut (26 April 2018), \"Raptor Vision\", Oxford Research Encyclopedia of Neuroscience, doi:10.1093/acrefore/9780190264086.013.232, ISBN\\xa0978-0-19-026408-6, retrieved 12 June 2023\\n^ Martin, Graham R. (January 1986). \"Vision: Shortcomings of an eagle\\'s eye\". Nature. 319 (6052): 357. Bibcode:1986Natur.319..357M. doi:10.1038/319357a0. ISSN\\xa01476-4687. PMID\\xa03945316. S2CID\\xa04233018.' metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 36:\n",
      "page_content='^ Reymond, L. (1985). \"Spatial visual acuity of the eagle Aquila audax: a behavioural, optical and anatomical investigation\". Vision Research. 25 (10): 1477–1491. doi:10.1016/0042-6989(85)90226-3. ISSN\\xa00042-6989. PMID\\xa04090282. S2CID\\xa020680520.\\n^ Mitkus, Mindaugas; Potier, Simon; Martin, Graham R.; Duriez, Olivier; Kelber, Almut (26 April 2018), \"Raptor Vision\", Oxford Research Encyclopedia of Neuroscience, doi:10.1093/acrefore/9780190264086.013.232, ISBN\\xa0978-0-19-026408-6, retrieved 12 June 2023\\n^ Leclerc, Georges (2010). The Natural History of Birds: From the French of the Count de Buffon; Illustrated with Engravings, and a Preface, Notes, and Additions, by the Translator. Cambridge University Press. pp.\\xa060–. ISBN\\xa0978-1-108-02298-9. Archived from the original on 29 April 2016.\\n^ Grambo, Rebecca L. (2003). Eagles. Voyageur Press. ISBN\\xa0978-0-89658-363-4. Archived from the original on 30 April 2016.\\n^ Grambo, Rebecca L (2003). Eagles. Voyageur Press. p.\\xa032. ISBN\\xa0978-0-89658-363-4.' metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 37:\n",
      "page_content='^ Grambo, Rebecca L (2003). Eagles. Voyageur Press. p.\\xa032. ISBN\\xa0978-0-89658-363-4.\\n^ Stinson, Christopher H (1979). \"On the Selective Advantage of Fratricide in Raptors\". Evolution. 33 (4): 1219–1225. doi:10.2307/2407480. JSTOR\\xa02407480. PMID\\xa028563923.\\n^ a b c d e Ferguson-Lees, J.; Christie, D. (2001). Raptors of the World. London: Christopher Helm. ISBN\\xa00-7136-8026-1.\\n^ \"Amazing Bird Records\". Trails.com. Archived from the original on 20 June 2017. Retrieved 20 July 2012.\\n^ \"Deer dropped by eagle knocks out power in Montana\". Reuters. 18 June 2011. Retrieved 11 July 2023.\\n^ Watson, Jeff (2011). The Golden Eagle (Second\\xa0ed.). Yale University Press. ISBN\\xa0978-0-30017-019-1.\\n^ Sutton, C.; Dunne, P.; Sibley, D. (1989). Hawks in Flight: The Flight Identification of North American Migrant Raptors. Boston: Houghton Mifflin Harcourt. ISBN\\xa00-3955-1022-8.\\n^ Ferguson-Lees, et al.' metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 38:\n",
      "page_content='^ Ferguson-Lees, et al.\\n^ a b del Hoyo, J; Elliot, A; Sargatal, J (1996). Handbook of the Birds of the World. Vol.\\xa03. Barcelona: Lynx Edicions. ISBN\\xa084-87334-20-2.\\n^ Gamauf, A.; Preleuthner, M. & Winkler, H. (1998). \"Philippine Birds of Prey: Interrelations among habitat, morphology and behavior\" (PDF). The Auk. 115 (3): 713–726. doi:10.2307/4089419. JSTOR\\xa04089419. Archived (PDF) from the original on 23 August 2014.\\n^ Morgan, A.M. \"The spread and weight of the Wedge-tailed Eagle\" (PDF). South Australian Ornithologist. 11: 156–157. Archived from the original (PDF) on 24 April 2013.\\n^ Wood, Gerald (1983). The Guinness Book of Animal Facts and Feats. Guinness Superlatives. ISBN\\xa0978-0-85112-235-9.\\n^ \"European Raptors: Golden Eagle\". www.europeanraptors.org (in German). Archived from the original on 7 May 2017. Retrieved 11 September 2017.\\n^ a b Stalcup, Carolyn. \"All About Eagles\". The American Eagle Foundation. Archived from the original on 14 July 2014. Retrieved 25 May 2014.' metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 39:\n",
      "page_content='^ a b Stalcup, Carolyn. \"All About Eagles\". The American Eagle Foundation. Archived from the original on 14 July 2014. Retrieved 25 May 2014.\\n^ a b c d e f Rutledge, Hope. \"Eagles of the World\". American Bald Eagle Information. Archived from the original on 28 May 2014. Retrieved 11 June 2014. from Grambo, Rebecca L. (1999). Eagles. Voyageur Press, Inc. ISBN\\xa09780896583634.\\n^ a b Lerner, H. R. L.; Mindell, D. P. (2005). \"Phylogeny of eagles, Old World vultures, and other Accipitridae based on nuclear and mitochondrial DNA\". Molecular Phylogenetics and Evolution. 37 (2): 327–346. Bibcode:2005MolPE..37..327L. doi:10.1016/j.ympev.2005.04.010. PMID\\xa015925523.\\n^ a b c d Lerner, Heather R. L.; Mindell, David P. (9 May 2006). \"Accipitridae\". The Tree of Life Web Project. Archived from the original on 23 December 2014.\\n^ \"Sea Eagles, Fish Eagles and Fishing Eagles\". www.oiseaux-birds.com. Retrieved 15 February 2024.' metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 40:\n",
      "page_content='^ \"Sea Eagles, Fish Eagles and Fishing Eagles\". www.oiseaux-birds.com. Retrieved 15 February 2024.\\n^ Moshin, Miron; Sayam, Chowdhury (9 May 2019). \"Breeding Density and Habitat Selection of the Grey-Headed Fish-Eagle in Noakhali District, Bangladesh\". Journal of Raptor Research. 53 (2): 134–141. doi:10.3356/JRR-18-33.\\n^ Morandini, Virginia; Baumbusch, Ryan; Balbontin, Javier; Ferrer, Miguel (25 May 2020). \"Age of the breeders, but not territory quality, explains hatching sex ratio in booted eagles\". Journal of Avian Biology. 51 (8). doi:10.1111/jav.02511.\\n^ a b Bouglouan, Nicole. \"The booted eagles throughout the world: introduction\". Oiseaux-birds. Archived from the original on 17 May 2014. Retrieved 11 June 2014.\\n^ Onofre, Nuno; Sampaio, Luís (2020). \"Feeding Ecology of Short-Toed Snake-Eagle (Circaetus gallicus [Gmelin, 1788]) in the Montados of Iberian Peninsula\" (PDF). Silva Lusitana. 28 (2): 155-179. doi:10.1051/silu/20202802139.' metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 41:\n",
      "page_content='^ \"TrekNature | Whitehead eagle Photo\". www.treknature.com. Retrieved 25 March 2022.\\n^ Reaney, P.H. The Origin of English Place Names (1964\\xa0ed.). Routledge and Kegan Paul. p.\\xa0166.\\n^ Horowitz, Wayne (1998). Mesopotamian Cosmic Geography. Winona Lake, Indiana: Eisenbrauns. pp.\\xa043–59. ISBN\\xa00-931464-99-4. Archived from the original on 6 December 2017.\\n^ Badke, David. The Medieval Bestiary Archived 22 November 2016 at the Wayback Machine\\n^ Hutchinson, John (1749). Philosophical and Theological Works of the Late Truly Learned John Hutchinson. London, UK: James Hedges. p.\\xa0402. Archived from the original on 25 April 2016.\\n^ Psalm 103 Archived 8 May 2015 at the Wayback Machine in Augustine\\'s commentary.\\n^ \"Lexicon: Strong\\'s H5404 - nešer\". Blue Letter Bible. 11 June 2023.\\n^ Fonck, L. (1910). St. John the Evangelist. In The Catholic Encyclopedia (New York: Robert Appleton Company). Retrieved 14 August 2017 from New Advent.' metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 42:\n",
      "page_content='^ Delderfield, Eric R. (1966). A Guide to Church Furniture. Newton Abbot: David & Charles.\\n^ Office of Law Enforcement. \"National Eagle Repository\". Mountain-Prairie Region. United States Fish and Wildlife Service. Archived from the original on 10 October 2007. Retrieved 20 November 2007.\\n^ Sin, Lena (30 April 2006). \"Charges laid in eagle-poaching case\". The Province. CanWest MediaWorks Publications Inc. Archived from the original on 31 May 2009. Retrieved 20 November 2007.\\n^ Larco Herrera, Rafael, and Berrin, Kathleen (1997) The Spirit of Ancient Peru Thames and Hudson, New York, ISBN\\xa00500018022\\n^ de Borhegyi, Carl (30 October 2012). \"Evidence of Mushroom Worship in Mesoamerica\". The Yucatan Times. Archived from the original on 12 September 2014. Retrieved 11 September 2014.\\n^ Noor, Naeem (14 April 2024). \"Coat of Arms of the World\". symbolhunt.com. Retrieved 26 April 2024.\\nExternal links[edit]\\nLook up eagle in Wiktionary, the free dictionary.' metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 43:\n",
      "page_content='External links[edit]\\nLook up eagle in Wiktionary, the free dictionary.\\nWikiquote has quotations related to Eagles.\\nWikisource has the text of the 1911 Encyclopædia Britannica article \"Eagle\".\\nWikimedia Commons has media related to Eagles.\\nPBS Nature: Eagles\\nEagle photos Archived 6 June 2012 at the Wayback Machine on Oriental Bird Images\\nEagle videos on the Internet Bird Collection\\nWeb of the Conservation Biology Team-Bonelli\\'s Eagle, of the University of Barcelona\\nDecorah Eagles: 24/7 Live Webcam from The Raptor Resource Project Archived 1 March 2012 at the Wayback Machine\\nEagleCAM: White-bellied Sea Eagles Live Webcam at Discovery Centre in Sydney, Australia\\n\"Eagle\"\\xa0. New International Encyclopedia. 1905.\\nvteSubfamily: ButeoninaeGenusSpecies (extinctions: † indicates a species confirmed to be extinct)Geranoaetus\\nBlack-chested buzzard-eagle\\nVariable hawk\\nWhite-tailed hawk\\nButeo\\nCommon buzzard\\nEastern buzzard\\nHimalayan buzzard\\nCape Verde buzzard\\nSocotra buzzard\\nRed-tailed hawk' metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 44:\n",
      "page_content=\"Variable hawk\\nWhite-tailed hawk\\nButeo\\nCommon buzzard\\nEastern buzzard\\nHimalayan buzzard\\nCape Verde buzzard\\nSocotra buzzard\\nRed-tailed hawk\\nLong-legged buzzard\\nRough-legged buzzard\\nFerruginous hawk\\nRed-shouldered hawk\\nBroad-winged hawk\\nSwainson's hawk\\nRidgway's hawk\\nWhite-rumped hawk\\nShort-tailed hawk\\nWhite-throated hawk\\nGalapagos hawk\\nGray hawk\\nZone-tailed hawk\\nHawaiian hawk\\nRufous-tailed hawk\\nForest buzzard\\nMountain buzzard\\nMadagascar buzzard\\nUpland buzzard\\nRed-necked buzzard\\nJackal buzzard\\nArcher's buzzard\\nAugur buzzard\\nRupornis\\nRoadside hawk\\nParabuteo\\nHarris's hawk\\nWhite-rumped hawk\\nButeogallus\\nRufous crab hawk\\nCommon black hawk\\nCuban black hawk\\nGreat black hawk\\nSavanna hawk\\nBusarellus\\nBlack-collared hawk\\nLeucopternis\\nWhite-browed hawk\\nWhite-necked hawk\\nBlack-faced hawk\\nPlumbeous hawk\\nBarred hawk\\nSlate-colored hawk\\nSemiplumbeous hawk\\nPseudastur\\nGrey-backed hawk\\nWhite hawk\\nMantled hawk\\nKaupifalco\\nLizard buzzard\\nButastur\\nRufous-winged buzzard\\nGrasshopper buzzard\\nWhite-eyed buzzard\" metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 45:\n",
      "page_content='Pseudastur\\nGrey-backed hawk\\nWhite hawk\\nMantled hawk\\nKaupifalco\\nLizard buzzard\\nButastur\\nRufous-winged buzzard\\nGrasshopper buzzard\\nWhite-eyed buzzard\\nGrey-faced buzzard\\nHarpyhaliaetus\\nChaco eagle\\nSolitary eagle\\nEagle\\nBuzzard\\nAuthority control databases International\\nFAST\\nNational\\nSpain\\nGermany\\nIsrael\\nUnited States\\nJapan\\nCzech Republic\\nOther\\nNARA\\nRetrieved from \"https://en.wikipedia.org/w/index.php?title=Eagle&oldid=1230787755\"' metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 46:\n",
      "page_content='Categories: EaglesAccipitridaeApex predatorsBird common namesNational symbols of ArmeniaNational symbols of AustriaNational symbols of the Czech RepublicNational symbols of GermanyNational symbols of GhanaNational symbols of LiechtensteinNational symbols of MexicoNational symbols of NigeriaNational symbols of PolandNational symbols of RomaniaNational symbols of SerbiaNational symbols of SpainNational symbols of SyriaNational symbols of YemenHidden categories: CS1 German-language sources (de)Webarchive template wayback linksArticles with short descriptionShort description is different from WikidataUse dmy dates from May 2015Articles with specifically marked weasel-worded phrases from June 2024All articles with unsourced statementsArticles with unsourced statements from June 2024Wikipedia articles needing clarification from June 2024All articles with vague or ambiguous timeVague or ambiguous time from June 2024All articles with specifically marked weasel-worded phrasesArticles' metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 47:\n",
      "page_content='articles with vague or ambiguous timeVague or ambiguous time from June 2024All articles with specifically marked weasel-worded phrasesArticles containing Latin-language textArticles containing French-language textArticles containing Old English (ca. 450-1100)-language textArticles containing Greek-language textArticles containing Russian-language textArticles containing Welsh-language textArticles needing additional references from July 2016All articles needing additional referencesCommons category link is locally definedWikipedia articles incorporating a citation from the New International EncyclopediaArticles with FAST identifiersArticles with BNE identifiersArticles with GND identifiersArticles with J9U identifiersArticles with LCCN identifiersArticles with NDL identifiersArticles with NKC identifiersArticles with NARA identifiers' metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n",
      "Page 48:\n",
      "page_content='This page was last edited on 24 June 2024, at 18:25\\xa0(UTC).\\nText is available under the Creative Commons Attribution-ShareAlike License 4.0;\\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\\nPrivacy policy\\nAbout Wikipedia\\nDisclaimers\\nContact Wikipedia\\nCode of Conduct\\nDevelopers\\nStatistics\\nCookie statement\\nMobile view' metadata={'source': 'https://en.wikipedia.org/wiki/Eagle', 'title': 'Eagle - Wikipedia', 'language': 'en'}\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "len(docs[0].page_content)\n",
    "\n",
    "docs[0].page_content = clean_text(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89\n",
      "82\n",
      "Page 1:\n",
      "page_content='Lok Sabha MP Om Birla was elected as Lok Sabha Speaker Wednesday by a voice vote for the second time in a row. The Opposition did not press for a division of votes and pro-tem Speaker Bhartruhari Mahtab declared Birla as elected, saying the “ayes have it”. With Birla’s election, Mahtab said, the other motions to propose and second the candidature of K Suresh became infructuous. Prime Minister Narendra Modi and senior Congress leader Rahul Gandhi, whom the Congress has named Leader of the Opposition in the Lok Sabha, congratulated Birla as he took charge. While PM Modi recalled the milestones of his first term as Speaker, Rahul Gandhi sought a voice for the Opposition and Samajwadi Party (SP) leader Akhilesh Yadav said he expects that members will not be suspended for raising their voices in the House. Modi said that Birla was becoming Speaker for the second time after a full term, like Balram Jakhar. He added that such was the work of the Speaker that past Speakers often either did not' metadata={'source': 'https://indianexpress.com/article/india/om-birla-election-lok-sabha-speaker-modi-opposition-9415980/'}\n",
      "----------------------------------------\n",
      "Page 2:\n",
      "page_content='for the second time after a full term, like Balram Jakhar. He added that such was the work of the Speaker that past Speakers often either did not contest another election or lost it. “The works that did not happen during 70 years of independence were made possible by this House under your chairmanship. Several milestones come in the long journey of democracy. A few occasions are such when we get the opportunity to establish milestones. I am very confident that the country will be proud of the achievements of the 17th Lok Sabha,” Modi said, recalling the Nari Shakti Vandan Adhiniyam to facilitate women’s reservation, the new penal laws, the reorganisation of Jammu and Kashmir after the abrogation of Article 370, etc. “I want to congratulate you on behalf of the House. It is a huge responsibility for you to sit on this post for the second time during Amrit Kaal. With your experience, we hope that you will guide us for the next five years. Aapke chehre par ye meethi meethi muskaan poore' metadata={'source': 'https://indianexpress.com/article/india/om-birla-election-lok-sabha-speaker-modi-opposition-9415980/'}\n",
      "----------------------------------------\n",
      "Page 3:\n",
      "page_content='time during Amrit Kaal. With your experience, we hope that you will guide us for the next five years. Aapke chehre par ye meethi meethi muskaan poore sadan ko prasann rakhti hai (This sweet smile on your face makes the whole House happy),” Modi said. The PM also noted that Birla had seen the shifting of the House from the old building to the new building, which happened in the first term of Birla. He recalled that Birla had as Speaker stayed in touch with members of the House when they were infected by coronavirus, and added that he would feel happy when Opposition members also told him so, showing that Birla helped members tide over the Covid crisis like the mukhiya (head) of a family. He also recalled that Birla had to sometimes take tough decisions to restore the dignity of the House with a heavy heart, but always saw the dignity of the House as supreme. Speaking after the PM, Rahul Gandhi congratulated Birla for being elected to the post for the second time and said he was' metadata={'source': 'https://indianexpress.com/article/india/om-birla-election-lok-sabha-speaker-modi-opposition-9415980/'}\n",
      "----------------------------------------\n",
      "Page 4:\n",
      "page_content='of the House as supreme. Speaking after the PM, Rahul Gandhi congratulated Birla for being elected to the post for the second time and said he was confident that the Speaker would allow the Opposition to represent their voices and the voice of the people of India. “I would like to congratulate you on behalf of the entire Opposition and INDIA alliance. Speaker sir, this House represents the voice of India’s people. And you are the final arbiter of that voice.” “Of course, the government has political power but the Opposition also represents the voice of the people. And this time, the Opposition represents significantly more voice of the Indian people than it did last time,” he added. “The Opposition would like to assist you in doing your work. We would like the House to function often and well. It is very important that cooperation happens on the basis of trust. It is very important that the voice of the Opposition is allowed to be represented in this House. I am confident that you' metadata={'source': 'https://indianexpress.com/article/india/om-birla-election-lok-sabha-speaker-modi-opposition-9415980/'}\n",
      "----------------------------------------\n",
      "Page 5:\n",
      "page_content='on the basis of trust. It is very important that the voice of the Opposition is allowed to be represented in this House. I am confident that you will allow us to represent our voice, allow us to speak to represent the voice of the people of India,” Gandhi said. “The question is not how efficiently the House is run; the question is how much of India’s voice is being allowed to be heard in this House. So the idea that you can run the House efficiently by silencing the voice of the Opposition is a non-democratic idea. And this election has shown that the people of India expect the Opposition to defend the Constitution of the country. And we are confident that by allowing the Opposition to speak to represent the people of India, you will do your duty of defending the Constitution of India,” said Gandhi. Congratulating Birla, Akhilesh Yadav, whose party is third largest in the present Lok Sabha with 37 MPs, expressed the hope that there would be no discrimination and the Speaker would' metadata={'source': 'https://indianexpress.com/article/india/om-birla-election-lok-sabha-speaker-modi-opposition-9415980/'}\n",
      "----------------------------------------\n",
      "Page 6:\n",
      "page_content='whose party is third largest in the present Lok Sabha with 37 MPs, expressed the hope that there would be no discrimination and the Speaker would provide equal opportunity to MPs of Opposition parties in the House. Yadav added that he expects that the voices of public representatives are not suppressed and the dignity of the House is not hurt by actions like suspension of the members – obliquely hinting at about 100 suspensions within a week from the House in December 2023. “Impartiality is a great responsibility of this great position…,” Yadav said, adding that not just the Opposition but the treasury benches should also be under the control of the Speaker. Addressing the Speaker, Yadav said, “Sadan aapke ishaare par chale, iske ulta na ho (the House should function under your control and not vice versa).” The Opposition is demanding the post of Deputy Speaker in Lok Sabha, which has been vacant since the start of the 17th Lok Sabha. Rahul Gandhi of Congress has stated that the' metadata={'source': 'https://indianexpress.com/article/india/om-birla-election-lok-sabha-speaker-modi-opposition-9415980/'}\n",
      "----------------------------------------\n",
      "Page 7:\n",
      "page_content=\"the post of Deputy Speaker in Lok Sabha, which has been vacant since the start of the 17th Lok Sabha. Rahul Gandhi of Congress has stated that the Opposition will support the NDA's candidate for Speaker if the Deputy Speaker's post is given to the Opposition. However, governments have often overlooked the constitutional mandate to appoint both positions promptly.\" metadata={'source': 'https://indianexpress.com/article/india/om-birla-election-lok-sabha-speaker-modi-opposition-9415980/'}\n",
      "----------------------------------------\n",
      "Page 8:\n",
      "page_content='Beautiful Soup is a\\nPython library for pulling data out of HTML and XML files. It works\\nwith your favorite parser to provide idiomatic ways of navigating,\\nsearching, and modifying the parse tree. It commonly saves programmers\\nhours or days of work. These instructions illustrate all major features of Beautiful Soup 4,\\nwith examples. I show you what the library is good for, how it works,\\nhow to use it, how to make it do what you want, and what to do when it\\nviolates your expectations. This document covers Beautiful Soup version 4.12.2. The examples in\\nthis documentation were written for Python 3.8. You might be looking for the documentation for Beautiful Soup 3.\\nIf so, you should know that Beautiful Soup 3 is no longer being\\ndeveloped and that all support for it was dropped on December\\n31, 2020. If you want to learn about the differences between Beautiful\\nSoup 3 and Beautiful Soup 4, see Porting code to BS4. This documentation has been translated into other languages by' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 9:\n",
      "page_content='Soup 3 and Beautiful Soup 4, see Porting code to BS4. This documentation has been translated into other languages by\\nBeautiful Soup users: 这篇文档当然还有中文版. このページは日本語で利用できます(外部リンク) 이 문서는 한국어 번역도 가능합니다. Este documento também está disponível em Português do Brasil. Este documento también está disponible en una traducción al español. Эта документация доступна на русском языке. If you have questions about Beautiful Soup, or run into problems,\\nsend mail to the discussion group. If\\nyour problem involves parsing an HTML document, be sure to mention\\nwhat the diagnose() function says about\\nthat document. When reporting an error in this documentation, please mention which\\ntranslation you’re reading. Here’s an HTML document I’ll be using as an example throughout this\\ndocument. It’s part of a story from Alice in Wonderland: Running the “three sisters” document through Beautiful Soup gives us a\\nBeautifulSoup object, which represents the document as a nested' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 10:\n",
      "page_content='BeautifulSoup object, which represents the document as a nested\\ndata structure: Here are some simple ways to navigate that data structure: One common task is extracting all the URLs found within a page’s <a> tags: Another common task is extracting all the text from a page: Does this look like what you need? If so, read on. If you’re using a recent version of Debian or Ubuntu Linux, you can\\ninstall Beautiful Soup with the system package manager: $ apt-get install python3-bs4 Beautiful Soup 4 is published through PyPi, so if you can’t install it\\nwith the system packager, you can install it with easy_install or\\npip. The package name is beautifulsoup4. Make sure you use the\\nright version of pip or easy_install for your Python version\\n(these may be named pip3 and easy_install3 respectively). $ easy_install beautifulsoup4 $ pip install beautifulsoup4 (The BeautifulSoup package is not what you want. That’s\\nthe previous major release, Beautiful Soup 3. Lots of software uses' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 11:\n",
      "page_content='the previous major release, Beautiful Soup 3. Lots of software uses\\nBS3, so it’s still available, but if you’re writing new code you\\nshould install beautifulsoup4.) If you don’t have easy_install or pip installed, you can\\ndownload the Beautiful Soup 4 source tarball and\\ninstall it with setup.py. $ python setup.py install If all else fails, the license for Beautiful Soup allows you to\\npackage the entire library with your application. You can download the\\ntarball, copy its bs4 directory into your application’s codebase,\\nand use Beautiful Soup without installing it at all. I use Python 3.10 to develop Beautiful Soup, but it should work with\\nother recent versions. Beautiful Soup supports the HTML parser included in Python’s standard\\nlibrary, but it also supports a number of third-party Python parsers.\\nOne is the lxml parser. Depending on your setup,' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 12:\n",
      "page_content='library, but it also supports a number of third-party Python parsers.\\nOne is the lxml parser. Depending on your setup,\\nyou might install lxml with one of these commands: $ apt-get install python-lxml $ easy_install lxml $ pip install lxml Another alternative is the pure-Python html5lib parser, which parses HTML the way a\\nweb browser does. Depending on your setup, you might install html5lib\\nwith one of these commands: $ apt-get install python3-html5lib $ pip install html5lib This table summarizes the advantages and disadvantages of each parser library: Parser Typical usage Advantages Disadvantages Python’s html.parser BeautifulSoup(markup, \"html.parser\") Batteries included Decent speed Not as fast as lxml,\\nless lenient than\\nhtml5lib. lxml’s HTML parser BeautifulSoup(markup, \"lxml\") Very fast External C dependency lxml’s XML parser BeautifulSoup(markup, \"lxml-xml\")\\nBeautifulSoup(markup, \"xml\") Very fast The only currently supported' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 13:\n",
      "page_content='BeautifulSoup(markup, \"xml\") Very fast The only currently supported\\nXML parser External C dependency html5lib BeautifulSoup(markup, \"html5lib\") Extremely lenient Parses pages the same way a\\nweb browser does Creates valid HTML5 Very slow External Python\\ndependency If you can, I recommend you install and use lxml for speed. Note that if a document is invalid, different parsers will generate\\ndifferent Beautiful Soup trees for it. See Differences\\nbetween parsers for details. To parse a document, pass it into the BeautifulSoup\\nconstructor. You can pass in a string or an open filehandle: First, the document is converted to Unicode, and HTML entities are\\nconverted to Unicode characters: Beautiful Soup then parses the document using the best available\\nparser. It will use an HTML parser unless you specifically tell it to\\nuse an XML parser. (See Parsing XML.) Beautiful Soup transforms a complex HTML document into a complex tree' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 14:\n",
      "page_content='use an XML parser. (See Parsing XML.) Beautiful Soup transforms a complex HTML document into a complex tree\\nof Python objects. But you’ll only ever have to deal with about four\\nkinds of objects: Tag, NavigableString, BeautifulSoup,\\nand Comment. These objects represent the HTML elements\\nthat comprise the page. A Tag object corresponds to an XML or HTML tag in the original document. Tags have a lot of attributes and methods, and I’ll cover most of them\\nin Navigating the tree and Searching the tree. For now, the most\\nimportant methods of a tag are for accessing its name and attributes. Every tag has a name: If you change a tag’s name, the change will be reflected in any\\nmarkup generated by Beautiful Soup down the line: An HTML or XML tag may have any number of attributes. The tag <b\\nid=\"boldest\"> has an attribute “id” whose value is\\n“boldest”. You can access a tag’s attributes by treating the tag like' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 15:\n",
      "page_content='id=\"boldest\"> has an attribute “id” whose value is\\n“boldest”. You can access a tag’s attributes by treating the tag like\\na dictionary: You can access the dictionary of attributes directly as .attrs: You can add, remove, and modify a tag’s attributes. Again, this is\\ndone by treating the tag as a dictionary: HTML 4 defines a few attributes that can have multiple values. HTML 5\\nremoves a couple of them, but defines a few more. The most common\\nmulti-valued attribute is class (that is, a tag can have more than\\none CSS class). Others include rel, rev, accept-charset,\\nheaders, and accesskey. By default, Beautiful Soup stores the value(s)\\nof a multi-valued attribute as a list: When you turn a tag back into a string, the values of any multi-valued\\nattributes are consolidated: If an attribute looks like it has more than one value, but it’s not\\na multi-valued attribute as defined by any version of the HTML' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 16:\n",
      "page_content='a multi-valued attribute as defined by any version of the HTML\\nstandard, Beautiful Soup stores it as a simple string: You can force all attributes to be stored as strings by passing\\nmulti_valued_attributes=None as a keyword argument into the\\nBeautifulSoup constructor: You can use get_attribute_list to always return the value in a list\\ncontainer, whether it’s a string or multi-valued attribute value: If you parse a document as XML, there are no multi-valued attributes: Again, you can configure this using the multi_valued_attributes argument: You probably won’t need to do this, but if you do, use the defaults as\\na guide. They implement the rules described in the HTML specification: A tag can contain strings as pieces of text. Beautiful Soup\\nuses the NavigableString class to contain these pieces of text: A NavigableString is just like a Python Unicode string, except\\nthat it also supports some of the features described in Navigating\\nthe tree and Searching the tree. You can convert a' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 17:\n",
      "page_content='that it also supports some of the features described in Navigating\\nthe tree and Searching the tree. You can convert a\\nNavigableString to a Unicode string with str: You can’t edit a string in place, but you can replace one string with\\nanother, using replace_with(): NavigableString supports most of the features described in\\nNavigating the tree and Searching the tree, but not all of\\nthem. In particular, since a string can’t contain anything (the way a\\ntag may contain a string or another tag), strings don’t support the\\n.contents or .string attributes, or the find() method. If you want to use a NavigableString outside of Beautiful Soup,\\nyou should call unicode() on it to turn it into a normal Python\\nUnicode string. If you don’t, your string will carry around a\\nreference to the entire Beautiful Soup parse tree, even when you’re\\ndone using Beautiful Soup. This is a big waste of memory. The BeautifulSoup object represents the parsed document as a' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 18:\n",
      "page_content='done using Beautiful Soup. This is a big waste of memory. The BeautifulSoup object represents the parsed document as a\\nwhole. For most purposes, you can treat it as a Tag\\nobject. This means it supports most of the methods described in\\nNavigating the tree and Searching the tree. You can also pass a BeautifulSoup object into one of the methods\\ndefined in Modifying the tree, just as you would a Tag. This\\nlets you do things like combine two parsed documents: Since the BeautifulSoup object doesn’t correspond to an actual\\nHTML or XML tag, it has no name and no attributes. But sometimes it’s\\nuseful to reference its .name (such as when writing code that works\\nwith both Tag and BeautifulSoup objects),\\nso it’s been given the special .name “[document]”: Tag, NavigableString, and\\nBeautifulSoup cover almost everything you’ll see in an\\nHTML or XML file, but there are a few leftover bits. The main one' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 19:\n",
      "page_content='BeautifulSoup cover almost everything you’ll see in an\\nHTML or XML file, but there are a few leftover bits. The main one\\nyou’ll probably encounter is the Comment. The Comment object is just a special type of NavigableString: But when it appears as part of an HTML document, a Comment is\\ndisplayed with special formatting: Beautiful Soup defines a few NavigableString subclasses to\\ncontain strings found inside specific HTML tags. This makes it easier\\nto pick out the main body of the page, by ignoring strings that\\nprobably represent programming directives found within the\\npage. (These classes are new in Beautiful Soup 4.9.0, and the\\nhtml5lib parser doesn’t use them.) A NavigableString subclass that represents embedded CSS\\nstylesheets; that is, any strings found inside a <style> tag\\nduring document parsing. A NavigableString subclass that represents embedded\\nJavascript; that is, any strings found inside a <script> tag' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 20:\n",
      "page_content='during document parsing. A NavigableString subclass that represents embedded\\nJavascript; that is, any strings found inside a <script> tag\\nduring document parsing. A NavigableString subclass that represents embedded HTML\\ntemplates; that is, any strings found inside a <template> tag during\\ndocument parsing. Beautiful Soup defines some NavigableString classes for\\nholding special types of strings that can be found in XML\\ndocuments. Like Comment, these classes are subclasses of\\nNavigableString that add something extra to the string on\\noutput. A NavigableString subclass representing the declaration at the beginning of\\nan XML document. A NavigableString subclass representing the document type\\ndeclaration which may\\nbe found near the beginning of an XML document. A NavigableString subclass that represents a CData section. A NavigableString subclass that represents the contents' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 21:\n",
      "page_content='of an XML processing instruction. Here’s the “Three sisters” HTML document again: I’ll use this as an example to show you how to move from one part of\\na document to another. Tags may contain strings and more tags. These elements are the tag’s\\nchildren. Beautiful Soup provides a lot of different attributes for\\nnavigating and iterating over a tag’s children. Note that Beautiful Soup strings don’t support any of these\\nattributes, because a string can’t have children. The simplest way to navigate the parse tree is to find a tag by name. To\\ndo this, you can use the find() method: For convenience, just saying the name of the tag you want is equivalent\\nto find() (if no built-in attribute has that name). If you want the\\n<head> tag, just say soup.head: You can use this trick again and again to zoom in on a certain part\\nof the parse tree. This code gets the first <b> tag beneath the <body> tag: find() (and its convenience equivalent) gives you only the first tag' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 22:\n",
      "page_content='of the parse tree. This code gets the first <b> tag beneath the <body> tag: find() (and its convenience equivalent) gives you only the first tag\\nby that name: If you need to get all the <a> tags, you can use find_all(): For more complicated tasks, such as pattern-matching and filtering, you can\\nuse the methods described in Searching the tree. A tag’s children are available in a list called .contents: The BeautifulSoup object itself has children. In this case, the\\n<html> tag is the child of the BeautifulSoup object.: A string does not have .contents, because it can’t contain\\nanything: Instead of getting them as a list, you can iterate over a tag’s\\nchildren using the .children generator: If you want to modify a tag’s children, use the methods described in\\nModifying the tree. Don’t modify the the .contents list\\ndirectly: that can lead to problems that are subtle and difficult to\\nspot. The .contents and .children attributes consider only a tag’s' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 23:\n",
      "page_content='directly: that can lead to problems that are subtle and difficult to\\nspot. The .contents and .children attributes consider only a tag’s\\ndirect children. For instance, the <head> tag has a single direct\\nchild–the <title> tag: But the <title> tag itself has a child: the string “The Dormouse’s\\nstory”. There’s a sense in which that string is also a child of the\\n<head> tag. The .descendants attribute lets you iterate over all\\nof a tag’s children, recursively: its direct children, the children of\\nits direct children, and so on: The <head> tag has only one child, but it has two descendants: the\\n<title> tag and the <title> tag’s child. The BeautifulSoup object\\nonly has one direct child (the <html> tag), but it has a whole lot of\\ndescendants: If a tag has only one child, and that child is a NavigableString,\\nthe child is made available as .string: If a tag’s only child is another tag, and that tag has a\\n.string, then the parent tag is considered to have the same' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 24:\n",
      "page_content='.string, then the parent tag is considered to have the same\\n.string as its child: If a tag contains more than one thing, then it’s not clear what\\n.string should refer to, so .string is defined to be\\nNone: If there’s more than one thing inside a tag, you can still look at\\njust the strings. Use the .strings generator to see all descendant\\nstrings: Newlines and spaces that separate tags are also strings. You can remove extra\\nwhitespace by using the .stripped_strings generator instead: Here, strings consisting entirely of whitespace are ignored, and\\nwhitespace at the beginning and end of strings is removed. Continuing the “family tree” analogy, every tag and every string has a\\nparent: the tag that contains it. You can access an element’s parent with the .parent attribute. In\\nthe example “three sisters” document, the <head> tag is the parent\\nof the <title> tag: The title string itself has a parent: the <title> tag that contains' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 25:\n",
      "page_content='of the <title> tag: The title string itself has a parent: the <title> tag that contains\\nit: The parent of a top-level tag like <html> is the BeautifulSoup object\\nitself: And the .parent of a BeautifulSoup object is defined as None: You can iterate over all of an element’s parents with\\n.parents. This example uses .parents to travel from an <a> tag\\nburied deep within the document, to the very top of the document: Consider a simple document like this: The <b> tag and the <c> tag are at the same level: they’re both direct\\nchildren of the same tag. We call them siblings. When a document is\\npretty-printed, siblings show up at the same indentation level. You\\ncan also use this relationship in the code you write. You can use .next_sibling and .previous_sibling to navigate\\nbetween page elements that are on the same level of the parse tree: The <b> tag has a .next_sibling, but no .previous_sibling,\\nbecause there’s nothing before the <b> tag on the same level of the' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 26:\n",
      "page_content='because there’s nothing before the <b> tag on the same level of the\\ntree. For the same reason, the <c> tag has a .previous_sibling\\nbut no .next_sibling: The strings “text1” and “text2” are not siblings, because they don’t\\nhave the same parent: In real documents, the .next_sibling or .previous_sibling of a\\ntag will usually be a string containing whitespace. Going back to the\\n“three sisters” document: You might think that the .next_sibling of the first <a> tag would\\nbe the second <a> tag. But actually, it’s a string: the comma and\\nnewline that separate the first <a> tag from the second: The second <a> tag is then the .next_sibling of the comma string: You can iterate over a tag’s siblings with .next_siblings or\\n.previous_siblings: (If the argument syntax to find tags by their attribute value is unfamiliar,' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 27:\n",
      "page_content='.previous_siblings: (If the argument syntax to find tags by their attribute value is unfamiliar,\\ndon’t worry; this is covered later in The keyword arguments.) Take a look at the beginning of the “three sisters” document: An HTML parser takes this string of characters and turns it into a\\nseries of events: “open an <html> tag”, “open a <head> tag”, “open a\\n<title> tag”, “add a string”, “close the <title> tag”, “open a <p>\\ntag”, and so on. The order in which the opening tags and strings are\\nencountered is called document order. Beautiful Soup offers tools for\\nsearching a document’s elements in document order. The .next_element attribute of a string or tag points to whatever\\nwas parsed immediately after the opening of the current tag or after\\nthe current string. It might be the same as .next_sibling, but it’s\\nusually drastically different. Here’s the final <a> tag in the “three sisters” document. Its\\n.next_sibling is a string: the conclusion of the sentence that was' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 28:\n",
      "page_content='.next_sibling is a string: the conclusion of the sentence that was\\ninterrupted by the start of the <a> tag: But the .next_element of that <a> tag, the thing that was parsed\\nimmediately after the <a> tag, is not the rest of that sentence:\\nit’s the string “Tillie” inside it: That’s because in the original markup, the word “Tillie” appeared\\nbefore that semicolon. The parser encountered an <a> tag, then the\\nword “Tillie”, then the closing </a> tag, then the semicolon and rest of\\nthe sentence. The semicolon is on the same level as the <a> tag, but the\\nword “Tillie” was encountered first. The .previous_element attribute is the exact opposite of\\n.next_element. It points to the opening tag or string that was\\nparsed immediately before this one: You should get the idea by now. You can use these iterators to move\\nforward or backward in the document as it was parsed: Beautiful Soup defines a lot of methods for searching the parse tree,' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 29:\n",
      "page_content='forward or backward in the document as it was parsed: Beautiful Soup defines a lot of methods for searching the parse tree,\\nbut they’re all very similar. I’m going to spend a lot of time explaining\\nthe two most popular methods: find() and find_all(). The other\\nmethods take almost exactly the same arguments, so I’ll just cover\\nthem briefly. Once again, I’ll be using the “three sisters” document as an example: By passing in a filter to a method like find_all(), you can\\nzoom in on the parts of the document you’re interested in. Before talking in detail about find_all() and similar methods, I\\nwant to show examples of different filters you can pass into these\\nmethods. These filters show up again and again, throughout the\\nsearch API. You can use them to filter based on a tag’s name,\\non its attributes, on the text of a string, or on some combination of\\nthese. The simplest filter is a string. Pass a string to a search method and' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 30:\n",
      "page_content='on its attributes, on the text of a string, or on some combination of\\nthese. The simplest filter is a string. Pass a string to a search method and\\nBeautiful Soup will perform a tag-name match against that exact string.\\nThis code finds all the <b> tags in the document: If you pass in a byte string, Beautiful Soup will assume the string is\\nencoded as UTF-8. You can avoid this by passing in a Unicode string instead. If you pass in a regular expression object, Beautiful Soup will filter\\nagainst that regular expression using its search() method. This code\\nfinds all the tags whose names start with the letter “b”; in this\\ncase, the <body> tag and the <b> tag: This code finds all the tags whose names contain the letter ‘t’: The value True matches every tag it can. This code finds all\\nthe tags in the document, but none of the text strings: If none of the other matches work for you, define a function that\\ntakes an element as its only argument. The function should return' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 31:\n",
      "page_content='takes an element as its only argument. The function should return\\nTrue if the argument matches, and False otherwise. Here’s a function that returns True if a tag defines the “class”\\nattribute but doesn’t define the “id” attribute: Pass this function into find_all() and you’ll pick up all the <p>\\ntags: This function picks up only the <p> tags. It doesn’t pick up the <a>\\ntags, because those tags define both “class” and “id”. It doesn’t pick\\nup tags like <html> and <title>, because those tags don’t define\\n“class”. The function can be as complicated as you need it to be. Here’s a\\nfunction that returns True if a tag is surrounded by string\\nobjects: If you pass in a list, Beautiful Soup will look for a match against\\nany string, regular expression, or function in that list. This' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 32:\n",
      "page_content='objects: If you pass in a list, Beautiful Soup will look for a match against\\nany string, regular expression, or function in that list. This\\ncode finds all the <a> tags and all the <b> tags: Now we’re ready to look at the search methods in detail. Method signature: find_all(name, attrs, recursive, string, limit, **kwargs) The find_all() method looks through a tag’s descendants and\\nretrieves all descendants that match your filters. I gave several\\nexamples in Kinds of filters, but here are a few more: Some of these should look familiar, but others are new. What does it\\nmean to pass in a value for string, or id? Why does\\nfind_all(\"p\", \"title\") find a <p> tag with the CSS class “title”?\\nLet’s look at the arguments to find_all(). Pass in a value for name and you’ll tell Beautiful Soup to only\\nconsider tags with certain names. Text strings will be ignored, as\\nwill tags whose names that don’t match. This is the simplest usage: Recall from Kinds of filters that the value to name can be a' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 33:\n",
      "page_content='will tags whose names that don’t match. This is the simplest usage: Recall from Kinds of filters that the value to name can be a\\nstring, a regular expression, a list, a function, or the value\\nTrue. Any keyword argument that’s not recognized will be turned into a filter\\nthat matches tags by their attributes. If you pass in a value for an argument called id, Beautiful Soup will\\nfilter against each tag’s ‘id’ attribute value: Just as with tags, you can filter an attribute based on a string,\\na regular expression, a list, a function, or the value True. If you pass in a regular expression object for href, Beautiful Soup will\\npattern-match against each tag’s ‘href’ attribute value: The value True matches every tag that defines the attribute. This code\\nfinds all tags with an id attribute: soup.find_all(id=True)\\n# [<a class=”sister” href=”http://example.com/elsie” id=”link1”>Elsie</a>,\\n#  <a class=”sister” href=”http://example.com/lacie” id=”link2”>Lacie</a>,' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 34:\n",
      "page_content='# [<a class=”sister” href=”http://example.com/elsie” id=”link1”>Elsie</a>,\\n#  <a class=”sister” href=”http://example.com/lacie” id=”link2”>Lacie</a>,\\n#  <a class=”sister” href=”http://example.com/tillie” id=”link3”>Tillie</a>] For more complex matches, you can define a function that takes an attribute\\nvalue as its only argument. The function should return True if the value\\nmatches, and False otherwise. Here’s a function that finds all a tags whose href attribute does not\\nmatch a regular expression: If you pass in a list for an argument, Beautiful Soup will look for an\\nattribute-value match against any string, regular expression, or function in\\nthat list. This code finds the first and last link: soup.find_all(id=[“link1”, re.compile(“3$”)])\\n# [<a class=”sister” href=”http://example.com/elsie” id=”link1”>Elsie</a>,\\n#  <a class=”sister” href=”http://example.com/tillie” id=”link3”>Tillie</a>] You can filter against multiple attributes at once by passing multiple' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 35:\n",
      "page_content='#  <a class=”sister” href=”http://example.com/tillie” id=”link3”>Tillie</a>] You can filter against multiple attributes at once by passing multiple\\nkeyword arguments: Some attributes, like the data-* attributes in HTML 5, have names that\\ncan’t be used as the names of keyword arguments: You can use these attributes in searches by putting them into a\\ndictionary and passing the dictionary into find_all() as the\\nattrs argument: Similarly, you can’t use a keyword argument to search for HTML’s ‘name’ attribute,\\nbecause Beautiful Soup uses the name argument to contain the name\\nof the tag itself. Instead, you can give a value to ‘name’ in the\\nattrs argument: It’s very useful to search for a tag that has a certain CSS class, but\\nthe name of the CSS attribute, “class”, is a reserved word in\\nPython. Using class as a keyword argument will give you a syntax\\nerror. As of Beautiful Soup 4.1.2, you can search by CSS class using' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 36:\n",
      "page_content='Python. Using class as a keyword argument will give you a syntax\\nerror. As of Beautiful Soup 4.1.2, you can search by CSS class using\\nthe keyword argument class_: As with any keyword argument, you can pass class_ a string, a regular\\nexpression, a function, or True: Remember that a single tag can have multiple\\nvalues for its “class” attribute. When you search for a tag that\\nmatches a certain CSS class, you’re matching against any of its CSS\\nclasses: You can also search for the exact string value of the class attribute: But searching for variants of the string value won’t work: In older versions of Beautiful Soup, which don’t have the class_\\nshortcut, you can use the attrs argument trick mentioned above.\\nCreate a dictionary whose value for “class” is the string (or regular\\nexpression, or whatever) you want to search for: To search for tags that match two or more CSS classes at once, use the' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 37:\n",
      "page_content='expression, or whatever) you want to search for: To search for tags that match two or more CSS classes at once, use the\\nselect() CSS selector method described here: With the string argument, you can search for strings instead of tags. As\\nwith name and attribute keyword arguments, you can pass in a string, a\\nregular expression, a function, a list, or the value True.\\nHere are some examples: If you use the string argument in a tag search, Beautiful Soup will find\\nall tags whose .string matches your value for string. This code finds\\nthe <a> tags whose .string is “Elsie”: The string argument is new in Beautiful Soup 4.4.0. In earlier\\nversions it was called text: find_all() returns all the tags and strings that match your\\nfilters. This can take a while if the document is large. If you don’t\\nneed all the results, you can pass in a number for limit. This\\nworks just like the LIMIT keyword in SQL. It tells Beautiful Soup to' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 38:\n",
      "page_content='need all the results, you can pass in a number for limit. This\\nworks just like the LIMIT keyword in SQL. It tells Beautiful Soup to\\nstop gathering results after it’s found a certain number. There are three links in the “three sisters” document, but this code\\nonly finds the first two: By default, mytag.find_all() will examine all the descendants of mytag:\\nits children, its children’s children, and so on. To consider only direct\\nchildren, you can pass in recursive=False. See the difference here: Here’s that part of the document: The <title> tag is beneath the <html> tag, but it’s not directly\\nbeneath the <html> tag: the <head> tag is in the way. Beautiful Soup\\nfinds the <title> tag when it’s allowed to look at all descendants of\\nthe <html> tag, but when recursive=False restricts it to the\\n<html> tag’s immediate children, it finds nothing. Beautiful Soup offers a lot of tree-searching methods (covered below),\\nand they mostly take the same arguments as find_all(): name,' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 39:\n",
      "page_content='and they mostly take the same arguments as find_all(): name,\\nattrs, string, limit, and attribute keyword arguments. But the\\nrecursive argument is specific to the find_all() and find() methods.\\nPassing recursive=False into a method like find_parents() wouldn’t be\\nvery useful. For convenience, calling a BeautifulSoup object or\\nTag object as a function is equivalent to calling\\nfind_all() (if no built-in method has the name of the tag you’re\\nlooking for). These two lines of code are equivalent: These two lines are also equivalent: Method signature: find(name, attrs, recursive, string, **kwargs) The find_all() method scans the entire document looking for\\nresults, but sometimes you only want to find one result. If you know a\\ndocument has only one <body> tag, it’s a waste of time to scan the\\nentire document looking for more. Rather than passing in limit=1\\nevery time you call find_all, you can use the find()' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 40:\n",
      "page_content='entire document looking for more. Rather than passing in limit=1\\nevery time you call find_all, you can use the find()\\nmethod. These two lines of code are nearly equivalent: The only difference is that find_all() returns a list containing\\nthe single result, and find() just returns the result. If find_all() can’t find anything, it returns an empty list. If\\nfind() can’t find anything, it returns None: Remember the soup.head.title trick from Navigating using tag\\nnames? That trick works by repeatedly calling find(): Method signature: find_parents(name, attrs, string, limit, **kwargs) Method signature: find_parent(name, attrs, string, **kwargs) I spent a lot of time above covering find_all() and\\nfind(). The Beautiful Soup API defines ten other methods for\\nsearching the tree, but don’t be afraid. Five of these methods are\\nbasically the same as find_all(), and the other five are basically\\nthe same as find(). The only differences are in how they move from' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 41:\n",
      "page_content='basically the same as find_all(), and the other five are basically\\nthe same as find(). The only differences are in how they move from\\none part of the tree to another. First let’s consider find_parents() and\\nfind_parent(). Remember that find_all() and find() work\\ntheir way down the tree, looking at tag’s descendants. These methods\\ndo the opposite: they work their way up the tree, looking at a tag’s\\n(or a string’s) parents. Let’s try them out, starting from a string\\nburied deep in the “three daughters” document: One of the three <a> tags is the direct parent of the string in\\nquestion, so our search finds it. One of the three <p> tags is an\\nindirect parent (ancestor) of the string, and our search finds that as\\nwell. There’s a <p> tag with the CSS class “title” somewhere in the\\ndocument, but it’s not one of this string’s parents, so we can’t find\\nit with find_parents(). You may have noticed a similarity between find_parent() and\\nfind_parents(), and the .parent and .parents attributes' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 42:\n",
      "page_content='it with find_parents(). You may have noticed a similarity between find_parent() and\\nfind_parents(), and the .parent and .parents attributes\\nmentioned earlier. These search methods actually use the .parents\\nattribute to iterate through all parents (unfiltered), checking each one\\nagainst the provided filter to see if it matches. Method signature: find_next_siblings(name, attrs, string, limit, **kwargs) Method signature: find_next_sibling(name, attrs, string, **kwargs) These methods use .next_siblings to\\niterate over the rest of an element’s siblings in the tree. The\\nfind_next_siblings() method returns all the siblings that match,\\nand find_next_sibling() returns only the first one: Method signature: find_previous_siblings(name, attrs, string, limit, **kwargs) Method signature: find_previous_sibling(name, attrs, string, **kwargs) These methods use .previous_siblings to iterate over an element’s\\nsiblings that precede it in the tree. The find_previous_siblings()' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 43:\n",
      "page_content='siblings that precede it in the tree. The find_previous_siblings()\\nmethod returns all the siblings that match, and\\nfind_previous_sibling() returns only the first one: Method signature: find_all_next(name, attrs, string, limit, **kwargs) Method signature: find_next(name, attrs, string, **kwargs) These methods use .next_elements to\\niterate over whatever tags and strings that come after it in the\\ndocument. The find_all_next() method returns all matches, and\\nfind_next() returns only the first match: In the first example, the string “Elsie” showed up, even though it was\\ncontained within the <a> tag we started from. In the second example,\\nthe last <p> tag in the document showed up, even though it’s not in\\nthe same part of the tree as the <a> tag we started from. For these\\nmethods, all that matters is that an element matches the filter and' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 44:\n",
      "page_content='the same part of the tree as the <a> tag we started from. For these\\nmethods, all that matters is that an element matches the filter and\\nit shows up later in the document in document order. Method signature: find_all_previous(name, attrs, string, limit, **kwargs) Method signature: find_previous(name, attrs, string, **kwargs) These methods use .previous_elements to\\niterate over the tags and strings that came before it in the\\ndocument. The find_all_previous() method returns all matches, and\\nfind_previous() only returns the first match: The call to find_all_previous(\"p\") found the first paragraph in\\nthe document (the one with class=”title”), but it also finds the\\nsecond paragraph, the <p> tag that contains the <a> tag we started\\nwith. This shouldn’t be too surprising: we’re looking at all the tags\\nthat show up earlier in the document in document order than the one we started with. A\\n<p> tag that contains an <a> tag must have shown up before the <a>' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 45:\n",
      "page_content='<p> tag that contains an <a> tag must have shown up before the <a>\\ntag it contains. BeautifulSoup and Tag objects support CSS selectors through\\ntheir .css property. The actual selector implementation is handled\\nby the Soup Sieve\\npackage, available on PyPI as soupsieve. If you installed\\nBeautiful Soup through pip, Soup Sieve was installed at the same\\ntime, so you don’t have to do anything extra. The Soup Sieve documentation lists all the currently supported CSS\\nselectors, but\\nhere are some of the basics. You can find tags by name: Find tags by ID: Find tags contained anywhere within other tags: Find tags directly within other tags: Find all matching next siblings of tags: Find the next sibling tag (but only if it matches): Find tags by CSS class: Find tags that match any selector from a list of selectors: Test for the existence of an attribute: Find tags by attribute value: There’s also a method called select_one(), which finds only the' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 46:\n",
      "page_content='first tag that matches a selector: As a convenience, you can call select() and select_one() can\\ndirectly on the BeautifulSoup or Tag object, omitting the\\n.css property: CSS selector support is a convenience for people who already know the\\nCSS selector syntax. You can do all of this with the Beautiful Soup\\nAPI. If CSS selectors are all you need, you should skip Beautiful Soup\\naltogether and parse the document with lxml: it’s a lot\\nfaster. But Soup Sieve lets you combine CSS selectors with the\\nBeautiful Soup API. Soup Sieve offers a substantial API beyond the select() and\\nselect_one() methods, and you can access most of that API through\\nthe .css attribute of Tag or BeautifulSoup. What follows\\nis just a list of the supported methods; see the Soup Sieve\\ndocumentation for full\\ndocumentation. The iselect() method works the same as select(), but it\\nreturns a generator instead of a list: The closest() method returns the nearest parent of a given Tag' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 47:\n",
      "page_content='returns a generator instead of a list: The closest() method returns the nearest parent of a given Tag\\nthat matches a CSS selector, similar to Beautiful Soup’s\\nfind_parent() method: The match() method returns a Boolean depending on whether or not a\\nspecific Tag matches a selector: The filter() method returns the subset of a tag’s direct children\\nthat match a selector: The escape() method escapes CSS identifiers that would otherwise\\nbe invalid: If you’ve parsed XML that defines namespaces, you can use them in CSS\\nselectors.: Beautiful Soup tries to use namespace prefixes that make sense based\\non what it saw while parsing the document, but you can always provide\\nyour own dictionary of abbreviations: The .css property was added in Beautiful Soup 4.12.0. Prior to this,\\nonly the .select() and .select_one() convenience methods were\\nsupported. The Soup Sieve integration was added in Beautiful Soup 4.7.0. Earlier\\nversions had the .select() method, but only the most commonly-used' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 48:\n",
      "page_content='supported. The Soup Sieve integration was added in Beautiful Soup 4.7.0. Earlier\\nversions had the .select() method, but only the most commonly-used\\nCSS selectors were supported. Beautiful Soup’s main strength is in searching the parse tree, but you\\ncan also modify the tree and write your changes as a new HTML or XML\\ndocument. I covered this earlier, in Tag.attrs, but it bears repeating. You\\ncan rename a tag, change the values of its attributes, add new\\nattributes, and delete attributes: If you set a tag’s .string attribute to a new string, the tag’s contents are\\nreplaced with that string: Be careful: if the tag contained other tags, they and all their\\ncontents will be destroyed. You can add to a tag’s contents with Tag.append(). It works just\\nlike calling .append() on a Python list: Starting in Beautiful Soup 4.7.0, Tag also supports a method\\ncalled .extend(), which adds every element of a list to a Tag,\\nin order: If you need to add a string to a document, no problem–you can pass a' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 49:\n",
      "page_content='called .extend(), which adds every element of a list to a Tag,\\nin order: If you need to add a string to a document, no problem–you can pass a\\nPython string in to append(), or you can call the NavigableString\\nconstructor: If you want to create a comment or some other subclass of\\nNavigableString, just call the constructor: (This is a new feature in Beautiful Soup 4.4.0.) What if you need to create a whole new tag? The best solution is to\\ncall the factory method BeautifulSoup.new_tag(): Only the first argument, the tag name, is required. Tag.insert() is just like Tag.append(), except the new element\\ndoesn’t necessarily go at the end of its parent’s\\n.contents. It’ll be inserted at whatever numeric position you\\nsay. It works just like .insert() on a Python list: The insert_before() method inserts tags or strings immediately\\nbefore something else in the parse tree: The insert_after() method inserts tags or strings immediately' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 50:\n",
      "page_content='before something else in the parse tree: The insert_after() method inserts tags or strings immediately\\nfollowing something else in the parse tree: Tag.clear() removes the contents of a tag: PageElement.extract() removes a tag or string from the tree. It\\nreturns the tag or string that was extracted: At this point you effectively have two parse trees: one rooted at the\\nBeautifulSoup object you used to parse the document, and one rooted\\nat the tag that was extracted. You can go on to call extract() on\\na child of the element you extracted: Tag.decompose() removes a tag from the tree, then completely\\ndestroys it and its contents: The behavior of a decomposed Tag or NavigableString is not\\ndefined and you should not use it for anything. If you’re not sure\\nwhether something has been decomposed, you can check its\\n.decomposed property (new in Beautiful Soup 4.9.0): PageElement.replace_with() extracts a tag or string from the tree,' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 51:\n",
      "page_content='.decomposed property (new in Beautiful Soup 4.9.0): PageElement.replace_with() extracts a tag or string from the tree,\\nthen replaces it with one or more tags or strings of your choice: replace_with() returns the tag or string that got replaced, so\\nthat you can examine it or add it back to another part of the tree. The ability to pass multiple arguments into replace_with() is new\\nin Beautiful Soup 4.10.0. PageElement.wrap() wraps an element in the Tag object you specify. It\\nreturns the new wrapper: This method is new in Beautiful Soup 4.0.5. Tag.unwrap() is the opposite of wrap(). It replaces a tag with\\nwhatever’s inside that tag. It’s good for stripping out markup: Like replace_with(), unwrap() returns the tag\\nthat was replaced. After calling a bunch of methods that modify the parse tree, you may end up\\nwith two or more NavigableString objects next to each other.\\nBeautiful Soup doesn’t have any problems with this, but since it can’t happen' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 52:\n",
      "page_content='with two or more NavigableString objects next to each other.\\nBeautiful Soup doesn’t have any problems with this, but since it can’t happen\\nin a freshly parsed document, you might not expect behavior like the\\nfollowing: You can call Tag.smooth() to clean up the parse tree by consolidating adjacent strings: This method is new in Beautiful Soup 4.8.0. The prettify() method will turn a Beautiful Soup parse tree into a\\nnicely formatted Unicode string, with a separate line for each\\ntag and each string: You can call prettify() on the top-level BeautifulSoup object,\\nor on any of its Tag objects: Since it adds whitespace (in the form of newlines), prettify()\\nchanges the meaning of an HTML document and should not be used to\\nreformat one. The goal of prettify() is to help you visually\\nunderstand the structure of the documents you work with. If you just want a string, with no fancy formatting, you can call' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 53:\n",
      "page_content='understand the structure of the documents you work with. If you just want a string, with no fancy formatting, you can call\\nstr() on a BeautifulSoup object, or on a Tag within it: The str() function returns a string encoded in UTF-8. See\\nEncodings for other options. You can also call encode() to get a bytestring, and decode()\\nto get Unicode. If you give Beautiful Soup a document that contains HTML entities like\\n“&lquot;”, they’ll be converted to Unicode characters: If you then convert the document to a bytestring, the Unicode characters\\nwill be encoded as UTF-8. You won’t get the HTML entities back: By default, the only characters that are escaped upon output are bare\\nampersands and angle brackets. These get turned into “&amp;”, “&lt;”,\\nand “&gt;”, so that Beautiful Soup doesn’t inadvertently generate\\ninvalid HTML or XML: You can change this behavior by providing a value for the\\nformatter argument to prettify(), encode(), or\\ndecode(). Beautiful Soup recognizes five possible values for' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 54:\n",
      "page_content='formatter argument to prettify(), encode(), or\\ndecode(). Beautiful Soup recognizes five possible values for\\nformatter. The default is formatter=\"minimal\". Strings will only be processed\\nenough to ensure that Beautiful Soup generates valid HTML/XML: If you pass in formatter=\"html\", Beautiful Soup will convert\\nUnicode characters to HTML entities whenever possible: If you pass in formatter=\"html5\", it’s similar to\\nformatter=\"html\", but Beautiful Soup will\\nomit the closing slash in HTML void tags like “br”: In addition, any attributes whose values are the empty string\\nwill become HTML-style Boolean attributes: (This behavior is new as of Beautiful Soup 4.10.0.) If you pass in formatter=None, Beautiful Soup will not modify\\nstrings at all on output. This is the fastest option, but it may lead\\nto Beautiful Soup generating invalid HTML/XML, as in these examples: If you need more sophisticated control over your output, you can\\ninstantiate one of Beautiful Soup’s formatter classes and pass that' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 55:\n",
      "page_content='instantiate one of Beautiful Soup’s formatter classes and pass that\\nobject in as formatter. Used to customize the formatting rules for HTML documents. Here’s a formatter that converts strings to uppercase, whether they\\noccur in a string object or an attribute value: Here’s a formatter that increases the indentation width when pretty-printing: Used to customize the formatting rules for XML documents. Subclassing HTMLFormatter or XMLFormatter will\\ngive you even more control over the output. For example, Beautiful\\nSoup sorts the attributes in every tag by default: To turn this off, you can subclass the Formatter.attributes()\\nmethod, which controls which attributes are output and in what\\norder. This implementation also filters out the attribute called “m”\\nwhenever it appears: One last caveat: if you create a CData object, the text inside\\nthat object is always presented exactly as it appears, with no\\nformatting. Beautiful Soup will call your entity substitution' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 56:\n",
      "page_content='that object is always presented exactly as it appears, with no\\nformatting. Beautiful Soup will call your entity substitution\\nfunction, just in case you’ve written a custom function that counts\\nall the strings in the document or something, but it will ignore the\\nreturn value: If you only want the human-readable text inside a document or tag, you can use the\\nget_text() method. It returns all the text in a document or\\nbeneath a tag, as a single Unicode string: You can specify a string to be used to join the bits of text\\ntogether: You can tell Beautiful Soup to strip whitespace from the beginning and\\nend of each bit of text: But at that point you might want to use the .stripped_strings\\ngenerator instead, and process the text yourself: As of Beautiful Soup version 4.9.0, when lxml or html.parser are in\\nuse, the contents of <script>, <style>, and <template>\\ntags are generally not considered to be ‘text’, since those tags are not part of' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 57:\n",
      "page_content='use, the contents of <script>, <style>, and <template>\\ntags are generally not considered to be ‘text’, since those tags are not part of\\nthe human-visible content of the page. As of Beautiful Soup version 4.10.0, you can call get_text(),\\n.strings, or .stripped_strings on a NavigableString object. It will\\neither return the object itself, or nothing, so the only reason to do\\nthis is when you’re iterating over a mixed list. If you just need to parse some HTML, you can dump the markup into the\\nBeautifulSoup constructor, and it’ll probably be fine. Beautiful\\nSoup will pick a parser for you and parse the data. But there are a\\nfew additional arguments you can pass in to the constructor to change\\nwhich parser is used. The first argument to the BeautifulSoup constructor is a string or\\nan open filehandle—the source of the markup you want parsed. The second\\nargument is how you’d like the markup parsed. If you don’t specify anything, you’ll get the best HTML parser that’s' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 58:\n",
      "page_content='argument is how you’d like the markup parsed. If you don’t specify anything, you’ll get the best HTML parser that’s\\ninstalled. Beautiful Soup ranks lxml’s parser as being the best, then\\nhtml5lib’s, then Python’s built-in parser. You can override this by\\nspecifying one of the following: What type of markup you want to parse. Currently supported values are\\n“html”, “xml”, and “html5”. The name of the parser library you want to use. Currently supported\\noptions are “lxml”, “html5lib”, and “html.parser” (Python’s\\nbuilt-in HTML parser). The section Installing a parser contrasts the supported parsers. If you don’t have an appropriate parser installed, Beautiful Soup will\\nignore your request and pick a different parser. Right now, the only\\nsupported XML parser is lxml. If you don’t have lxml installed, asking\\nfor an XML parser won’t give you one, and asking for “lxml” won’t work\\neither. Beautiful Soup presents the same interface to a number of different' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 59:\n",
      "page_content='for an XML parser won’t give you one, and asking for “lxml” won’t work\\neither. Beautiful Soup presents the same interface to a number of different\\nparsers, but each parser is different. Different parsers will create\\ndifferent parse trees from the same document. The biggest differences\\nare between the HTML parsers and the XML parsers. Here’s a short\\ndocument, parsed as HTML using the parser that comes with Python: Since a standalone <b/> tag is not valid HTML, html.parser turns it into\\na <b></b> tag pair. Here’s the same document parsed as XML (running this requires that you\\nhave lxml installed). Note that the standalone <b/> tag is left alone, and\\nthat the document is given an XML declaration instead of being put\\ninto an <html> tag.: There are also differences between HTML parsers. If you give Beautiful\\nSoup a perfectly-formed HTML document, these differences won’t\\nmatter. One parser will be faster than another, but they’ll all give' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 60:\n",
      "page_content='Soup a perfectly-formed HTML document, these differences won’t\\nmatter. One parser will be faster than another, but they’ll all give\\nyou a data structure that looks exactly like the original HTML\\ndocument. But if the document is not perfectly-formed, different parsers will\\ngive different results. Here’s a short, invalid document parsed using\\nlxml’s HTML parser. Note that the <a> tag gets wrapped in <body> and\\n<html> tags, and the dangling </p> tag is simply ignored: Here’s the same document parsed using html5lib: Instead of ignoring the dangling </p> tag, html5lib pairs it with an\\nopening <p> tag. html5lib also adds an empty <head> tag; lxml didn’t\\nbother. Here’s the same document parsed with Python’s built-in HTML\\nparser: Like lxml, this parser ignores the closing </p> tag. Unlike\\nhtml5lib or lxml, this parser makes no attempt to create a\\nwell-formed HTML document by adding <html> or <body> tags. Since the document “<a></p>” is invalid, none of these techniques is' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 61:\n",
      "page_content='well-formed HTML document by adding <html> or <body> tags. Since the document “<a></p>” is invalid, none of these techniques is\\nthe ‘correct’ way to handle it. The html5lib parser uses techniques\\nthat are part of the HTML5 standard, so it has the best claim on being\\nthe ‘correct’ way, but all three techniques are legitimate. Differences between parsers can affect your script. If you’re planning\\non distributing your script to other people, or running it on multiple\\nmachines, you should specify a parser in the BeautifulSoup\\nconstructor. That will reduce the chances that your users parse a\\ndocument differently from the way you parse it. Any HTML or XML document is written in a specific encoding like ASCII\\nor UTF-8. But when you load that document into Beautiful Soup, you’ll\\ndiscover it’s been converted to Unicode: It’s not magic. (That sure would be nice.) Beautiful Soup uses a\\nsub-library called Unicode, Dammit to detect a document’s encoding' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 62:\n",
      "page_content='sub-library called Unicode, Dammit to detect a document’s encoding\\nand convert it to Unicode. The autodetected encoding is available as\\nthe .original_encoding attribute of the BeautifulSoup object: Unicode, Dammit guesses correctly most of the time, but sometimes it\\nmakes mistakes. Sometimes it guesses correctly, but only after a\\nbyte-by-byte search of the document that takes a very long time. If\\nyou happen to know a document’s encoding ahead of time, you can avoid\\nmistakes and delays by passing it to the BeautifulSoup constructor\\nas from_encoding. Here’s a document written in ISO-8859-8. The document is so short that\\nUnicode, Dammit can’t get a lock on it, and misidentifies it as\\nISO-8859-7: We can fix this by passing in the correct from_encoding: If you don’t know what the correct encoding is, but you know that\\nUnicode, Dammit is guessing wrong, you can pass the wrong guesses in\\nas exclude_encodings: Windows-1255 isn’t 100% correct, but that encoding is a compatible' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 63:\n",
      "page_content='as exclude_encodings: Windows-1255 isn’t 100% correct, but that encoding is a compatible\\nsuperset of ISO-8859-8, so it’s close enough. (exclude_encodings\\nis a new feature in Beautiful Soup 4.4.0.) In rare cases (usually when a UTF-8 document contains text written in\\na completely different encoding), the only way to get Unicode may be\\nto replace some characters with the special Unicode character\\n“REPLACEMENT CHARACTER” (U+FFFD, �). If Unicode, Dammit needs to do\\nthis, it will set the .contains_replacement_characters attribute\\nto True on the UnicodeDammit or BeautifulSoup object. This\\nlets you know that the Unicode representation is not an exact\\nrepresentation of the original–some data was lost. If a document\\ncontains �, but .contains_replacement_characters is False,\\nyou’ll know that the � was there originally (as it is in this\\nparagraph) and doesn’t stand in for missing data. When you write out an output document from Beautiful Soup, you get a UTF-8' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 64:\n",
      "page_content='paragraph) and doesn’t stand in for missing data. When you write out an output document from Beautiful Soup, you get a UTF-8\\ndocument, even if the input document wasn’t in UTF-8 to begin with. Here’s a\\ndocument written in the Latin-1 encoding: Note that the <meta> tag has been rewritten to reflect the fact that\\nthe document is now in UTF-8. If you don’t want UTF-8, you can pass an encoding into prettify(): You can also call encode() on the BeautifulSoup object, or any\\nelement in the soup, just as if it were a Python string: Any characters that can’t be represented in your chosen encoding will\\nbe converted into numeric XML entity references. Here’s a document\\nthat includes the Unicode character SNOWMAN: The SNOWMAN character can be part of a UTF-8 document (it looks like\\n☃), but there’s no representation for that character in ISO-Latin-1 or\\nASCII, so it’s converted into “&#9731” for those encodings: You can use Unicode, Dammit without using Beautiful Soup. It’s useful' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 65:\n",
      "page_content='ASCII, so it’s converted into “&#9731” for those encodings: You can use Unicode, Dammit without using Beautiful Soup. It’s useful\\nwhenever you have data in an unknown encoding and you just want it to\\nbecome Unicode: Unicode, Dammit’s guesses will get a lot more accurate if you install\\none of these Python libraries: charset-normalizer, chardet, or\\ncchardet. The more data you give Unicode, Dammit, the more\\naccurately it will guess. If you have your own suspicions as to what\\nthe encoding might be, you can pass them in as a list: Unicode, Dammit has two special features that Beautiful Soup doesn’t\\nuse. You can use Unicode, Dammit to convert Microsoft smart quotes to HTML or XML\\nentities: You can also convert Microsoft smart quotes to ASCII quotes: Hopefully you’ll find this feature useful, but Beautiful Soup doesn’t\\nuse it. Beautiful Soup prefers the default behavior, which is to\\nconvert Microsoft smart quotes to Unicode characters along with' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 66:\n",
      "page_content='use it. Beautiful Soup prefers the default behavior, which is to\\nconvert Microsoft smart quotes to Unicode characters along with\\neverything else: Sometimes a document is mostly in UTF-8, but contains Windows-1252\\ncharacters such as (again) Microsoft smart quotes. This can happen\\nwhen a website includes data from multiple sources. You can use\\nUnicodeDammit.detwingle() to turn such a document into pure\\nUTF-8. Here’s a simple example: This document is a mess. The snowmen are in UTF-8 and the quotes are\\nin Windows-1252. You can display the snowmen or the quotes, but not\\nboth: Decoding the document as UTF-8 raises a UnicodeDecodeError, and\\ndecoding it as Windows-1252 gives you gibberish. Fortunately,\\nUnicodeDammit.detwingle() will convert the string to pure UTF-8,\\nallowing you to decode it to Unicode and display the snowmen and quote\\nmarks simultaneously: UnicodeDammit.detwingle() only knows how to handle Windows-1252\\nembedded in UTF-8 (or vice versa, I suppose), but this is the most' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 67:\n",
      "page_content='embedded in UTF-8 (or vice versa, I suppose), but this is the most\\ncommon case. Note that you must know to call UnicodeDammit.detwingle() on your\\ndata before passing it into BeautifulSoup or the UnicodeDammit\\nconstructor. Beautiful Soup assumes that a document has a single\\nencoding, whatever it might be. If you pass it a document that\\ncontains both UTF-8 and Windows-1252, it’s likely to think the whole\\ndocument is Windows-1252, and the document will come out looking like\\nâ˜ƒâ˜ƒâ˜ƒ“I like snowmen!”. UnicodeDammit.detwingle() is new in Beautiful Soup 4.1.0. The html.parser and html5lib parsers can keep track of where in\\nthe original document each Tag was found. You can access this\\ninformation as Tag.sourceline (line number) and Tag.sourcepos\\n(position of the start tag within a line): Note that the two parsers mean slightly different things by\\nsourceline and sourcepos. For html.parser, these numbers\\nrepresent the position of the initial less-than sign. For html5lib,' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 68:\n",
      "page_content='sourceline and sourcepos. For html.parser, these numbers\\nrepresent the position of the initial less-than sign. For html5lib,\\nthese numbers represent the position of the final greater-than sign: You can shut off this feature by passing store_line_numbers=False\\ninto the BeautifulSoup constructor: This feature is new in 4.8.1, and the parsers based on lxml don’t\\nsupport it. Beautiful Soup says that two NavigableString or Tag objects\\nare equal when they represent the same HTML or XML markup, even if their\\nattributes are in a different order or they live in different parts of the\\nobject tree. In this example, the two <b> tags are treated as equal, because\\nthey both look like “<b>pizza</b>”: If you want to see whether two variables refer to exactly the same\\nobject, use is: You can use copy.copy() to create a copy of any Tag or\\nNavigableString: The copy is considered equal to the original, since it represents the' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 69:\n",
      "page_content='NavigableString: The copy is considered equal to the original, since it represents the\\nsame markup as the original, but it’s not the same object: The only real difference is that the copy is completely detached from\\nthe original Beautiful Soup object tree, just as if extract() had\\nbeen called on it: This is because two different Tag objects can’t occupy the same\\nspace at the same time. Beautiful Soup offers a number of ways to customize how the parser\\ntreats incoming HTML and XML. This section covers the most commonly\\nused customization techniques. Let’s say you want to use Beautiful Soup to look at a document’s <a>\\ntags. It’s a waste of time and memory to parse the entire document and\\nthen go over it again looking for <a> tags. It would be much faster to\\nignore everything that wasn’t an <a> tag in the first place. The\\nSoupStrainer class allows you to choose which parts of an incoming\\ndocument are parsed. You just create a SoupStrainer and pass it in' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 70:\n",
      "page_content='SoupStrainer class allows you to choose which parts of an incoming\\ndocument are parsed. You just create a SoupStrainer and pass it in\\nto the BeautifulSoup constructor as the parse_only argument. (Note that this feature won’t work if you’re using the html5lib parser.\\nIf you use html5lib, the whole document will be parsed, no\\nmatter what. This is because html5lib constantly rearranges the parse\\ntree as it works, and if some part of the document didn’t actually\\nmake it into the parse tree, it’ll crash. To avoid confusion, in the\\nexamples below I’ll be forcing Beautiful Soup to use Python’s\\nbuilt-in parser.) The SoupStrainer class takes the same arguments as a typical\\nmethod from Searching the tree: name, attrs, string, and **kwargs. Here are\\nthree SoupStrainer objects: I’m going to bring back the “three sisters” document one more time,\\nand we’ll see what the document looks like when it’s parsed with these' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 71:\n",
      "page_content='and we’ll see what the document looks like when it’s parsed with these\\nthree SoupStrainer objects: The SoupStrainer behavior is as follows: When a tag matches, it is kept (including all its contents, whether they also\\nmatch or not). When a tag does not match, the tag itself is not kept, but parsing continues\\ninto its contents to look for other tags that do match. You can also pass a SoupStrainer into any of the methods covered\\nin Searching the tree. This probably isn’t terribly useful, but I\\nthought I’d mention it: In an HTML document, an attribute like class is given a list of\\nvalues, and an attribute like id is given a single value, because\\nthe HTML specification treats those attributes differently: You can turn this off by passing in\\nmulti_valued_attributes=None. Than all attributes will be given a\\nsingle value: You can customize this behavior quite a bit by passing in a\\ndictionary for multi_valued_attributes. If you need this, look at' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 72:\n",
      "page_content=\"single value: You can customize this behavior quite a bit by passing in a\\ndictionary for multi_valued_attributes. If you need this, look at\\nHTMLTreeBuilder.DEFAULT_CDATA_LIST_ATTRIBUTES to see the\\nconfiguration Beautiful Soup uses by default, which is based on the\\nHTML specification. (This is a new feature in Beautiful Soup 4.8.0.) When using the html.parser parser, you can use the\\non_duplicate_attribute constructor argument to customize what\\nBeautiful Soup does when it encounters a tag that defines the same\\nattribute more than once: The default behavior is to use the last value found for the tag: With on_duplicate_attribute='ignore' you can tell Beautiful Soup\\nto use the first value found and ignore the rest: (lxml and html5lib always do it this way; their behavior can’t be\\nconfigured from within Beautiful Soup.) If you need more control, you can pass in a function that’s called on each\" metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 73:\n",
      "page_content='configured from within Beautiful Soup.) If you need more control, you can pass in a function that’s called on each\\nduplicate value: (This is a new feature in Beautiful Soup 4.9.1.) When a parser tells Beautiful Soup about a tag or a string, Beautiful\\nSoup will instantiate a Tag or NavigableString object to\\ncontain that information. Instead of that default behavior, you can\\ntell Beautiful Soup to instantiate subclasses of Tag or\\nNavigableString, subclasses you define with custom behavior: This can be useful when incorporating Beautiful Soup into a test\\nframework. (This is a new feature in Beautiful Soup 4.8.1.) If you’re having trouble understanding what Beautiful Soup does to a\\ndocument, pass the document into the diagnose() function. (This function is new in\\nBeautiful Soup 4.2.0.) Beautiful Soup will print out a report showing\\nyou how different parsers handle the document, and tell you if you’re' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 74:\n",
      "page_content='Beautiful Soup 4.2.0.) Beautiful Soup will print out a report showing\\nyou how different parsers handle the document, and tell you if you’re\\nmissing a parser that Beautiful Soup could be using: Just looking at the output of diagnose() might show you how to solve the\\nproblem. Even if not, you can paste the output of diagnose() when\\nasking for help. There are two different kinds of parse errors. There are crashes,\\nwhere you feed a document to Beautiful Soup and it raises an\\nexception (usually an HTMLParser.HTMLParseError). And there is\\nunexpected behavior, where a Beautiful Soup parse tree looks a lot\\ndifferent than the document used to create it. These problems are almost never problems with Beautiful Soup itself.\\nThis is not because Beautiful Soup is an amazingly well-written piece\\nof software. It’s because Beautiful Soup doesn’t include any parsing\\ncode. Instead, it relies on external parsers. If one parser isn’t\\nworking on a certain document, the best solution is to try a different' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 75:\n",
      "page_content=\"code. Instead, it relies on external parsers. If one parser isn’t\\nworking on a certain document, the best solution is to try a different\\nparser. See Installing a parser for details and a parser\\ncomparison. If this doesn’t help, you might need to inspect the\\ndocument tree found inside the BeautifulSoup object, to see where\\nthe markup you’re looking for actually ended up. SyntaxError: Invalid syntax (on the line ROOT_TAG_NAME =\\n'[document]'): Caused by running an old Python 2 version of\\nBeautiful Soup under Python 3, without converting the code. ImportError: No module named HTMLParser - Caused by running an old\\nPython 2 version of Beautiful Soup under Python 3. ImportError: No module named html.parser - Caused by running the\\nPython 3 version of Beautiful Soup under Python 2. ImportError: No module named BeautifulSoup - Caused by running\\nBeautiful Soup 3 code in an environment that doesn’t have BS3\\ninstalled. Or, by writing Beautiful Soup 4 code without knowing that\" metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 76:\n",
      "page_content='Beautiful Soup 3 code in an environment that doesn’t have BS3\\ninstalled. Or, by writing Beautiful Soup 4 code without knowing that\\nthe package name has changed to bs4. ImportError: No module named bs4 - Caused by running Beautiful\\nSoup 4 code in an environment that doesn’t have BS4 installed. By default, Beautiful Soup parses documents as HTML. To parse a\\ndocument as XML, pass in “xml” as the second argument to the\\nBeautifulSoup constructor: You’ll need to have lxml installed. If your script works on one computer but not another, or in one\\nvirtual environment but not another, or outside the virtual\\nenvironment but not inside, it’s probably because the two\\nenvironments have different parser libraries available. For example,\\nyou may have developed the script on a computer that has lxml\\ninstalled, and then tried to run it on a computer that only has\\nhtml5lib installed. See Differences between parsers for why this\\nmatters, and fix the problem by mentioning a specific parser library' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 77:\n",
      "page_content=\"html5lib installed. See Differences between parsers for why this\\nmatters, and fix the problem by mentioning a specific parser library\\nin the BeautifulSoup constructor. Because HTML tags and attributes are case-insensitive, all three HTML\\nparsers convert tag and attribute names to lowercase. That is, the\\nmarkup <TAG></TAG> is converted to <tag></tag>. If you want to\\npreserve mixed-case or uppercase tags and attributes, you’ll need to\\nparse the document as XML. UnicodeEncodeError: 'charmap' codec can't encode character\\n'\\\\xfoo' in position bar (or just about any other\\nUnicodeEncodeError) - This problem shows up in two main\\nsituations. First, when you try to print a Unicode character that\\nyour console doesn’t know how to display. (See this page on the\\nPython wiki for help.)\\nSecond, when you’re writing to a file and you pass in a Unicode\\ncharacter that’s not supported by your default encoding. In this\\ncase, the simplest solution is to explicitly encode the Unicode\" metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 78:\n",
      "page_content='character that’s not supported by your default encoding. In this\\ncase, the simplest solution is to explicitly encode the Unicode\\nstring into UTF-8 with u.encode(\"utf8\"). KeyError: [attr] - Caused by accessing tag[\\'attr\\'] when the\\ntag in question doesn’t define the attr attribute. The most\\ncommon errors are KeyError: \\'href\\' and KeyError: \\'class\\'.\\nUse tag.get(\\'attr\\') if you’re not sure attr is\\ndefined, just as you would with a Python dictionary. AttributeError: \\'ResultSet\\' object has no attribute \\'foo\\' - This\\nusually happens because you expected find_all() to return a\\nsingle tag or string. But find_all() returns a list of tags\\nand strings–a ResultSet object. You need to iterate over the\\nlist and look at the .foo of each one. Or, if you really only\\nwant one result, you need to use find() instead of\\nfind_all(). AttributeError: \\'NoneType\\' object has no attribute \\'foo\\' - This\\nusually happens because you called find() and then tried to' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 79:\n",
      "page_content=\"find_all(). AttributeError: 'NoneType' object has no attribute 'foo' - This\\nusually happens because you called find() and then tried to\\naccess the .foo` attribute of the result. But in your case,\\nfind() didn’t find anything, so it returned None, instead of\\nreturning a tag or a string. You need to figure out why your\\nfind() call isn’t returning anything. AttributeError: 'NavigableString' object has no attribute\\n'foo' - This usually happens because you’re treating a string as\\nthough it were a tag. You may be iterating over a list, expecting\\nthat it contains nothing but tags, when it actually contains both tags and\\nstrings. Beautiful Soup will never be as fast as the parsers it sits on top\\nof. If response time is critical, if you’re paying for computer time\\nby the hour, or if there’s any other reason why computer time is more\\nvaluable than programmer time, you should forget about Beautiful Soup\" metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 80:\n",
      "page_content='by the hour, or if there’s any other reason why computer time is more\\nvaluable than programmer time, you should forget about Beautiful Soup\\nand work directly atop lxml. That said, there are things you can do to speed up Beautiful Soup. If\\nyou’re not using lxml as the underlying parser, my advice is to\\nstart. Beautiful Soup parses documents\\nsignificantly faster using lxml than using html.parser or html5lib. You can speed up encoding detection significantly by installing the\\ncchardet library. Parsing only part of a document won’t save you much time parsing\\nthe document, but it can save a lot of memory, and it’ll make\\nsearching the document much faster. New translations of the Beautiful Soup documentation are greatly\\nappreciated. Translations should be licensed under the MIT license,\\njust like Beautiful Soup and its English documentation are. There are two ways of getting your translation into the main code base' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 81:\n",
      "page_content='just like Beautiful Soup and its English documentation are. There are two ways of getting your translation into the main code base\\nand onto the Beautiful Soup website: Create a branch of the Beautiful Soup repository, add your\\ntranslation, and propose a merge with the main branch, the same\\nas you would do with a proposed change to the source code. Send a message to the Beautiful Soup discussion group with a link to\\nyour translation, or attach your translation to the message. Use the Chinese or Brazilian Portuguese translations as your model. In\\nparticular, please translate the source file doc/source/index.rst,\\nrather than the HTML version of the documentation. This makes it\\npossible to publish the documentation in a variety of formats, not\\njust HTML. Beautiful Soup 3 is the previous release series, and is no longer\\nsupported. Development of Beautiful Soup 3 stopped in 2012, and the\\npackage was completely discontinued in 2021. There’s no reason to' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 82:\n",
      "page_content='supported. Development of Beautiful Soup 3 stopped in 2012, and the\\npackage was completely discontinued in 2021. There’s no reason to\\ninstall it unless you’re trying to get very old software to work, but\\nit’s published through PyPi as BeautifulSoup: $ pip install BeautifulSoup You can also download a tarball of the final release, 3.2.2. If you ran pip install beautifulsoup or pip install\\nBeautifulSoup, but your code doesn’t work, you installed Beautiful\\nSoup 3 by mistake. You need to run pip install beautifulsoup4. The documentation for Beautiful Soup 3 is archived online. Most code written against Beautiful Soup 3 will work against Beautiful\\nSoup 4 with one simple change. All you should have to do is change the\\npackage name from BeautifulSoup to bs4. So this: becomes this: If you get the ImportError “No module named BeautifulSoup”, your\\nproblem is that you’re trying to run Beautiful Soup 3 code, but you' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 83:\n",
      "page_content='problem is that you’re trying to run Beautiful Soup 3 code, but you\\nonly have Beautiful Soup 4 installed. If you get the ImportError “No module named bs4”, your problem\\nis that you’re trying to run Beautiful Soup 4 code, but you only\\nhave Beautiful Soup 3 installed. Although BS4 is mostly backward-compatible with BS3, most of its\\nmethods have been deprecated and given new names for PEP 8 compliance. There are numerous other\\nrenames and changes, and a few of them break backward compatibility. Here’s what you’ll need to know to convert your BS3 code and habits to BS4: Beautiful Soup 3 used Python’s SGMLParser, a module that was\\ndeprecated and removed in Python 3.0. Beautiful Soup 4 uses\\nhtml.parser by default, but you can plug in lxml or html5lib and\\nuse that instead. See Installing a parser for a comparison. Since html.parser is not the same parser as SGMLParser, you\\nmay find that Beautiful Soup 4 gives you a different parse tree than' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 84:\n",
      "page_content='may find that Beautiful Soup 4 gives you a different parse tree than\\nBeautiful Soup 3 for the same markup. If you swap out html.parser\\nfor lxml or html5lib, you may find that the parse tree changes yet\\nagain. If this happens, you’ll need to update your scraping code to\\nprocess the new tree. renderContents -> encode_contents replaceWith -> replace_with replaceWithChildren -> unwrap findAll -> find_all findAllNext -> find_all_next findAllPrevious -> find_all_previous findNext -> find_next findNextSibling -> find_next_sibling findNextSiblings -> find_next_siblings findParent -> find_parent findParents -> find_parents findPrevious -> find_previous findPreviousSibling -> find_previous_sibling findPreviousSiblings -> find_previous_siblings getText -> get_text nextSibling -> next_sibling previousSibling -> previous_sibling Some arguments to the Beautiful Soup constructor were renamed for the' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 85:\n",
      "page_content='same reasons: BeautifulSoup(parseOnlyThese=...) -> BeautifulSoup(parse_only=...) BeautifulSoup(fromEncoding=...) -> BeautifulSoup(from_encoding=...) I renamed one method for compatibility with Python 3: Tag.has_key() -> Tag.has_attr() I renamed one attribute to use more accurate terminology: Tag.isSelfClosing -> Tag.is_empty_element I renamed three attributes to avoid using words that have special\\nmeaning to Python. Unlike the others, these changes are not backwards\\ncompatible. If you used these attributes in BS3, your code will break\\nin BS4 until you change them. UnicodeDammit.unicode -> UnicodeDammit.unicode_markup Tag.next -> Tag.next_element Tag.previous -> Tag.previous_element These methods are left over from the Beautiful Soup 2 API. They’ve' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 86:\n",
      "page_content='been deprecated since 2006 and should not be used at all: Tag.fetchNextSiblings Tag.fetchPreviousSiblings Tag.fetchPrevious Tag.fetchPreviousSiblings Tag.fetchParents Tag.findChild Tag.findChildren I gave the generators PEP 8-compliant names, and transformed them into\\nproperties: childGenerator() -> children nextGenerator() -> next_elements nextSiblingGenerator() -> next_siblings previousGenerator() -> previous_elements previousSiblingGenerator() -> previous_siblings recursiveChildGenerator() -> descendants parentGenerator() -> parents So instead of this: You can write this: (But the old code will still work.) Some of the generators used to yield None after they were done, and\\nthen stop. That was a bug. Now the generators just stop. There are two new generators, .strings and\\n.stripped_strings. .strings yields\\nNavigableString objects, and .stripped_strings yields Python\\nstrings that have had whitespace stripped. There is no longer a BeautifulStoneSoup class for parsing XML. To' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 87:\n",
      "page_content='strings that have had whitespace stripped. There is no longer a BeautifulStoneSoup class for parsing XML. To\\nparse XML you pass in “xml” as the second argument to the\\nBeautifulSoup constructor. For the same reason, the\\nBeautifulSoup constructor no longer recognizes the isHTML\\nargument. Beautiful Soup’s handling of empty-element XML tags has been\\nimproved. Previously when you parsed XML you had to explicitly say\\nwhich tags were considered empty-element tags. The selfClosingTags\\nargument to the constructor is no longer recognized. Instead,\\nBeautiful Soup considers any empty tag to be an empty-element tag. If\\nyou add a child to an empty-element tag, it stops being an\\nempty-element tag. An incoming HTML or XML entity is always converted into the\\ncorresponding Unicode character. Beautiful Soup 3 had a number of\\noverlapping ways of dealing with entities, which have been\\nremoved. The BeautifulSoup constructor no longer recognizes the\\nsmartQuotesTo or convertEntities arguments. (Unicode,' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 88:\n",
      "page_content='removed. The BeautifulSoup constructor no longer recognizes the\\nsmartQuotesTo or convertEntities arguments. (Unicode,\\nDammit still has smart_quotes_to, but its default is now to turn\\nsmart quotes into Unicode.) The constants HTML_ENTITIES,\\nXML_ENTITIES, and XHTML_ENTITIES have been removed, since they\\nconfigure a feature (transforming some but not all entities into\\nUnicode characters) that no longer exists. If you want to turn Unicode characters back into HTML entities on\\noutput, rather than turning them into UTF-8 characters, you need to\\nuse an output formatter. Tag.string now operates recursively. If tag A\\ncontains a single tag B and nothing else, then A.string is the same as\\nB.string. (Previously, it was None.) Multi-valued attributes like class have lists of strings as\\ntheir values, not simple strings. This may affect the way you search by CSS\\nclass. Tag objects now implement the __hash__ method, such that two\\nTag objects are considered equal if they generate the same' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n",
      "Page 89:\n",
      "page_content='class. Tag objects now implement the __hash__ method, such that two\\nTag objects are considered equal if they generate the same\\nmarkup. This may change your script’s behavior if you put Tag\\nobjects into a dictionary or set. If you pass one of the find* methods both string and\\na tag-specific argument like name, Beautiful Soup will\\nsearch for tags that match your tag-specific criteria and whose\\nTag.string matches your string\\nvalue. It will not find the strings themselves. Previously,\\nBeautiful Soup ignored the tag-specific arguments and looked for\\nstrings. The BeautifulSoup constructor no longer recognizes the\\nmarkupMassage argument. It’s now the parser’s responsibility to\\nhandle markup correctly. The rarely-used alternate parser classes like\\nICantBelieveItsBeautifulSoup and BeautifulSOAP have been\\nremoved. It’s now the parser’s decision how to handle ambiguous\\nmarkup. The prettify() method now returns a Unicode string, not a bytestring.' metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "cleaned_r_pages = r_splitter.split_documents(docs)\n",
    "cleaned_c_pages = c_splitter.split_documents(docs)\n",
    "\n",
    "print(len(cleaned_r_pages))\n",
    "print(len(cleaned_c_pages))\n",
    "\n",
    "# Print the cleaned pages\n",
    "for i, page in enumerate(cleaned_r_pages):\n",
    "    print(f\"Page {i+1}:\\n{page}\\n{'-'*40}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Lok Sabha MP Om Birla was elected as Lok Sabha Speaker Wednesday by a voice vote for the second time in a row. The Opposition did not press for a division of votes and pro-tem Speaker Bhartruhari Mahtab declared Birla as elected, saying the “ayes have it”. With Birla’s election, Mahtab said, the other motions to propose and second the candidature of K Suresh became infructuous. Prime Minister Narendra Modi and senior Congress leader Rahul Gandhi, whom the Congress has named Leader of the Opposition in the Lok Sabha, congratulated Birla as he took charge. While PM Modi recalled the milestones of his first term as Speaker, Rahul Gandhi sought a voice for the Opposition and Samajwadi Party (SP) leader Akhilesh Yadav said he expects that members will not be suspended for raising their voices in the House. Modi said that Birla was becoming Speaker for the second time after a full term, like Balram Jakhar. He added that such was the work of the Speaker that past Speakers often either did not', metadata={'source': 'https://indianexpress.com/article/india/om-birla-election-lok-sabha-speaker-modi-opposition-9415980/'}),\n",
       " Document(page_content='for the second time after a full term, like Balram Jakhar. He added that such was the work of the Speaker that past Speakers often either did not contest another election or lost it. “The works that did not happen during 70 years of independence were made possible by this House under your chairmanship. Several milestones come in the long journey of democracy. A few occasions are such when we get the opportunity to establish milestones. I am very confident that the country will be proud of the achievements of the 17th Lok Sabha,” Modi said, recalling the Nari Shakti Vandan Adhiniyam to facilitate women’s reservation, the new penal laws, the reorganisation of Jammu and Kashmir after the abrogation of Article 370, etc. “I want to congratulate you on behalf of the House. It is a huge responsibility for you to sit on this post for the second time during Amrit Kaal. With your experience, we hope that you will guide us for the next five years. Aapke chehre par ye meethi meethi muskaan poore', metadata={'source': 'https://indianexpress.com/article/india/om-birla-election-lok-sabha-speaker-modi-opposition-9415980/'}),\n",
       " Document(page_content='time during Amrit Kaal. With your experience, we hope that you will guide us for the next five years. Aapke chehre par ye meethi meethi muskaan poore sadan ko prasann rakhti hai (This sweet smile on your face makes the whole House happy),” Modi said. The PM also noted that Birla had seen the shifting of the House from the old building to the new building, which happened in the first term of Birla. He recalled that Birla had as Speaker stayed in touch with members of the House when they were infected by coronavirus, and added that he would feel happy when Opposition members also told him so, showing that Birla helped members tide over the Covid crisis like the mukhiya (head) of a family. He also recalled that Birla had to sometimes take tough decisions to restore the dignity of the House with a heavy heart, but always saw the dignity of the House as supreme. Speaking after the PM, Rahul Gandhi congratulated Birla for being elected to the post for the second time and said he was', metadata={'source': 'https://indianexpress.com/article/india/om-birla-election-lok-sabha-speaker-modi-opposition-9415980/'}),\n",
       " Document(page_content='of the House as supreme. Speaking after the PM, Rahul Gandhi congratulated Birla for being elected to the post for the second time and said he was confident that the Speaker would allow the Opposition to represent their voices and the voice of the people of India. “I would like to congratulate you on behalf of the entire Opposition and INDIA alliance. Speaker sir, this House represents the voice of India’s people. And you are the final arbiter of that voice.” “Of course, the government has political power but the Opposition also represents the voice of the people. And this time, the Opposition represents significantly more voice of the Indian people than it did last time,” he added. “The Opposition would like to assist you in doing your work. We would like the House to function often and well. It is very important that cooperation happens on the basis of trust. It is very important that the voice of the Opposition is allowed to be represented in this House. I am confident that you', metadata={'source': 'https://indianexpress.com/article/india/om-birla-election-lok-sabha-speaker-modi-opposition-9415980/'}),\n",
       " Document(page_content='on the basis of trust. It is very important that the voice of the Opposition is allowed to be represented in this House. I am confident that you will allow us to represent our voice, allow us to speak to represent the voice of the people of India,” Gandhi said. “The question is not how efficiently the House is run; the question is how much of India’s voice is being allowed to be heard in this House. So the idea that you can run the House efficiently by silencing the voice of the Opposition is a non-democratic idea. And this election has shown that the people of India expect the Opposition to defend the Constitution of the country. And we are confident that by allowing the Opposition to speak to represent the people of India, you will do your duty of defending the Constitution of India,” said Gandhi. Congratulating Birla, Akhilesh Yadav, whose party is third largest in the present Lok Sabha with 37 MPs, expressed the hope that there would be no discrimination and the Speaker would', metadata={'source': 'https://indianexpress.com/article/india/om-birla-election-lok-sabha-speaker-modi-opposition-9415980/'}),\n",
       " Document(page_content='whose party is third largest in the present Lok Sabha with 37 MPs, expressed the hope that there would be no discrimination and the Speaker would provide equal opportunity to MPs of Opposition parties in the House. Yadav added that he expects that the voices of public representatives are not suppressed and the dignity of the House is not hurt by actions like suspension of the members – obliquely hinting at about 100 suspensions within a week from the House in December 2023. “Impartiality is a great responsibility of this great position…,” Yadav said, adding that not just the Opposition but the treasury benches should also be under the control of the Speaker. Addressing the Speaker, Yadav said, “Sadan aapke ishaare par chale, iske ulta na ho (the House should function under your control and not vice versa).” The Opposition is demanding the post of Deputy Speaker in Lok Sabha, which has been vacant since the start of the 17th Lok Sabha. Rahul Gandhi of Congress has stated that the', metadata={'source': 'https://indianexpress.com/article/india/om-birla-election-lok-sabha-speaker-modi-opposition-9415980/'}),\n",
       " Document(page_content=\"the post of Deputy Speaker in Lok Sabha, which has been vacant since the start of the 17th Lok Sabha. Rahul Gandhi of Congress has stated that the Opposition will support the NDA's candidate for Speaker if the Deputy Speaker's post is given to the Opposition. However, governments have often overlooked the constitutional mandate to appoint both positions promptly.\", metadata={'source': 'https://indianexpress.com/article/india/om-birla-election-lok-sabha-speaker-modi-opposition-9415980/'}),\n",
       " Document(page_content='Beautiful Soup is a\\nPython library for pulling data out of HTML and XML files. It works\\nwith your favorite parser to provide idiomatic ways of navigating,\\nsearching, and modifying the parse tree. It commonly saves programmers\\nhours or days of work. These instructions illustrate all major features of Beautiful Soup 4,\\nwith examples. I show you what the library is good for, how it works,\\nhow to use it, how to make it do what you want, and what to do when it\\nviolates your expectations. This document covers Beautiful Soup version 4.12.2. The examples in\\nthis documentation were written for Python 3.8. You might be looking for the documentation for Beautiful Soup 3.\\nIf so, you should know that Beautiful Soup 3 is no longer being\\ndeveloped and that all support for it was dropped on December\\n31, 2020. If you want to learn about the differences between Beautiful\\nSoup 3 and Beautiful Soup 4, see Porting code to BS4. This documentation has been translated into other languages by', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='Soup 3 and Beautiful Soup 4, see Porting code to BS4. This documentation has been translated into other languages by\\nBeautiful Soup users: 这篇文档当然还有中文版. このページは日本語で利用できます(外部リンク) 이 문서는 한국어 번역도 가능합니다. Este documento também está disponível em Português do Brasil. Este documento también está disponible en una traducción al español. Эта документация доступна на русском языке. If you have questions about Beautiful Soup, or run into problems,\\nsend mail to the discussion group. If\\nyour problem involves parsing an HTML document, be sure to mention\\nwhat the diagnose() function says about\\nthat document. When reporting an error in this documentation, please mention which\\ntranslation you’re reading. Here’s an HTML document I’ll be using as an example throughout this\\ndocument. It’s part of a story from Alice in Wonderland: Running the “three sisters” document through Beautiful Soup gives us a\\nBeautifulSoup object, which represents the document as a nested', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='BeautifulSoup object, which represents the document as a nested\\ndata structure: Here are some simple ways to navigate that data structure: One common task is extracting all the URLs found within a page’s <a> tags: Another common task is extracting all the text from a page: Does this look like what you need? If so, read on. If you’re using a recent version of Debian or Ubuntu Linux, you can\\ninstall Beautiful Soup with the system package manager: $ apt-get install python3-bs4 Beautiful Soup 4 is published through PyPi, so if you can’t install it\\nwith the system packager, you can install it with easy_install or\\npip. The package name is beautifulsoup4. Make sure you use the\\nright version of pip or easy_install for your Python version\\n(these may be named pip3 and easy_install3 respectively). $ easy_install beautifulsoup4 $ pip install beautifulsoup4 (The BeautifulSoup package is not what you want. That’s\\nthe previous major release, Beautiful Soup 3. Lots of software uses', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='the previous major release, Beautiful Soup 3. Lots of software uses\\nBS3, so it’s still available, but if you’re writing new code you\\nshould install beautifulsoup4.) If you don’t have easy_install or pip installed, you can\\ndownload the Beautiful Soup 4 source tarball and\\ninstall it with setup.py. $ python setup.py install If all else fails, the license for Beautiful Soup allows you to\\npackage the entire library with your application. You can download the\\ntarball, copy its bs4 directory into your application’s codebase,\\nand use Beautiful Soup without installing it at all. I use Python 3.10 to develop Beautiful Soup, but it should work with\\nother recent versions. Beautiful Soup supports the HTML parser included in Python’s standard\\nlibrary, but it also supports a number of third-party Python parsers.\\nOne is the lxml parser. Depending on your setup,', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='library, but it also supports a number of third-party Python parsers.\\nOne is the lxml parser. Depending on your setup,\\nyou might install lxml with one of these commands: $ apt-get install python-lxml $ easy_install lxml $ pip install lxml Another alternative is the pure-Python html5lib parser, which parses HTML the way a\\nweb browser does. Depending on your setup, you might install html5lib\\nwith one of these commands: $ apt-get install python3-html5lib $ pip install html5lib This table summarizes the advantages and disadvantages of each parser library: Parser Typical usage Advantages Disadvantages Python’s html.parser BeautifulSoup(markup, \"html.parser\") Batteries included Decent speed Not as fast as lxml,\\nless lenient than\\nhtml5lib. lxml’s HTML parser BeautifulSoup(markup, \"lxml\") Very fast External C dependency lxml’s XML parser BeautifulSoup(markup, \"lxml-xml\")\\nBeautifulSoup(markup, \"xml\") Very fast The only currently supported', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='BeautifulSoup(markup, \"xml\") Very fast The only currently supported\\nXML parser External C dependency html5lib BeautifulSoup(markup, \"html5lib\") Extremely lenient Parses pages the same way a\\nweb browser does Creates valid HTML5 Very slow External Python\\ndependency If you can, I recommend you install and use lxml for speed. Note that if a document is invalid, different parsers will generate\\ndifferent Beautiful Soup trees for it. See Differences\\nbetween parsers for details. To parse a document, pass it into the BeautifulSoup\\nconstructor. You can pass in a string or an open filehandle: First, the document is converted to Unicode, and HTML entities are\\nconverted to Unicode characters: Beautiful Soup then parses the document using the best available\\nparser. It will use an HTML parser unless you specifically tell it to\\nuse an XML parser. (See Parsing XML.) Beautiful Soup transforms a complex HTML document into a complex tree', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='use an XML parser. (See Parsing XML.) Beautiful Soup transforms a complex HTML document into a complex tree\\nof Python objects. But you’ll only ever have to deal with about four\\nkinds of objects: Tag, NavigableString, BeautifulSoup,\\nand Comment. These objects represent the HTML elements\\nthat comprise the page. A Tag object corresponds to an XML or HTML tag in the original document. Tags have a lot of attributes and methods, and I’ll cover most of them\\nin Navigating the tree and Searching the tree. For now, the most\\nimportant methods of a tag are for accessing its name and attributes. Every tag has a name: If you change a tag’s name, the change will be reflected in any\\nmarkup generated by Beautiful Soup down the line: An HTML or XML tag may have any number of attributes. The tag <b\\nid=\"boldest\"> has an attribute “id” whose value is\\n“boldest”. You can access a tag’s attributes by treating the tag like', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='id=\"boldest\"> has an attribute “id” whose value is\\n“boldest”. You can access a tag’s attributes by treating the tag like\\na dictionary: You can access the dictionary of attributes directly as .attrs: You can add, remove, and modify a tag’s attributes. Again, this is\\ndone by treating the tag as a dictionary: HTML 4 defines a few attributes that can have multiple values. HTML 5\\nremoves a couple of them, but defines a few more. The most common\\nmulti-valued attribute is class (that is, a tag can have more than\\none CSS class). Others include rel, rev, accept-charset,\\nheaders, and accesskey. By default, Beautiful Soup stores the value(s)\\nof a multi-valued attribute as a list: When you turn a tag back into a string, the values of any multi-valued\\nattributes are consolidated: If an attribute looks like it has more than one value, but it’s not\\na multi-valued attribute as defined by any version of the HTML', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='a multi-valued attribute as defined by any version of the HTML\\nstandard, Beautiful Soup stores it as a simple string: You can force all attributes to be stored as strings by passing\\nmulti_valued_attributes=None as a keyword argument into the\\nBeautifulSoup constructor: You can use get_attribute_list to always return the value in a list\\ncontainer, whether it’s a string or multi-valued attribute value: If you parse a document as XML, there are no multi-valued attributes: Again, you can configure this using the multi_valued_attributes argument: You probably won’t need to do this, but if you do, use the defaults as\\na guide. They implement the rules described in the HTML specification: A tag can contain strings as pieces of text. Beautiful Soup\\nuses the NavigableString class to contain these pieces of text: A NavigableString is just like a Python Unicode string, except\\nthat it also supports some of the features described in Navigating\\nthe tree and Searching the tree. You can convert a', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='that it also supports some of the features described in Navigating\\nthe tree and Searching the tree. You can convert a\\nNavigableString to a Unicode string with str: You can’t edit a string in place, but you can replace one string with\\nanother, using replace_with(): NavigableString supports most of the features described in\\nNavigating the tree and Searching the tree, but not all of\\nthem. In particular, since a string can’t contain anything (the way a\\ntag may contain a string or another tag), strings don’t support the\\n.contents or .string attributes, or the find() method. If you want to use a NavigableString outside of Beautiful Soup,\\nyou should call unicode() on it to turn it into a normal Python\\nUnicode string. If you don’t, your string will carry around a\\nreference to the entire Beautiful Soup parse tree, even when you’re\\ndone using Beautiful Soup. This is a big waste of memory. The BeautifulSoup object represents the parsed document as a', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='done using Beautiful Soup. This is a big waste of memory. The BeautifulSoup object represents the parsed document as a\\nwhole. For most purposes, you can treat it as a Tag\\nobject. This means it supports most of the methods described in\\nNavigating the tree and Searching the tree. You can also pass a BeautifulSoup object into one of the methods\\ndefined in Modifying the tree, just as you would a Tag. This\\nlets you do things like combine two parsed documents: Since the BeautifulSoup object doesn’t correspond to an actual\\nHTML or XML tag, it has no name and no attributes. But sometimes it’s\\nuseful to reference its .name (such as when writing code that works\\nwith both Tag and BeautifulSoup objects),\\nso it’s been given the special .name “[document]”: Tag, NavigableString, and\\nBeautifulSoup cover almost everything you’ll see in an\\nHTML or XML file, but there are a few leftover bits. The main one', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='BeautifulSoup cover almost everything you’ll see in an\\nHTML or XML file, but there are a few leftover bits. The main one\\nyou’ll probably encounter is the Comment. The Comment object is just a special type of NavigableString: But when it appears as part of an HTML document, a Comment is\\ndisplayed with special formatting: Beautiful Soup defines a few NavigableString subclasses to\\ncontain strings found inside specific HTML tags. This makes it easier\\nto pick out the main body of the page, by ignoring strings that\\nprobably represent programming directives found within the\\npage. (These classes are new in Beautiful Soup 4.9.0, and the\\nhtml5lib parser doesn’t use them.) A NavigableString subclass that represents embedded CSS\\nstylesheets; that is, any strings found inside a <style> tag\\nduring document parsing. A NavigableString subclass that represents embedded\\nJavascript; that is, any strings found inside a <script> tag', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='during document parsing. A NavigableString subclass that represents embedded\\nJavascript; that is, any strings found inside a <script> tag\\nduring document parsing. A NavigableString subclass that represents embedded HTML\\ntemplates; that is, any strings found inside a <template> tag during\\ndocument parsing. Beautiful Soup defines some NavigableString classes for\\nholding special types of strings that can be found in XML\\ndocuments. Like Comment, these classes are subclasses of\\nNavigableString that add something extra to the string on\\noutput. A NavigableString subclass representing the declaration at the beginning of\\nan XML document. A NavigableString subclass representing the document type\\ndeclaration which may\\nbe found near the beginning of an XML document. A NavigableString subclass that represents a CData section. A NavigableString subclass that represents the contents', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='of an XML processing instruction. Here’s the “Three sisters” HTML document again: I’ll use this as an example to show you how to move from one part of\\na document to another. Tags may contain strings and more tags. These elements are the tag’s\\nchildren. Beautiful Soup provides a lot of different attributes for\\nnavigating and iterating over a tag’s children. Note that Beautiful Soup strings don’t support any of these\\nattributes, because a string can’t have children. The simplest way to navigate the parse tree is to find a tag by name. To\\ndo this, you can use the find() method: For convenience, just saying the name of the tag you want is equivalent\\nto find() (if no built-in attribute has that name). If you want the\\n<head> tag, just say soup.head: You can use this trick again and again to zoom in on a certain part\\nof the parse tree. This code gets the first <b> tag beneath the <body> tag: find() (and its convenience equivalent) gives you only the first tag', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='of the parse tree. This code gets the first <b> tag beneath the <body> tag: find() (and its convenience equivalent) gives you only the first tag\\nby that name: If you need to get all the <a> tags, you can use find_all(): For more complicated tasks, such as pattern-matching and filtering, you can\\nuse the methods described in Searching the tree. A tag’s children are available in a list called .contents: The BeautifulSoup object itself has children. In this case, the\\n<html> tag is the child of the BeautifulSoup object.: A string does not have .contents, because it can’t contain\\nanything: Instead of getting them as a list, you can iterate over a tag’s\\nchildren using the .children generator: If you want to modify a tag’s children, use the methods described in\\nModifying the tree. Don’t modify the the .contents list\\ndirectly: that can lead to problems that are subtle and difficult to\\nspot. The .contents and .children attributes consider only a tag’s', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='directly: that can lead to problems that are subtle and difficult to\\nspot. The .contents and .children attributes consider only a tag’s\\ndirect children. For instance, the <head> tag has a single direct\\nchild–the <title> tag: But the <title> tag itself has a child: the string “The Dormouse’s\\nstory”. There’s a sense in which that string is also a child of the\\n<head> tag. The .descendants attribute lets you iterate over all\\nof a tag’s children, recursively: its direct children, the children of\\nits direct children, and so on: The <head> tag has only one child, but it has two descendants: the\\n<title> tag and the <title> tag’s child. The BeautifulSoup object\\nonly has one direct child (the <html> tag), but it has a whole lot of\\ndescendants: If a tag has only one child, and that child is a NavigableString,\\nthe child is made available as .string: If a tag’s only child is another tag, and that tag has a\\n.string, then the parent tag is considered to have the same', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='.string, then the parent tag is considered to have the same\\n.string as its child: If a tag contains more than one thing, then it’s not clear what\\n.string should refer to, so .string is defined to be\\nNone: If there’s more than one thing inside a tag, you can still look at\\njust the strings. Use the .strings generator to see all descendant\\nstrings: Newlines and spaces that separate tags are also strings. You can remove extra\\nwhitespace by using the .stripped_strings generator instead: Here, strings consisting entirely of whitespace are ignored, and\\nwhitespace at the beginning and end of strings is removed. Continuing the “family tree” analogy, every tag and every string has a\\nparent: the tag that contains it. You can access an element’s parent with the .parent attribute. In\\nthe example “three sisters” document, the <head> tag is the parent\\nof the <title> tag: The title string itself has a parent: the <title> tag that contains', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='of the <title> tag: The title string itself has a parent: the <title> tag that contains\\nit: The parent of a top-level tag like <html> is the BeautifulSoup object\\nitself: And the .parent of a BeautifulSoup object is defined as None: You can iterate over all of an element’s parents with\\n.parents. This example uses .parents to travel from an <a> tag\\nburied deep within the document, to the very top of the document: Consider a simple document like this: The <b> tag and the <c> tag are at the same level: they’re both direct\\nchildren of the same tag. We call them siblings. When a document is\\npretty-printed, siblings show up at the same indentation level. You\\ncan also use this relationship in the code you write. You can use .next_sibling and .previous_sibling to navigate\\nbetween page elements that are on the same level of the parse tree: The <b> tag has a .next_sibling, but no .previous_sibling,\\nbecause there’s nothing before the <b> tag on the same level of the', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='because there’s nothing before the <b> tag on the same level of the\\ntree. For the same reason, the <c> tag has a .previous_sibling\\nbut no .next_sibling: The strings “text1” and “text2” are not siblings, because they don’t\\nhave the same parent: In real documents, the .next_sibling or .previous_sibling of a\\ntag will usually be a string containing whitespace. Going back to the\\n“three sisters” document: You might think that the .next_sibling of the first <a> tag would\\nbe the second <a> tag. But actually, it’s a string: the comma and\\nnewline that separate the first <a> tag from the second: The second <a> tag is then the .next_sibling of the comma string: You can iterate over a tag’s siblings with .next_siblings or\\n.previous_siblings: (If the argument syntax to find tags by their attribute value is unfamiliar,', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='.previous_siblings: (If the argument syntax to find tags by their attribute value is unfamiliar,\\ndon’t worry; this is covered later in The keyword arguments.) Take a look at the beginning of the “three sisters” document: An HTML parser takes this string of characters and turns it into a\\nseries of events: “open an <html> tag”, “open a <head> tag”, “open a\\n<title> tag”, “add a string”, “close the <title> tag”, “open a <p>\\ntag”, and so on. The order in which the opening tags and strings are\\nencountered is called document order. Beautiful Soup offers tools for\\nsearching a document’s elements in document order. The .next_element attribute of a string or tag points to whatever\\nwas parsed immediately after the opening of the current tag or after\\nthe current string. It might be the same as .next_sibling, but it’s\\nusually drastically different. Here’s the final <a> tag in the “three sisters” document. Its\\n.next_sibling is a string: the conclusion of the sentence that was', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='.next_sibling is a string: the conclusion of the sentence that was\\ninterrupted by the start of the <a> tag: But the .next_element of that <a> tag, the thing that was parsed\\nimmediately after the <a> tag, is not the rest of that sentence:\\nit’s the string “Tillie” inside it: That’s because in the original markup, the word “Tillie” appeared\\nbefore that semicolon. The parser encountered an <a> tag, then the\\nword “Tillie”, then the closing </a> tag, then the semicolon and rest of\\nthe sentence. The semicolon is on the same level as the <a> tag, but the\\nword “Tillie” was encountered first. The .previous_element attribute is the exact opposite of\\n.next_element. It points to the opening tag or string that was\\nparsed immediately before this one: You should get the idea by now. You can use these iterators to move\\nforward or backward in the document as it was parsed: Beautiful Soup defines a lot of methods for searching the parse tree,', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='forward or backward in the document as it was parsed: Beautiful Soup defines a lot of methods for searching the parse tree,\\nbut they’re all very similar. I’m going to spend a lot of time explaining\\nthe two most popular methods: find() and find_all(). The other\\nmethods take almost exactly the same arguments, so I’ll just cover\\nthem briefly. Once again, I’ll be using the “three sisters” document as an example: By passing in a filter to a method like find_all(), you can\\nzoom in on the parts of the document you’re interested in. Before talking in detail about find_all() and similar methods, I\\nwant to show examples of different filters you can pass into these\\nmethods. These filters show up again and again, throughout the\\nsearch API. You can use them to filter based on a tag’s name,\\non its attributes, on the text of a string, or on some combination of\\nthese. The simplest filter is a string. Pass a string to a search method and', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='on its attributes, on the text of a string, or on some combination of\\nthese. The simplest filter is a string. Pass a string to a search method and\\nBeautiful Soup will perform a tag-name match against that exact string.\\nThis code finds all the <b> tags in the document: If you pass in a byte string, Beautiful Soup will assume the string is\\nencoded as UTF-8. You can avoid this by passing in a Unicode string instead. If you pass in a regular expression object, Beautiful Soup will filter\\nagainst that regular expression using its search() method. This code\\nfinds all the tags whose names start with the letter “b”; in this\\ncase, the <body> tag and the <b> tag: This code finds all the tags whose names contain the letter ‘t’: The value True matches every tag it can. This code finds all\\nthe tags in the document, but none of the text strings: If none of the other matches work for you, define a function that\\ntakes an element as its only argument. The function should return', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='takes an element as its only argument. The function should return\\nTrue if the argument matches, and False otherwise. Here’s a function that returns True if a tag defines the “class”\\nattribute but doesn’t define the “id” attribute: Pass this function into find_all() and you’ll pick up all the <p>\\ntags: This function picks up only the <p> tags. It doesn’t pick up the <a>\\ntags, because those tags define both “class” and “id”. It doesn’t pick\\nup tags like <html> and <title>, because those tags don’t define\\n“class”. The function can be as complicated as you need it to be. Here’s a\\nfunction that returns True if a tag is surrounded by string\\nobjects: If you pass in a list, Beautiful Soup will look for a match against\\nany string, regular expression, or function in that list. This', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='objects: If you pass in a list, Beautiful Soup will look for a match against\\nany string, regular expression, or function in that list. This\\ncode finds all the <a> tags and all the <b> tags: Now we’re ready to look at the search methods in detail. Method signature: find_all(name, attrs, recursive, string, limit, **kwargs) The find_all() method looks through a tag’s descendants and\\nretrieves all descendants that match your filters. I gave several\\nexamples in Kinds of filters, but here are a few more: Some of these should look familiar, but others are new. What does it\\nmean to pass in a value for string, or id? Why does\\nfind_all(\"p\", \"title\") find a <p> tag with the CSS class “title”?\\nLet’s look at the arguments to find_all(). Pass in a value for name and you’ll tell Beautiful Soup to only\\nconsider tags with certain names. Text strings will be ignored, as\\nwill tags whose names that don’t match. This is the simplest usage: Recall from Kinds of filters that the value to name can be a', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='will tags whose names that don’t match. This is the simplest usage: Recall from Kinds of filters that the value to name can be a\\nstring, a regular expression, a list, a function, or the value\\nTrue. Any keyword argument that’s not recognized will be turned into a filter\\nthat matches tags by their attributes. If you pass in a value for an argument called id, Beautiful Soup will\\nfilter against each tag’s ‘id’ attribute value: Just as with tags, you can filter an attribute based on a string,\\na regular expression, a list, a function, or the value True. If you pass in a regular expression object for href, Beautiful Soup will\\npattern-match against each tag’s ‘href’ attribute value: The value True matches every tag that defines the attribute. This code\\nfinds all tags with an id attribute: soup.find_all(id=True)\\n# [<a class=”sister” href=”http://example.com/elsie” id=”link1”>Elsie</a>,\\n#  <a class=”sister” href=”http://example.com/lacie” id=”link2”>Lacie</a>,', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='# [<a class=”sister” href=”http://example.com/elsie” id=”link1”>Elsie</a>,\\n#  <a class=”sister” href=”http://example.com/lacie” id=”link2”>Lacie</a>,\\n#  <a class=”sister” href=”http://example.com/tillie” id=”link3”>Tillie</a>] For more complex matches, you can define a function that takes an attribute\\nvalue as its only argument. The function should return True if the value\\nmatches, and False otherwise. Here’s a function that finds all a tags whose href attribute does not\\nmatch a regular expression: If you pass in a list for an argument, Beautiful Soup will look for an\\nattribute-value match against any string, regular expression, or function in\\nthat list. This code finds the first and last link: soup.find_all(id=[“link1”, re.compile(“3$”)])\\n# [<a class=”sister” href=”http://example.com/elsie” id=”link1”>Elsie</a>,\\n#  <a class=”sister” href=”http://example.com/tillie” id=”link3”>Tillie</a>] You can filter against multiple attributes at once by passing multiple', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='#  <a class=”sister” href=”http://example.com/tillie” id=”link3”>Tillie</a>] You can filter against multiple attributes at once by passing multiple\\nkeyword arguments: Some attributes, like the data-* attributes in HTML 5, have names that\\ncan’t be used as the names of keyword arguments: You can use these attributes in searches by putting them into a\\ndictionary and passing the dictionary into find_all() as the\\nattrs argument: Similarly, you can’t use a keyword argument to search for HTML’s ‘name’ attribute,\\nbecause Beautiful Soup uses the name argument to contain the name\\nof the tag itself. Instead, you can give a value to ‘name’ in the\\nattrs argument: It’s very useful to search for a tag that has a certain CSS class, but\\nthe name of the CSS attribute, “class”, is a reserved word in\\nPython. Using class as a keyword argument will give you a syntax\\nerror. As of Beautiful Soup 4.1.2, you can search by CSS class using', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='Python. Using class as a keyword argument will give you a syntax\\nerror. As of Beautiful Soup 4.1.2, you can search by CSS class using\\nthe keyword argument class_: As with any keyword argument, you can pass class_ a string, a regular\\nexpression, a function, or True: Remember that a single tag can have multiple\\nvalues for its “class” attribute. When you search for a tag that\\nmatches a certain CSS class, you’re matching against any of its CSS\\nclasses: You can also search for the exact string value of the class attribute: But searching for variants of the string value won’t work: In older versions of Beautiful Soup, which don’t have the class_\\nshortcut, you can use the attrs argument trick mentioned above.\\nCreate a dictionary whose value for “class” is the string (or regular\\nexpression, or whatever) you want to search for: To search for tags that match two or more CSS classes at once, use the', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='expression, or whatever) you want to search for: To search for tags that match two or more CSS classes at once, use the\\nselect() CSS selector method described here: With the string argument, you can search for strings instead of tags. As\\nwith name and attribute keyword arguments, you can pass in a string, a\\nregular expression, a function, a list, or the value True.\\nHere are some examples: If you use the string argument in a tag search, Beautiful Soup will find\\nall tags whose .string matches your value for string. This code finds\\nthe <a> tags whose .string is “Elsie”: The string argument is new in Beautiful Soup 4.4.0. In earlier\\nversions it was called text: find_all() returns all the tags and strings that match your\\nfilters. This can take a while if the document is large. If you don’t\\nneed all the results, you can pass in a number for limit. This\\nworks just like the LIMIT keyword in SQL. It tells Beautiful Soup to', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='need all the results, you can pass in a number for limit. This\\nworks just like the LIMIT keyword in SQL. It tells Beautiful Soup to\\nstop gathering results after it’s found a certain number. There are three links in the “three sisters” document, but this code\\nonly finds the first two: By default, mytag.find_all() will examine all the descendants of mytag:\\nits children, its children’s children, and so on. To consider only direct\\nchildren, you can pass in recursive=False. See the difference here: Here’s that part of the document: The <title> tag is beneath the <html> tag, but it’s not directly\\nbeneath the <html> tag: the <head> tag is in the way. Beautiful Soup\\nfinds the <title> tag when it’s allowed to look at all descendants of\\nthe <html> tag, but when recursive=False restricts it to the\\n<html> tag’s immediate children, it finds nothing. Beautiful Soup offers a lot of tree-searching methods (covered below),\\nand they mostly take the same arguments as find_all(): name,', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='and they mostly take the same arguments as find_all(): name,\\nattrs, string, limit, and attribute keyword arguments. But the\\nrecursive argument is specific to the find_all() and find() methods.\\nPassing recursive=False into a method like find_parents() wouldn’t be\\nvery useful. For convenience, calling a BeautifulSoup object or\\nTag object as a function is equivalent to calling\\nfind_all() (if no built-in method has the name of the tag you’re\\nlooking for). These two lines of code are equivalent: These two lines are also equivalent: Method signature: find(name, attrs, recursive, string, **kwargs) The find_all() method scans the entire document looking for\\nresults, but sometimes you only want to find one result. If you know a\\ndocument has only one <body> tag, it’s a waste of time to scan the\\nentire document looking for more. Rather than passing in limit=1\\nevery time you call find_all, you can use the find()', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='entire document looking for more. Rather than passing in limit=1\\nevery time you call find_all, you can use the find()\\nmethod. These two lines of code are nearly equivalent: The only difference is that find_all() returns a list containing\\nthe single result, and find() just returns the result. If find_all() can’t find anything, it returns an empty list. If\\nfind() can’t find anything, it returns None: Remember the soup.head.title trick from Navigating using tag\\nnames? That trick works by repeatedly calling find(): Method signature: find_parents(name, attrs, string, limit, **kwargs) Method signature: find_parent(name, attrs, string, **kwargs) I spent a lot of time above covering find_all() and\\nfind(). The Beautiful Soup API defines ten other methods for\\nsearching the tree, but don’t be afraid. Five of these methods are\\nbasically the same as find_all(), and the other five are basically\\nthe same as find(). The only differences are in how they move from', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='basically the same as find_all(), and the other five are basically\\nthe same as find(). The only differences are in how they move from\\none part of the tree to another. First let’s consider find_parents() and\\nfind_parent(). Remember that find_all() and find() work\\ntheir way down the tree, looking at tag’s descendants. These methods\\ndo the opposite: they work their way up the tree, looking at a tag’s\\n(or a string’s) parents. Let’s try them out, starting from a string\\nburied deep in the “three daughters” document: One of the three <a> tags is the direct parent of the string in\\nquestion, so our search finds it. One of the three <p> tags is an\\nindirect parent (ancestor) of the string, and our search finds that as\\nwell. There’s a <p> tag with the CSS class “title” somewhere in the\\ndocument, but it’s not one of this string’s parents, so we can’t find\\nit with find_parents(). You may have noticed a similarity between find_parent() and\\nfind_parents(), and the .parent and .parents attributes', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='it with find_parents(). You may have noticed a similarity between find_parent() and\\nfind_parents(), and the .parent and .parents attributes\\nmentioned earlier. These search methods actually use the .parents\\nattribute to iterate through all parents (unfiltered), checking each one\\nagainst the provided filter to see if it matches. Method signature: find_next_siblings(name, attrs, string, limit, **kwargs) Method signature: find_next_sibling(name, attrs, string, **kwargs) These methods use .next_siblings to\\niterate over the rest of an element’s siblings in the tree. The\\nfind_next_siblings() method returns all the siblings that match,\\nand find_next_sibling() returns only the first one: Method signature: find_previous_siblings(name, attrs, string, limit, **kwargs) Method signature: find_previous_sibling(name, attrs, string, **kwargs) These methods use .previous_siblings to iterate over an element’s\\nsiblings that precede it in the tree. The find_previous_siblings()', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='siblings that precede it in the tree. The find_previous_siblings()\\nmethod returns all the siblings that match, and\\nfind_previous_sibling() returns only the first one: Method signature: find_all_next(name, attrs, string, limit, **kwargs) Method signature: find_next(name, attrs, string, **kwargs) These methods use .next_elements to\\niterate over whatever tags and strings that come after it in the\\ndocument. The find_all_next() method returns all matches, and\\nfind_next() returns only the first match: In the first example, the string “Elsie” showed up, even though it was\\ncontained within the <a> tag we started from. In the second example,\\nthe last <p> tag in the document showed up, even though it’s not in\\nthe same part of the tree as the <a> tag we started from. For these\\nmethods, all that matters is that an element matches the filter and', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='the same part of the tree as the <a> tag we started from. For these\\nmethods, all that matters is that an element matches the filter and\\nit shows up later in the document in document order. Method signature: find_all_previous(name, attrs, string, limit, **kwargs) Method signature: find_previous(name, attrs, string, **kwargs) These methods use .previous_elements to\\niterate over the tags and strings that came before it in the\\ndocument. The find_all_previous() method returns all matches, and\\nfind_previous() only returns the first match: The call to find_all_previous(\"p\") found the first paragraph in\\nthe document (the one with class=”title”), but it also finds the\\nsecond paragraph, the <p> tag that contains the <a> tag we started\\nwith. This shouldn’t be too surprising: we’re looking at all the tags\\nthat show up earlier in the document in document order than the one we started with. A\\n<p> tag that contains an <a> tag must have shown up before the <a>', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='<p> tag that contains an <a> tag must have shown up before the <a>\\ntag it contains. BeautifulSoup and Tag objects support CSS selectors through\\ntheir .css property. The actual selector implementation is handled\\nby the Soup Sieve\\npackage, available on PyPI as soupsieve. If you installed\\nBeautiful Soup through pip, Soup Sieve was installed at the same\\ntime, so you don’t have to do anything extra. The Soup Sieve documentation lists all the currently supported CSS\\nselectors, but\\nhere are some of the basics. You can find tags by name: Find tags by ID: Find tags contained anywhere within other tags: Find tags directly within other tags: Find all matching next siblings of tags: Find the next sibling tag (but only if it matches): Find tags by CSS class: Find tags that match any selector from a list of selectors: Test for the existence of an attribute: Find tags by attribute value: There’s also a method called select_one(), which finds only the', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='first tag that matches a selector: As a convenience, you can call select() and select_one() can\\ndirectly on the BeautifulSoup or Tag object, omitting the\\n.css property: CSS selector support is a convenience for people who already know the\\nCSS selector syntax. You can do all of this with the Beautiful Soup\\nAPI. If CSS selectors are all you need, you should skip Beautiful Soup\\naltogether and parse the document with lxml: it’s a lot\\nfaster. But Soup Sieve lets you combine CSS selectors with the\\nBeautiful Soup API. Soup Sieve offers a substantial API beyond the select() and\\nselect_one() methods, and you can access most of that API through\\nthe .css attribute of Tag or BeautifulSoup. What follows\\nis just a list of the supported methods; see the Soup Sieve\\ndocumentation for full\\ndocumentation. The iselect() method works the same as select(), but it\\nreturns a generator instead of a list: The closest() method returns the nearest parent of a given Tag', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='returns a generator instead of a list: The closest() method returns the nearest parent of a given Tag\\nthat matches a CSS selector, similar to Beautiful Soup’s\\nfind_parent() method: The match() method returns a Boolean depending on whether or not a\\nspecific Tag matches a selector: The filter() method returns the subset of a tag’s direct children\\nthat match a selector: The escape() method escapes CSS identifiers that would otherwise\\nbe invalid: If you’ve parsed XML that defines namespaces, you can use them in CSS\\nselectors.: Beautiful Soup tries to use namespace prefixes that make sense based\\non what it saw while parsing the document, but you can always provide\\nyour own dictionary of abbreviations: The .css property was added in Beautiful Soup 4.12.0. Prior to this,\\nonly the .select() and .select_one() convenience methods were\\nsupported. The Soup Sieve integration was added in Beautiful Soup 4.7.0. Earlier\\nversions had the .select() method, but only the most commonly-used', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='supported. The Soup Sieve integration was added in Beautiful Soup 4.7.0. Earlier\\nversions had the .select() method, but only the most commonly-used\\nCSS selectors were supported. Beautiful Soup’s main strength is in searching the parse tree, but you\\ncan also modify the tree and write your changes as a new HTML or XML\\ndocument. I covered this earlier, in Tag.attrs, but it bears repeating. You\\ncan rename a tag, change the values of its attributes, add new\\nattributes, and delete attributes: If you set a tag’s .string attribute to a new string, the tag’s contents are\\nreplaced with that string: Be careful: if the tag contained other tags, they and all their\\ncontents will be destroyed. You can add to a tag’s contents with Tag.append(). It works just\\nlike calling .append() on a Python list: Starting in Beautiful Soup 4.7.0, Tag also supports a method\\ncalled .extend(), which adds every element of a list to a Tag,\\nin order: If you need to add a string to a document, no problem–you can pass a', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='called .extend(), which adds every element of a list to a Tag,\\nin order: If you need to add a string to a document, no problem–you can pass a\\nPython string in to append(), or you can call the NavigableString\\nconstructor: If you want to create a comment or some other subclass of\\nNavigableString, just call the constructor: (This is a new feature in Beautiful Soup 4.4.0.) What if you need to create a whole new tag? The best solution is to\\ncall the factory method BeautifulSoup.new_tag(): Only the first argument, the tag name, is required. Tag.insert() is just like Tag.append(), except the new element\\ndoesn’t necessarily go at the end of its parent’s\\n.contents. It’ll be inserted at whatever numeric position you\\nsay. It works just like .insert() on a Python list: The insert_before() method inserts tags or strings immediately\\nbefore something else in the parse tree: The insert_after() method inserts tags or strings immediately', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='before something else in the parse tree: The insert_after() method inserts tags or strings immediately\\nfollowing something else in the parse tree: Tag.clear() removes the contents of a tag: PageElement.extract() removes a tag or string from the tree. It\\nreturns the tag or string that was extracted: At this point you effectively have two parse trees: one rooted at the\\nBeautifulSoup object you used to parse the document, and one rooted\\nat the tag that was extracted. You can go on to call extract() on\\na child of the element you extracted: Tag.decompose() removes a tag from the tree, then completely\\ndestroys it and its contents: The behavior of a decomposed Tag or NavigableString is not\\ndefined and you should not use it for anything. If you’re not sure\\nwhether something has been decomposed, you can check its\\n.decomposed property (new in Beautiful Soup 4.9.0): PageElement.replace_with() extracts a tag or string from the tree,', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='.decomposed property (new in Beautiful Soup 4.9.0): PageElement.replace_with() extracts a tag or string from the tree,\\nthen replaces it with one or more tags or strings of your choice: replace_with() returns the tag or string that got replaced, so\\nthat you can examine it or add it back to another part of the tree. The ability to pass multiple arguments into replace_with() is new\\nin Beautiful Soup 4.10.0. PageElement.wrap() wraps an element in the Tag object you specify. It\\nreturns the new wrapper: This method is new in Beautiful Soup 4.0.5. Tag.unwrap() is the opposite of wrap(). It replaces a tag with\\nwhatever’s inside that tag. It’s good for stripping out markup: Like replace_with(), unwrap() returns the tag\\nthat was replaced. After calling a bunch of methods that modify the parse tree, you may end up\\nwith two or more NavigableString objects next to each other.\\nBeautiful Soup doesn’t have any problems with this, but since it can’t happen', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='with two or more NavigableString objects next to each other.\\nBeautiful Soup doesn’t have any problems with this, but since it can’t happen\\nin a freshly parsed document, you might not expect behavior like the\\nfollowing: You can call Tag.smooth() to clean up the parse tree by consolidating adjacent strings: This method is new in Beautiful Soup 4.8.0. The prettify() method will turn a Beautiful Soup parse tree into a\\nnicely formatted Unicode string, with a separate line for each\\ntag and each string: You can call prettify() on the top-level BeautifulSoup object,\\nor on any of its Tag objects: Since it adds whitespace (in the form of newlines), prettify()\\nchanges the meaning of an HTML document and should not be used to\\nreformat one. The goal of prettify() is to help you visually\\nunderstand the structure of the documents you work with. If you just want a string, with no fancy formatting, you can call', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='understand the structure of the documents you work with. If you just want a string, with no fancy formatting, you can call\\nstr() on a BeautifulSoup object, or on a Tag within it: The str() function returns a string encoded in UTF-8. See\\nEncodings for other options. You can also call encode() to get a bytestring, and decode()\\nto get Unicode. If you give Beautiful Soup a document that contains HTML entities like\\n“&lquot;”, they’ll be converted to Unicode characters: If you then convert the document to a bytestring, the Unicode characters\\nwill be encoded as UTF-8. You won’t get the HTML entities back: By default, the only characters that are escaped upon output are bare\\nampersands and angle brackets. These get turned into “&amp;”, “&lt;”,\\nand “&gt;”, so that Beautiful Soup doesn’t inadvertently generate\\ninvalid HTML or XML: You can change this behavior by providing a value for the\\nformatter argument to prettify(), encode(), or\\ndecode(). Beautiful Soup recognizes five possible values for', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='formatter argument to prettify(), encode(), or\\ndecode(). Beautiful Soup recognizes five possible values for\\nformatter. The default is formatter=\"minimal\". Strings will only be processed\\nenough to ensure that Beautiful Soup generates valid HTML/XML: If you pass in formatter=\"html\", Beautiful Soup will convert\\nUnicode characters to HTML entities whenever possible: If you pass in formatter=\"html5\", it’s similar to\\nformatter=\"html\", but Beautiful Soup will\\nomit the closing slash in HTML void tags like “br”: In addition, any attributes whose values are the empty string\\nwill become HTML-style Boolean attributes: (This behavior is new as of Beautiful Soup 4.10.0.) If you pass in formatter=None, Beautiful Soup will not modify\\nstrings at all on output. This is the fastest option, but it may lead\\nto Beautiful Soup generating invalid HTML/XML, as in these examples: If you need more sophisticated control over your output, you can\\ninstantiate one of Beautiful Soup’s formatter classes and pass that', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='instantiate one of Beautiful Soup’s formatter classes and pass that\\nobject in as formatter. Used to customize the formatting rules for HTML documents. Here’s a formatter that converts strings to uppercase, whether they\\noccur in a string object or an attribute value: Here’s a formatter that increases the indentation width when pretty-printing: Used to customize the formatting rules for XML documents. Subclassing HTMLFormatter or XMLFormatter will\\ngive you even more control over the output. For example, Beautiful\\nSoup sorts the attributes in every tag by default: To turn this off, you can subclass the Formatter.attributes()\\nmethod, which controls which attributes are output and in what\\norder. This implementation also filters out the attribute called “m”\\nwhenever it appears: One last caveat: if you create a CData object, the text inside\\nthat object is always presented exactly as it appears, with no\\nformatting. Beautiful Soup will call your entity substitution', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='that object is always presented exactly as it appears, with no\\nformatting. Beautiful Soup will call your entity substitution\\nfunction, just in case you’ve written a custom function that counts\\nall the strings in the document or something, but it will ignore the\\nreturn value: If you only want the human-readable text inside a document or tag, you can use the\\nget_text() method. It returns all the text in a document or\\nbeneath a tag, as a single Unicode string: You can specify a string to be used to join the bits of text\\ntogether: You can tell Beautiful Soup to strip whitespace from the beginning and\\nend of each bit of text: But at that point you might want to use the .stripped_strings\\ngenerator instead, and process the text yourself: As of Beautiful Soup version 4.9.0, when lxml or html.parser are in\\nuse, the contents of <script>, <style>, and <template>\\ntags are generally not considered to be ‘text’, since those tags are not part of', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='use, the contents of <script>, <style>, and <template>\\ntags are generally not considered to be ‘text’, since those tags are not part of\\nthe human-visible content of the page. As of Beautiful Soup version 4.10.0, you can call get_text(),\\n.strings, or .stripped_strings on a NavigableString object. It will\\neither return the object itself, or nothing, so the only reason to do\\nthis is when you’re iterating over a mixed list. If you just need to parse some HTML, you can dump the markup into the\\nBeautifulSoup constructor, and it’ll probably be fine. Beautiful\\nSoup will pick a parser for you and parse the data. But there are a\\nfew additional arguments you can pass in to the constructor to change\\nwhich parser is used. The first argument to the BeautifulSoup constructor is a string or\\nan open filehandle—the source of the markup you want parsed. The second\\nargument is how you’d like the markup parsed. If you don’t specify anything, you’ll get the best HTML parser that’s', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='argument is how you’d like the markup parsed. If you don’t specify anything, you’ll get the best HTML parser that’s\\ninstalled. Beautiful Soup ranks lxml’s parser as being the best, then\\nhtml5lib’s, then Python’s built-in parser. You can override this by\\nspecifying one of the following: What type of markup you want to parse. Currently supported values are\\n“html”, “xml”, and “html5”. The name of the parser library you want to use. Currently supported\\noptions are “lxml”, “html5lib”, and “html.parser” (Python’s\\nbuilt-in HTML parser). The section Installing a parser contrasts the supported parsers. If you don’t have an appropriate parser installed, Beautiful Soup will\\nignore your request and pick a different parser. Right now, the only\\nsupported XML parser is lxml. If you don’t have lxml installed, asking\\nfor an XML parser won’t give you one, and asking for “lxml” won’t work\\neither. Beautiful Soup presents the same interface to a number of different', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='for an XML parser won’t give you one, and asking for “lxml” won’t work\\neither. Beautiful Soup presents the same interface to a number of different\\nparsers, but each parser is different. Different parsers will create\\ndifferent parse trees from the same document. The biggest differences\\nare between the HTML parsers and the XML parsers. Here’s a short\\ndocument, parsed as HTML using the parser that comes with Python: Since a standalone <b/> tag is not valid HTML, html.parser turns it into\\na <b></b> tag pair. Here’s the same document parsed as XML (running this requires that you\\nhave lxml installed). Note that the standalone <b/> tag is left alone, and\\nthat the document is given an XML declaration instead of being put\\ninto an <html> tag.: There are also differences between HTML parsers. If you give Beautiful\\nSoup a perfectly-formed HTML document, these differences won’t\\nmatter. One parser will be faster than another, but they’ll all give', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='Soup a perfectly-formed HTML document, these differences won’t\\nmatter. One parser will be faster than another, but they’ll all give\\nyou a data structure that looks exactly like the original HTML\\ndocument. But if the document is not perfectly-formed, different parsers will\\ngive different results. Here’s a short, invalid document parsed using\\nlxml’s HTML parser. Note that the <a> tag gets wrapped in <body> and\\n<html> tags, and the dangling </p> tag is simply ignored: Here’s the same document parsed using html5lib: Instead of ignoring the dangling </p> tag, html5lib pairs it with an\\nopening <p> tag. html5lib also adds an empty <head> tag; lxml didn’t\\nbother. Here’s the same document parsed with Python’s built-in HTML\\nparser: Like lxml, this parser ignores the closing </p> tag. Unlike\\nhtml5lib or lxml, this parser makes no attempt to create a\\nwell-formed HTML document by adding <html> or <body> tags. Since the document “<a></p>” is invalid, none of these techniques is', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='well-formed HTML document by adding <html> or <body> tags. Since the document “<a></p>” is invalid, none of these techniques is\\nthe ‘correct’ way to handle it. The html5lib parser uses techniques\\nthat are part of the HTML5 standard, so it has the best claim on being\\nthe ‘correct’ way, but all three techniques are legitimate. Differences between parsers can affect your script. If you’re planning\\non distributing your script to other people, or running it on multiple\\nmachines, you should specify a parser in the BeautifulSoup\\nconstructor. That will reduce the chances that your users parse a\\ndocument differently from the way you parse it. Any HTML or XML document is written in a specific encoding like ASCII\\nor UTF-8. But when you load that document into Beautiful Soup, you’ll\\ndiscover it’s been converted to Unicode: It’s not magic. (That sure would be nice.) Beautiful Soup uses a\\nsub-library called Unicode, Dammit to detect a document’s encoding', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='sub-library called Unicode, Dammit to detect a document’s encoding\\nand convert it to Unicode. The autodetected encoding is available as\\nthe .original_encoding attribute of the BeautifulSoup object: Unicode, Dammit guesses correctly most of the time, but sometimes it\\nmakes mistakes. Sometimes it guesses correctly, but only after a\\nbyte-by-byte search of the document that takes a very long time. If\\nyou happen to know a document’s encoding ahead of time, you can avoid\\nmistakes and delays by passing it to the BeautifulSoup constructor\\nas from_encoding. Here’s a document written in ISO-8859-8. The document is so short that\\nUnicode, Dammit can’t get a lock on it, and misidentifies it as\\nISO-8859-7: We can fix this by passing in the correct from_encoding: If you don’t know what the correct encoding is, but you know that\\nUnicode, Dammit is guessing wrong, you can pass the wrong guesses in\\nas exclude_encodings: Windows-1255 isn’t 100% correct, but that encoding is a compatible', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='as exclude_encodings: Windows-1255 isn’t 100% correct, but that encoding is a compatible\\nsuperset of ISO-8859-8, so it’s close enough. (exclude_encodings\\nis a new feature in Beautiful Soup 4.4.0.) In rare cases (usually when a UTF-8 document contains text written in\\na completely different encoding), the only way to get Unicode may be\\nto replace some characters with the special Unicode character\\n“REPLACEMENT CHARACTER” (U+FFFD, �). If Unicode, Dammit needs to do\\nthis, it will set the .contains_replacement_characters attribute\\nto True on the UnicodeDammit or BeautifulSoup object. This\\nlets you know that the Unicode representation is not an exact\\nrepresentation of the original–some data was lost. If a document\\ncontains �, but .contains_replacement_characters is False,\\nyou’ll know that the � was there originally (as it is in this\\nparagraph) and doesn’t stand in for missing data. When you write out an output document from Beautiful Soup, you get a UTF-8', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='paragraph) and doesn’t stand in for missing data. When you write out an output document from Beautiful Soup, you get a UTF-8\\ndocument, even if the input document wasn’t in UTF-8 to begin with. Here’s a\\ndocument written in the Latin-1 encoding: Note that the <meta> tag has been rewritten to reflect the fact that\\nthe document is now in UTF-8. If you don’t want UTF-8, you can pass an encoding into prettify(): You can also call encode() on the BeautifulSoup object, or any\\nelement in the soup, just as if it were a Python string: Any characters that can’t be represented in your chosen encoding will\\nbe converted into numeric XML entity references. Here’s a document\\nthat includes the Unicode character SNOWMAN: The SNOWMAN character can be part of a UTF-8 document (it looks like\\n☃), but there’s no representation for that character in ISO-Latin-1 or\\nASCII, so it’s converted into “&#9731” for those encodings: You can use Unicode, Dammit without using Beautiful Soup. It’s useful', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='ASCII, so it’s converted into “&#9731” for those encodings: You can use Unicode, Dammit without using Beautiful Soup. It’s useful\\nwhenever you have data in an unknown encoding and you just want it to\\nbecome Unicode: Unicode, Dammit’s guesses will get a lot more accurate if you install\\none of these Python libraries: charset-normalizer, chardet, or\\ncchardet. The more data you give Unicode, Dammit, the more\\naccurately it will guess. If you have your own suspicions as to what\\nthe encoding might be, you can pass them in as a list: Unicode, Dammit has two special features that Beautiful Soup doesn’t\\nuse. You can use Unicode, Dammit to convert Microsoft smart quotes to HTML or XML\\nentities: You can also convert Microsoft smart quotes to ASCII quotes: Hopefully you’ll find this feature useful, but Beautiful Soup doesn’t\\nuse it. Beautiful Soup prefers the default behavior, which is to\\nconvert Microsoft smart quotes to Unicode characters along with', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='use it. Beautiful Soup prefers the default behavior, which is to\\nconvert Microsoft smart quotes to Unicode characters along with\\neverything else: Sometimes a document is mostly in UTF-8, but contains Windows-1252\\ncharacters such as (again) Microsoft smart quotes. This can happen\\nwhen a website includes data from multiple sources. You can use\\nUnicodeDammit.detwingle() to turn such a document into pure\\nUTF-8. Here’s a simple example: This document is a mess. The snowmen are in UTF-8 and the quotes are\\nin Windows-1252. You can display the snowmen or the quotes, but not\\nboth: Decoding the document as UTF-8 raises a UnicodeDecodeError, and\\ndecoding it as Windows-1252 gives you gibberish. Fortunately,\\nUnicodeDammit.detwingle() will convert the string to pure UTF-8,\\nallowing you to decode it to Unicode and display the snowmen and quote\\nmarks simultaneously: UnicodeDammit.detwingle() only knows how to handle Windows-1252\\nembedded in UTF-8 (or vice versa, I suppose), but this is the most', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='embedded in UTF-8 (or vice versa, I suppose), but this is the most\\ncommon case. Note that you must know to call UnicodeDammit.detwingle() on your\\ndata before passing it into BeautifulSoup or the UnicodeDammit\\nconstructor. Beautiful Soup assumes that a document has a single\\nencoding, whatever it might be. If you pass it a document that\\ncontains both UTF-8 and Windows-1252, it’s likely to think the whole\\ndocument is Windows-1252, and the document will come out looking like\\nâ˜ƒâ˜ƒâ˜ƒ“I like snowmen!”. UnicodeDammit.detwingle() is new in Beautiful Soup 4.1.0. The html.parser and html5lib parsers can keep track of where in\\nthe original document each Tag was found. You can access this\\ninformation as Tag.sourceline (line number) and Tag.sourcepos\\n(position of the start tag within a line): Note that the two parsers mean slightly different things by\\nsourceline and sourcepos. For html.parser, these numbers\\nrepresent the position of the initial less-than sign. For html5lib,', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='sourceline and sourcepos. For html.parser, these numbers\\nrepresent the position of the initial less-than sign. For html5lib,\\nthese numbers represent the position of the final greater-than sign: You can shut off this feature by passing store_line_numbers=False\\ninto the BeautifulSoup constructor: This feature is new in 4.8.1, and the parsers based on lxml don’t\\nsupport it. Beautiful Soup says that two NavigableString or Tag objects\\nare equal when they represent the same HTML or XML markup, even if their\\nattributes are in a different order or they live in different parts of the\\nobject tree. In this example, the two <b> tags are treated as equal, because\\nthey both look like “<b>pizza</b>”: If you want to see whether two variables refer to exactly the same\\nobject, use is: You can use copy.copy() to create a copy of any Tag or\\nNavigableString: The copy is considered equal to the original, since it represents the', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='NavigableString: The copy is considered equal to the original, since it represents the\\nsame markup as the original, but it’s not the same object: The only real difference is that the copy is completely detached from\\nthe original Beautiful Soup object tree, just as if extract() had\\nbeen called on it: This is because two different Tag objects can’t occupy the same\\nspace at the same time. Beautiful Soup offers a number of ways to customize how the parser\\ntreats incoming HTML and XML. This section covers the most commonly\\nused customization techniques. Let’s say you want to use Beautiful Soup to look at a document’s <a>\\ntags. It’s a waste of time and memory to parse the entire document and\\nthen go over it again looking for <a> tags. It would be much faster to\\nignore everything that wasn’t an <a> tag in the first place. The\\nSoupStrainer class allows you to choose which parts of an incoming\\ndocument are parsed. You just create a SoupStrainer and pass it in', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='SoupStrainer class allows you to choose which parts of an incoming\\ndocument are parsed. You just create a SoupStrainer and pass it in\\nto the BeautifulSoup constructor as the parse_only argument. (Note that this feature won’t work if you’re using the html5lib parser.\\nIf you use html5lib, the whole document will be parsed, no\\nmatter what. This is because html5lib constantly rearranges the parse\\ntree as it works, and if some part of the document didn’t actually\\nmake it into the parse tree, it’ll crash. To avoid confusion, in the\\nexamples below I’ll be forcing Beautiful Soup to use Python’s\\nbuilt-in parser.) The SoupStrainer class takes the same arguments as a typical\\nmethod from Searching the tree: name, attrs, string, and **kwargs. Here are\\nthree SoupStrainer objects: I’m going to bring back the “three sisters” document one more time,\\nand we’ll see what the document looks like when it’s parsed with these', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='and we’ll see what the document looks like when it’s parsed with these\\nthree SoupStrainer objects: The SoupStrainer behavior is as follows: When a tag matches, it is kept (including all its contents, whether they also\\nmatch or not). When a tag does not match, the tag itself is not kept, but parsing continues\\ninto its contents to look for other tags that do match. You can also pass a SoupStrainer into any of the methods covered\\nin Searching the tree. This probably isn’t terribly useful, but I\\nthought I’d mention it: In an HTML document, an attribute like class is given a list of\\nvalues, and an attribute like id is given a single value, because\\nthe HTML specification treats those attributes differently: You can turn this off by passing in\\nmulti_valued_attributes=None. Than all attributes will be given a\\nsingle value: You can customize this behavior quite a bit by passing in a\\ndictionary for multi_valued_attributes. If you need this, look at', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content=\"single value: You can customize this behavior quite a bit by passing in a\\ndictionary for multi_valued_attributes. If you need this, look at\\nHTMLTreeBuilder.DEFAULT_CDATA_LIST_ATTRIBUTES to see the\\nconfiguration Beautiful Soup uses by default, which is based on the\\nHTML specification. (This is a new feature in Beautiful Soup 4.8.0.) When using the html.parser parser, you can use the\\non_duplicate_attribute constructor argument to customize what\\nBeautiful Soup does when it encounters a tag that defines the same\\nattribute more than once: The default behavior is to use the last value found for the tag: With on_duplicate_attribute='ignore' you can tell Beautiful Soup\\nto use the first value found and ignore the rest: (lxml and html5lib always do it this way; their behavior can’t be\\nconfigured from within Beautiful Soup.) If you need more control, you can pass in a function that’s called on each\", metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='configured from within Beautiful Soup.) If you need more control, you can pass in a function that’s called on each\\nduplicate value: (This is a new feature in Beautiful Soup 4.9.1.) When a parser tells Beautiful Soup about a tag or a string, Beautiful\\nSoup will instantiate a Tag or NavigableString object to\\ncontain that information. Instead of that default behavior, you can\\ntell Beautiful Soup to instantiate subclasses of Tag or\\nNavigableString, subclasses you define with custom behavior: This can be useful when incorporating Beautiful Soup into a test\\nframework. (This is a new feature in Beautiful Soup 4.8.1.) If you’re having trouble understanding what Beautiful Soup does to a\\ndocument, pass the document into the diagnose() function. (This function is new in\\nBeautiful Soup 4.2.0.) Beautiful Soup will print out a report showing\\nyou how different parsers handle the document, and tell you if you’re', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='Beautiful Soup 4.2.0.) Beautiful Soup will print out a report showing\\nyou how different parsers handle the document, and tell you if you’re\\nmissing a parser that Beautiful Soup could be using: Just looking at the output of diagnose() might show you how to solve the\\nproblem. Even if not, you can paste the output of diagnose() when\\nasking for help. There are two different kinds of parse errors. There are crashes,\\nwhere you feed a document to Beautiful Soup and it raises an\\nexception (usually an HTMLParser.HTMLParseError). And there is\\nunexpected behavior, where a Beautiful Soup parse tree looks a lot\\ndifferent than the document used to create it. These problems are almost never problems with Beautiful Soup itself.\\nThis is not because Beautiful Soup is an amazingly well-written piece\\nof software. It’s because Beautiful Soup doesn’t include any parsing\\ncode. Instead, it relies on external parsers. If one parser isn’t\\nworking on a certain document, the best solution is to try a different', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content=\"code. Instead, it relies on external parsers. If one parser isn’t\\nworking on a certain document, the best solution is to try a different\\nparser. See Installing a parser for details and a parser\\ncomparison. If this doesn’t help, you might need to inspect the\\ndocument tree found inside the BeautifulSoup object, to see where\\nthe markup you’re looking for actually ended up. SyntaxError: Invalid syntax (on the line ROOT_TAG_NAME =\\n'[document]'): Caused by running an old Python 2 version of\\nBeautiful Soup under Python 3, without converting the code. ImportError: No module named HTMLParser - Caused by running an old\\nPython 2 version of Beautiful Soup under Python 3. ImportError: No module named html.parser - Caused by running the\\nPython 3 version of Beautiful Soup under Python 2. ImportError: No module named BeautifulSoup - Caused by running\\nBeautiful Soup 3 code in an environment that doesn’t have BS3\\ninstalled. Or, by writing Beautiful Soup 4 code without knowing that\", metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='Beautiful Soup 3 code in an environment that doesn’t have BS3\\ninstalled. Or, by writing Beautiful Soup 4 code without knowing that\\nthe package name has changed to bs4. ImportError: No module named bs4 - Caused by running Beautiful\\nSoup 4 code in an environment that doesn’t have BS4 installed. By default, Beautiful Soup parses documents as HTML. To parse a\\ndocument as XML, pass in “xml” as the second argument to the\\nBeautifulSoup constructor: You’ll need to have lxml installed. If your script works on one computer but not another, or in one\\nvirtual environment but not another, or outside the virtual\\nenvironment but not inside, it’s probably because the two\\nenvironments have different parser libraries available. For example,\\nyou may have developed the script on a computer that has lxml\\ninstalled, and then tried to run it on a computer that only has\\nhtml5lib installed. See Differences between parsers for why this\\nmatters, and fix the problem by mentioning a specific parser library', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content=\"html5lib installed. See Differences between parsers for why this\\nmatters, and fix the problem by mentioning a specific parser library\\nin the BeautifulSoup constructor. Because HTML tags and attributes are case-insensitive, all three HTML\\nparsers convert tag and attribute names to lowercase. That is, the\\nmarkup <TAG></TAG> is converted to <tag></tag>. If you want to\\npreserve mixed-case or uppercase tags and attributes, you’ll need to\\nparse the document as XML. UnicodeEncodeError: 'charmap' codec can't encode character\\n'\\\\xfoo' in position bar (or just about any other\\nUnicodeEncodeError) - This problem shows up in two main\\nsituations. First, when you try to print a Unicode character that\\nyour console doesn’t know how to display. (See this page on the\\nPython wiki for help.)\\nSecond, when you’re writing to a file and you pass in a Unicode\\ncharacter that’s not supported by your default encoding. In this\\ncase, the simplest solution is to explicitly encode the Unicode\", metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='character that’s not supported by your default encoding. In this\\ncase, the simplest solution is to explicitly encode the Unicode\\nstring into UTF-8 with u.encode(\"utf8\"). KeyError: [attr] - Caused by accessing tag[\\'attr\\'] when the\\ntag in question doesn’t define the attr attribute. The most\\ncommon errors are KeyError: \\'href\\' and KeyError: \\'class\\'.\\nUse tag.get(\\'attr\\') if you’re not sure attr is\\ndefined, just as you would with a Python dictionary. AttributeError: \\'ResultSet\\' object has no attribute \\'foo\\' - This\\nusually happens because you expected find_all() to return a\\nsingle tag or string. But find_all() returns a list of tags\\nand strings–a ResultSet object. You need to iterate over the\\nlist and look at the .foo of each one. Or, if you really only\\nwant one result, you need to use find() instead of\\nfind_all(). AttributeError: \\'NoneType\\' object has no attribute \\'foo\\' - This\\nusually happens because you called find() and then tried to', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content=\"find_all(). AttributeError: 'NoneType' object has no attribute 'foo' - This\\nusually happens because you called find() and then tried to\\naccess the .foo` attribute of the result. But in your case,\\nfind() didn’t find anything, so it returned None, instead of\\nreturning a tag or a string. You need to figure out why your\\nfind() call isn’t returning anything. AttributeError: 'NavigableString' object has no attribute\\n'foo' - This usually happens because you’re treating a string as\\nthough it were a tag. You may be iterating over a list, expecting\\nthat it contains nothing but tags, when it actually contains both tags and\\nstrings. Beautiful Soup will never be as fast as the parsers it sits on top\\nof. If response time is critical, if you’re paying for computer time\\nby the hour, or if there’s any other reason why computer time is more\\nvaluable than programmer time, you should forget about Beautiful Soup\", metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='by the hour, or if there’s any other reason why computer time is more\\nvaluable than programmer time, you should forget about Beautiful Soup\\nand work directly atop lxml. That said, there are things you can do to speed up Beautiful Soup. If\\nyou’re not using lxml as the underlying parser, my advice is to\\nstart. Beautiful Soup parses documents\\nsignificantly faster using lxml than using html.parser or html5lib. You can speed up encoding detection significantly by installing the\\ncchardet library. Parsing only part of a document won’t save you much time parsing\\nthe document, but it can save a lot of memory, and it’ll make\\nsearching the document much faster. New translations of the Beautiful Soup documentation are greatly\\nappreciated. Translations should be licensed under the MIT license,\\njust like Beautiful Soup and its English documentation are. There are two ways of getting your translation into the main code base', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='just like Beautiful Soup and its English documentation are. There are two ways of getting your translation into the main code base\\nand onto the Beautiful Soup website: Create a branch of the Beautiful Soup repository, add your\\ntranslation, and propose a merge with the main branch, the same\\nas you would do with a proposed change to the source code. Send a message to the Beautiful Soup discussion group with a link to\\nyour translation, or attach your translation to the message. Use the Chinese or Brazilian Portuguese translations as your model. In\\nparticular, please translate the source file doc/source/index.rst,\\nrather than the HTML version of the documentation. This makes it\\npossible to publish the documentation in a variety of formats, not\\njust HTML. Beautiful Soup 3 is the previous release series, and is no longer\\nsupported. Development of Beautiful Soup 3 stopped in 2012, and the\\npackage was completely discontinued in 2021. There’s no reason to', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='supported. Development of Beautiful Soup 3 stopped in 2012, and the\\npackage was completely discontinued in 2021. There’s no reason to\\ninstall it unless you’re trying to get very old software to work, but\\nit’s published through PyPi as BeautifulSoup: $ pip install BeautifulSoup You can also download a tarball of the final release, 3.2.2. If you ran pip install beautifulsoup or pip install\\nBeautifulSoup, but your code doesn’t work, you installed Beautiful\\nSoup 3 by mistake. You need to run pip install beautifulsoup4. The documentation for Beautiful Soup 3 is archived online. Most code written against Beautiful Soup 3 will work against Beautiful\\nSoup 4 with one simple change. All you should have to do is change the\\npackage name from BeautifulSoup to bs4. So this: becomes this: If you get the ImportError “No module named BeautifulSoup”, your\\nproblem is that you’re trying to run Beautiful Soup 3 code, but you', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='problem is that you’re trying to run Beautiful Soup 3 code, but you\\nonly have Beautiful Soup 4 installed. If you get the ImportError “No module named bs4”, your problem\\nis that you’re trying to run Beautiful Soup 4 code, but you only\\nhave Beautiful Soup 3 installed. Although BS4 is mostly backward-compatible with BS3, most of its\\nmethods have been deprecated and given new names for PEP 8 compliance. There are numerous other\\nrenames and changes, and a few of them break backward compatibility. Here’s what you’ll need to know to convert your BS3 code and habits to BS4: Beautiful Soup 3 used Python’s SGMLParser, a module that was\\ndeprecated and removed in Python 3.0. Beautiful Soup 4 uses\\nhtml.parser by default, but you can plug in lxml or html5lib and\\nuse that instead. See Installing a parser for a comparison. Since html.parser is not the same parser as SGMLParser, you\\nmay find that Beautiful Soup 4 gives you a different parse tree than', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='may find that Beautiful Soup 4 gives you a different parse tree than\\nBeautiful Soup 3 for the same markup. If you swap out html.parser\\nfor lxml or html5lib, you may find that the parse tree changes yet\\nagain. If this happens, you’ll need to update your scraping code to\\nprocess the new tree. renderContents -> encode_contents replaceWith -> replace_with replaceWithChildren -> unwrap findAll -> find_all findAllNext -> find_all_next findAllPrevious -> find_all_previous findNext -> find_next findNextSibling -> find_next_sibling findNextSiblings -> find_next_siblings findParent -> find_parent findParents -> find_parents findPrevious -> find_previous findPreviousSibling -> find_previous_sibling findPreviousSiblings -> find_previous_siblings getText -> get_text nextSibling -> next_sibling previousSibling -> previous_sibling Some arguments to the Beautiful Soup constructor were renamed for the', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='same reasons: BeautifulSoup(parseOnlyThese=...) -> BeautifulSoup(parse_only=...) BeautifulSoup(fromEncoding=...) -> BeautifulSoup(from_encoding=...) I renamed one method for compatibility with Python 3: Tag.has_key() -> Tag.has_attr() I renamed one attribute to use more accurate terminology: Tag.isSelfClosing -> Tag.is_empty_element I renamed three attributes to avoid using words that have special\\nmeaning to Python. Unlike the others, these changes are not backwards\\ncompatible. If you used these attributes in BS3, your code will break\\nin BS4 until you change them. UnicodeDammit.unicode -> UnicodeDammit.unicode_markup Tag.next -> Tag.next_element Tag.previous -> Tag.previous_element These methods are left over from the Beautiful Soup 2 API. They’ve', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='been deprecated since 2006 and should not be used at all: Tag.fetchNextSiblings Tag.fetchPreviousSiblings Tag.fetchPrevious Tag.fetchPreviousSiblings Tag.fetchParents Tag.findChild Tag.findChildren I gave the generators PEP 8-compliant names, and transformed them into\\nproperties: childGenerator() -> children nextGenerator() -> next_elements nextSiblingGenerator() -> next_siblings previousGenerator() -> previous_elements previousSiblingGenerator() -> previous_siblings recursiveChildGenerator() -> descendants parentGenerator() -> parents So instead of this: You can write this: (But the old code will still work.) Some of the generators used to yield None after they were done, and\\nthen stop. That was a bug. Now the generators just stop. There are two new generators, .strings and\\n.stripped_strings. .strings yields\\nNavigableString objects, and .stripped_strings yields Python\\nstrings that have had whitespace stripped. There is no longer a BeautifulStoneSoup class for parsing XML. To', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='strings that have had whitespace stripped. There is no longer a BeautifulStoneSoup class for parsing XML. To\\nparse XML you pass in “xml” as the second argument to the\\nBeautifulSoup constructor. For the same reason, the\\nBeautifulSoup constructor no longer recognizes the isHTML\\nargument. Beautiful Soup’s handling of empty-element XML tags has been\\nimproved. Previously when you parsed XML you had to explicitly say\\nwhich tags were considered empty-element tags. The selfClosingTags\\nargument to the constructor is no longer recognized. Instead,\\nBeautiful Soup considers any empty tag to be an empty-element tag. If\\nyou add a child to an empty-element tag, it stops being an\\nempty-element tag. An incoming HTML or XML entity is always converted into the\\ncorresponding Unicode character. Beautiful Soup 3 had a number of\\noverlapping ways of dealing with entities, which have been\\nremoved. The BeautifulSoup constructor no longer recognizes the\\nsmartQuotesTo or convertEntities arguments. (Unicode,', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='removed. The BeautifulSoup constructor no longer recognizes the\\nsmartQuotesTo or convertEntities arguments. (Unicode,\\nDammit still has smart_quotes_to, but its default is now to turn\\nsmart quotes into Unicode.) The constants HTML_ENTITIES,\\nXML_ENTITIES, and XHTML_ENTITIES have been removed, since they\\nconfigure a feature (transforming some but not all entities into\\nUnicode characters) that no longer exists. If you want to turn Unicode characters back into HTML entities on\\noutput, rather than turning them into UTF-8 characters, you need to\\nuse an output formatter. Tag.string now operates recursively. If tag A\\ncontains a single tag B and nothing else, then A.string is the same as\\nB.string. (Previously, it was None.) Multi-valued attributes like class have lists of strings as\\ntheir values, not simple strings. This may affect the way you search by CSS\\nclass. Tag objects now implement the __hash__ method, such that two\\nTag objects are considered equal if they generate the same', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}),\n",
       " Document(page_content='class. Tag objects now implement the __hash__ method, such that two\\nTag objects are considered equal if they generate the same\\nmarkup. This may change your script’s behavior if you put Tag\\nobjects into a dictionary or set. If you pass one of the find* methods both string and\\na tag-specific argument like name, Beautiful Soup will\\nsearch for tags that match your tag-specific criteria and whose\\nTag.string matches your string\\nvalue. It will not find the strings themselves. Previously,\\nBeautiful Soup ignored the tag-specific arguments and looked for\\nstrings. The BeautifulSoup constructor no longer recognizes the\\nmarkupMassage argument. It’s now the parser’s responsibility to\\nhandle markup correctly. The rarely-used alternate parser classes like\\nICantBelieveItsBeautifulSoup and BeautifulSOAP have been\\nremoved. It’s now the parser’s decision how to handle ambiguous\\nmarkup. The prettify() method now returns a Unicode string, not a bytestring.', metadata={'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'})]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_r_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma, FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = cleaned_r_pages\n",
    "embedding = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "\n",
    "vectordb = FAISS.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89\n"
     ]
    }
   ],
   "source": [
    "print(vectordb.index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"who is om birla\"\n",
    "retriever = vectordb.as_retriever()\n",
    "docs = retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1:\n",
      "{'source': 'https://indianexpress.com/article/india/om-birla-election-lok-sabha-speaker-modi-opposition-9415980/'}\n",
      "----------------------------------------\n",
      "Page 2:\n",
      "{'source': 'https://indianexpress.com/article/india/om-birla-election-lok-sabha-speaker-modi-opposition-9415980/'}\n",
      "----------------------------------------\n",
      "Page 3:\n",
      "{'source': 'https://indianexpress.com/article/india/om-birla-election-lok-sabha-speaker-modi-opposition-9415980/'}\n",
      "----------------------------------------\n",
      "Page 4:\n",
      "{'source': 'https://indianexpress.com/article/india/om-birla-election-lok-sabha-speaker-modi-opposition-9415980/'}\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "docs\n",
    "# Print the cleaned pages\n",
    "for i, page in enumerate(docs):\n",
    "    print(f\"Page {i+1}:\\n{page.metadata}\\n{'-'*40}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.save_local(\"faiss_index\",)\n",
    "\n",
    "new_db = FAISS.load_local(\"faiss_index\", embedding, allow_dangerous_deserialization = True)\n",
    "\n",
    "docs = new_db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://indianexpress.com/article/india/om-birla-election-lok-sabha-speaker-modi-opposition-9415980/'}\n",
      "{'source': 'https://indianexpress.com/article/india/om-birla-election-lok-sabha-speaker-modi-opposition-9415980/'}\n",
      "{'source': 'https://indianexpress.com/article/india/om-birla-election-lok-sabha-speaker-modi-opposition-9415980/'}\n",
      "{'source': 'https://indianexpress.com/article/india/om-birla-election-lok-sabha-speaker-modi-opposition-9415980/'}\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"what is beautifulsoup?\"\n",
    "docs_ss = new_db.similarity_search(question,k=3)\n",
    "len(docs_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "{'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "{'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n"
     ]
    }
   ],
   "source": [
    "for doc in docs_ss:\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "{'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n",
      "{'source': 'https://www.crummy.com/software/BeautifulSoup/bs4/doc/'}\n"
     ]
    }
   ],
   "source": [
    "docs_mmr = vectordb.max_marginal_relevance_search(question,k=3)\n",
    "len(docs_mmr)\n",
    "for doc in docs_mmr:\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 7, 9]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Sot_list:\n",
    "\n",
    "    def sort(self, list1):\n",
    "\n",
    "        for i in range(len(list1)-1):\n",
    "\n",
    "            for i in range(len(list1)-1):\n",
    "\n",
    "                curr = list1[i]\n",
    "                next = list1[i+1]\n",
    "\n",
    "                if curr >= next:\n",
    "                    list1[i] , list1[i+1] = list1[i+1], list1[i]\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "        return list1\n",
    "        \n",
    "    \n",
    "l = [2,7,4,3,9,1]\n",
    "sort = Sot_list()\n",
    "sl = sort.sort(l)\n",
    "print(sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'GOOGLE_API_KEY'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGOOGLE_API_KEY\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[0;32m<frozen os>:679\u001b[0m, in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'GOOGLE_API_KEY'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
